{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "from operator import add\n",
    "from datetime import datetime, date, timedelta\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',69)\n",
    "pd.set_option('display.max_rows',138)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Climate Scenarios (CH2018, and Random Scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leap(year):\n",
    "    \"\"\" return true for leap years, False for non leap years \"\"\"\n",
    "    return year % 4 == 0 and ( year % 100 != 0 or year % 400 == 0)\n",
    "\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pcp(dfpcp,FirstYear, LastYear, ScenarioNumbers, RCPNames):\n",
    "    \n",
    "    #outFileName = OutFileName\n",
    "    dfpcpCol = dfpcp.columns\n",
    "    #sceNum = len(dfCol)\n",
    "\n",
    "    sceNum = ScenarioNumbers\n",
    "    firstYear = FirstYear\n",
    "    lastYear = LastYear\n",
    "    simLen = lastYear - firstYear + 1\n",
    "\n",
    "    from random import shuffle\n",
    "    a = []\n",
    "    for x in range(simLen): \n",
    "        randomInd = [z for z in range(sceNum)]\n",
    "        shuffle(randomInd)\n",
    "        a.append(randomInd)\n",
    "        \n",
    "    RCP = RCPNames\n",
    "    columnsDfpcp = ['sc_' + RCP + str(k) for k in range(1, sceNum+1,1)] \n",
    "    c = [[19810101 for p in range(sceNum)]]\n",
    "    #df1 = 'df' + str(outDFNumber)\n",
    "    df1pcp = pd.DataFrame(c, columns =columnsDfpcp)\n",
    "    #df1.to_csv('SAeidVaghefimodified1111222.csv', index = False)\n",
    "\n",
    "    c.clear()\n",
    "\n",
    "    i = 0\n",
    "    p = 1\n",
    "    for m in range(firstYear, lastYear + 1, 1):\n",
    "        if is_leap(m):\n",
    "            for j in range(((m - firstYear)*365+p) , ((m - firstYear)*365+367+p-1), 1):\n",
    "                c.append(dfpcp[dfpcpCol[a[i]]].iloc[j].values) \n",
    "            i += 1 # counter i; equal to simulation length (simLen)\n",
    "            p += 1\n",
    "\n",
    "        else:\n",
    "            for j in range(((m - firstYear)*365+p), ((m - firstYear)*365+366+p-1), 1):\n",
    "                c.append(dfpcp[dfpcpCol[a[i]]].iloc[j].values) \n",
    "            i += 1\n",
    "                \n",
    "        print(m)\n",
    "\n",
    "        dfnewpcp = 'df' + str(m)\n",
    "        dfnewpcp = pd.DataFrame(c, columns =columnsDfpcp)\n",
    "        c.clear()\n",
    "        df1pcp = df1pcp.append(dfnewpcp, ignore_index=True)\n",
    "        #df1.to_csv('SAmodified1111222.csv', mode='w', header=True, index = False)\n",
    "    return df1pcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tmp(dftmp,FirstYear, LastYear, ScenarioNumbers, RCPNames):\n",
    "\n",
    "    #dfCol = df.columns\n",
    "    #sceNum = len(dfCol) // 2\n",
    "    sceNum = ScenarioNumbers\n",
    "    firstYear = FirstYear\n",
    "    lastYear = LastYear\n",
    "    simLen = lastYear - firstYear + 1\n",
    "\n",
    "    dftmpColMax = dftmp.columns[[i for i in range(0, sceNum*2, 2)]]\n",
    "    dftmpColMin = dftmp.columns[[i for i in range(1, sceNum*2, 2)]]\n",
    "\n",
    "    ## yek list be toole 119 ke dakhelesh list haye 68 ta ee darim be soorate random\n",
    "    from random import shuffle\n",
    "    a = []\n",
    "    for i in range(simLen): \n",
    "        randomInd = [j for j in range(sceNum)]\n",
    "        #x = [[i] for i in range(10)] \n",
    "        shuffle(randomInd)\n",
    "        a.append(randomInd)\n",
    "        \n",
    "    #print('end!')\n",
    "\n",
    "    cT = []\n",
    "    RCP = RCPNames\n",
    "    columnsDfOdd = ['sc_' + RCP + str(k)  for k in range(1, sceNum+1,1)] \n",
    "    columnsDfEven = [\"\"] * sceNum\n",
    "\n",
    "    columnsDftmp = []\n",
    "    #colOdd = ['Scr_' + str(i) for i in range(1, sceNum+1, 1)]\n",
    "    #colEven = ['' for i in range(1, sceNum+1,1)]\n",
    "\n",
    "    for i in range (sceNum):\n",
    "        columnsDftmp.append(columnsDfOdd[i])\n",
    "        columnsDftmp.append(columnsDfEven[i])\n",
    "\n",
    "\n",
    "    #### OR:\n",
    "    #columnsDf = [\"Sr\", \"\"] * sceNum\n",
    "    df1tmp = pd.DataFrame(cT, columns =columnsDftmp)\n",
    "    #df1.to_csv(\"rrrrrrrrmodified1111222.csv\", index = False)\n",
    "\n",
    "\n",
    "    cMax = [[19810101 for p in range(sceNum)]]\n",
    "    cMin = [[\"\" for p in range(sceNum)]]\n",
    "    i = 0\n",
    "    p = 1\n",
    "    for m in range(firstYear, lastYear + 1, 1):\n",
    "        if is_leap(m):\n",
    "            for j in range(((m - firstYear)*365+p) , ((m - firstYear)*365+367+p-1), 1):\n",
    "                cMax.append(dftmp[dftmpColMax[a[i]]].iloc[j].values)\n",
    "                cMin.append(dftmp[dftmpColMin[a[i]]].iloc[j].values) \n",
    "    \n",
    "            i += 1\n",
    "\n",
    "        else:\n",
    "            for j in range(((m - firstYear)*365+p), ((m - firstYear)*365+366+p-1), 1):\n",
    "                cMax.append(dftmp[dftmpColMax[a[i]]].iloc[j].values)\n",
    "                cMin.append(dftmp[dftmpColMin[a[i]]].iloc[j].values)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        c = []\n",
    "        for y in range(0, len(cMax), 1): # the length of simulation years\n",
    "            for z in range(sceNum): # range(4)\n",
    "                c.append(cMax[y][z])\n",
    "                c.append(cMin[y][z])\n",
    "\n",
    "        cMax.clear()\n",
    "        cMin.clear()\n",
    "\n",
    "        cMain = []\n",
    "        cMain = list(chunks(c, sceNum * 2))\n",
    "        print(m)\n",
    "\n",
    "    ### Should be checked\n",
    "\n",
    "        dfnewtmp = 'dftmp' + str(m)\n",
    "        #columnsDf = [\"Sr\", \"\"]*sceNum\n",
    "        #columnsDf = [['sc_' + str(k), \"\"] for k in range(1, sceNum+1,1)] \n",
    "        dfnewtmp = pd.DataFrame(cMain, columns =columnsDftmp)\n",
    "        c.clear()\n",
    "        df1tmp = df1tmp.append(dfnewtmp, ignore_index=True)\n",
    "        #df1.to_csv(\"rrrrrrrrmodified1111222.csv\", mode='w', header=True, index = False)\n",
    "    return df1tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcp files start working!\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "end!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-b9287bb2017a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'end!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "def randomness_pcp_tmp(fnames):\n",
    "    for f in fnames:\n",
    "        if 'p.csv' in f:\n",
    "            print('pcp files start working!')\n",
    "            #df = pd.read_csv('47-0625000_8-6666667p.csv')\n",
    "            dfpcp = pd.read_csv(f)\n",
    "\n",
    "\n",
    "            filt1 = dfpcp.columns.str.contains('RCP26') #12\n",
    "            filt2 = dfpcp.columns.str.contains('RCP45') #25\n",
    "            filt3 = dfpcp.columns.str.contains('RCP85') #31\n",
    "\n",
    "            dfpcpRCP26 = dfpcp.loc[:, filt1]\n",
    "            dfpcpRCP45 = dfpcp.loc[:, filt2]\n",
    "            dfpcpRCP85 = dfpcp.loc[:, filt3]\n",
    "\n",
    "            dfpcpRCP26_n = random_pcp(dfpcpRCP26, 1981, 2099, 12, '26_')\n",
    "            dfpcpRCP45_n = random_pcp(dfpcpRCP45, 1981, 2099, 25, '45_')\n",
    "            dfpcpRCP85_n = random_pcp(dfpcpRCP85, 1981, 2099, 31, '85_')\n",
    "\n",
    "\n",
    "            result = pd.concat([dfpcpRCP26_n, dfpcpRCP45_n, dfpcpRCP85_n], axis=1, sort=False)\n",
    "            #result.to_csv('47-0625000_8-6666667p_n1.csv', index = False)\n",
    "\n",
    "\n",
    "            #newName = 'n_'+ f\n",
    "            newName = f\n",
    "            #filepath = os.path.join(os.getcwd(), newName)\n",
    "            root = os.getcwd()\n",
    "            if os.path.isdir(os.path.join(root, 'Outputs_randomness')):\n",
    "                pass\n",
    "            else: os.mkdir(os.path.join(root, 'Outputs_randomness'))\n",
    "\n",
    "            outfolder = os.path.join(os.getcwd(), 'Outputs_randomness')\n",
    "            filepath = os.path.join(outfolder, newName)\n",
    "\n",
    "            result.to_csv(filepath, index = False)\n",
    "            print('end!')\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "        elif 't.csv' in f:\n",
    "            print('tmp files start working!')\n",
    "            dftmp = pd.read_csv(f)\n",
    "            dftmpCol = list(dftmp.columns)\n",
    "\n",
    "            filt1_max = [dftmpCol.index(s) for s in dftmpCol if \"RCP26\" in s]\n",
    "            filt2_max = [dftmpCol.index(s) for s in dftmpCol if \"RCP45\" in s]\n",
    "            filt3_max = [dftmpCol.index(s) for s in dftmpCol if \"RCP85\" in s]\n",
    "\n",
    "            aOnefilt1= [1]*len(filt1_max)\n",
    "            aOnefilt2= [1]*len(filt2_max)\n",
    "            aOnefilt3= [1]*len(filt3_max)\n",
    "\n",
    "            filt1_min = list(map(add, filt1_max, aOnefilt1))\n",
    "            filt2_min = list(map(add, filt2_max, aOnefilt2))\n",
    "            filt3_min = list(map(add, filt3_max, aOnefilt3))\n",
    "\n",
    "            filt1Tot = []\n",
    "            for i in range(len(filt1_max)):\n",
    "                filt1Tot.append(filt1_max[i])\n",
    "                filt1Tot.append(filt1_min[i])\n",
    "\n",
    "            filt2Tot = []\n",
    "            for j in range(len(filt2_max)):\n",
    "                filt2Tot.append(filt2_max[j])\n",
    "                filt2Tot.append(filt2_min[j])\n",
    "\n",
    "            filt3Tot = []\n",
    "            for k in range(len(filt3_max)):\n",
    "                filt3Tot.append(filt3_max[k])\n",
    "                filt3Tot.append(filt3_min[k])\n",
    "\n",
    "            dftmpRCP26 = dftmp[dftmp.columns[filt1Tot]]\n",
    "            dftmpRCP45 = dftmp[dftmp.columns[filt2Tot]]\n",
    "            dftmpRCP85 = dftmp[dftmp.columns[filt3Tot]]\n",
    "\n",
    "            dftmpRCP26_n = random_tmp (dftmpRCP26, 1981, 2099, 12, '26_')\n",
    "            dftmpRCP45_n = random_tmp (dftmpRCP45, 1981, 2099, 25, '45_')\n",
    "            dftmpRCP85_n = random_tmp (dftmpRCP85, 1981, 2099, 31, '85_')\n",
    "\n",
    "            result = pd.concat([dftmpRCP26_n, dftmpRCP45_n, dftmpRCP85_n], axis=1, sort=False)\n",
    "\n",
    "            #ewName = 'n'+f\n",
    "            #ilepath = os.path.join(os.environ.get('HOME'), newName)\n",
    "            #esult.to_csv(filepath, index = False)\n",
    "\n",
    "            #newName = 'n_'+ f\n",
    "            newName = f\n",
    "            #filepath = os.path.join(os.getcwd(), newName)\n",
    "\n",
    "            outfolder =os.path.join(os.getcwd(), 'Outputs_randomness')\n",
    "            filepath = os.path.join(outfolder, newName)\n",
    "            result.to_csv(filepath, index = False)\n",
    "\n",
    "            print('end!')\n",
    "\n",
    "        else :\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootFolder = r'C:\\Users\\ashrafse\\SA_2\\snowModelUZH\\case3_hoch-ybrig'\n",
    "inputFolder = os.path.join(rootFolder,'input')\n",
    "climate_ref_Folder = os.path.join(inputFolder, 'Climate_ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(climate_ref_Folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['47-0000000_8-7708333p.csv',\n",
       " '47-0000000_8-7708333t.csv',\n",
       " 'pcp.txt',\n",
       " 'tmp.txt']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomness_pcp_tmp(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: function for initiating the main dictionary of climate stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dic(a):\n",
    "    '''Function: creating a dictionary for each climate station'''\n",
    "    \n",
    "    a = {}\n",
    "    keys = ['fM', 'iPot', 'rSnow', 'dSnow', 'cPrec', 'dP', 'elev', 'lat', 'long', 'fileName']\n",
    "    a = {key: None for key in keys}\n",
    "    return a\n",
    "\n",
    "def initialize_input_dict (mainFolderSki):\n",
    "    ''' This function returns a dictionary , and addresses of 4 folders'''\n",
    "    \n",
    "    \n",
    "    '''Step 1''' \n",
    "    rootFolder = mainFolderSki\n",
    "    inputFolder = os.path.join(rootFolder,'input')\n",
    "    ablationFolder = os.path.join(inputFolder, 'Ablation')\n",
    "    accumulationFolder = os.path.join(inputFolder, 'Accumulation')\n",
    "    climate_ref_Folder = os.path.join(inputFolder, 'Climate_ref')\n",
    "    \n",
    "    \n",
    "    '''Step 2: Reading all files names inside the Ablation, Accumulation, and Climate folders'''  \n",
    "    ablationFiles = []\n",
    "    for filename in os.walk(ablationFolder):\n",
    "        ablationFiles = filename[2]\n",
    "    \n",
    "    accumulationFiles = list()\n",
    "    for filename in os.walk(accumulationFolder):\n",
    "        accumulationFiles = filename[2]\n",
    "\n",
    "    climate_ref_Files = list()\n",
    "    for filename in os.walk(climate_ref_Folder):\n",
    "        climate_ref_Files = filename[2]\n",
    "        \n",
    "        \n",
    "    '''Step 3: Reading files inside ablation folder '''\n",
    "    os.chdir(ablationFolder)\n",
    "    with open(ablationFiles[0], 'r') as file:\n",
    "        FM1 = file.read()\n",
    "    with open(ablationFiles[1], 'r') as file:\n",
    "        Ipot1 = file.read()\n",
    "    with open(ablationFiles[2], 'r') as file:\n",
    "        Rsnow1 = file.read()\n",
    "        \n",
    "        \n",
    "    '''Step 4: Reading the lines of files inside ablation folder'''\n",
    "    FM1 = FM1.replace('\\n', '\\t')\n",
    "    FM1 = FM1.split('\\t')\n",
    "    Ipot1 = Ipot1.replace('\\n', '\\t').split('\\t')\n",
    "    Rsnow1 = Rsnow1.replace('\\n', '\\t').split('\\t')\n",
    "        \n",
    "        \n",
    "    '''Step 5: Reading the lines of files inside accumulation folder''' \n",
    "    os.chdir(accumulationFolder)\n",
    "    \n",
    "    with open(accumulationFiles[0], 'r') as file:\n",
    "        cPrec = file.read()\n",
    "    with open(accumulationFiles[1], 'r') as file:\n",
    "        dSnow1 = file.read()\n",
    "    \n",
    "    cPrec = cPrec.replace('\\n', '\\t')\n",
    "    cPrec = cPrec.split('\\t')\n",
    "    dSnow1 = dSnow1.replace('\\n', '\\t').split('\\t')\n",
    "    \n",
    "    \n",
    "    '''Step 6: Reading the lines of files inside climate folder''' \n",
    "    os.chdir(climate_ref_Folder)\n",
    "    \n",
    "    with open('pcp.txt', 'r') as file:\n",
    "        pcpData = file.read()\n",
    "    with open('tmp.txt', 'r') as file:\n",
    "        tmpData = file.read()\n",
    "        \n",
    "    pcpData = pcpData.split('\\n')\n",
    "    \n",
    "    for i in range(len(pcpData)):\n",
    "        pcpData[i] = pcpData[i].split(',')\n",
    "        \n",
    "        \n",
    "    '''Step 7: Initialazing the input dictionary of climate stations which holds the information of accumulation\n",
    "     and ablation, and etc of the stations''' \n",
    "    nameStn = []\n",
    "    for file in climate_ref_Files:\n",
    "        if 'p.csv' in file:\n",
    "            #nameStn.append('n_' + file[-25: -5])\n",
    "            nameStn.append(file[-25: -5])\n",
    "\n",
    "    stnDicts = []\n",
    "    for i in range(len(nameStn)):\n",
    "        stnDicts.append(create_dic(nameStn[i]))\n",
    "    \n",
    "    \n",
    "    '''Step 8: Assigning the file names to the dictionary'''\n",
    "    for i in range (len(nameStn)):\n",
    "        stnDicts[i]['fileName'] = nameStn[i]\n",
    "\n",
    "    \n",
    "    '''Step 9: Assigning the accumulation and ablation values'''\n",
    "    for stnDict in stnDicts:\n",
    "        for i, element in enumerate(FM1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['fM'] = FM1[i+1]\n",
    "                \n",
    "        for i, element in enumerate(Ipot1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['iPot'] = Ipot1[i+1]\n",
    "\n",
    "        for i, element in enumerate(Rsnow1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:  \n",
    "                stnDict['rSnow'] = Rsnow1[i+1]\n",
    "\n",
    "        for i, element in enumerate(dSnow1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['dSnow'] = dSnow1[i+1]\n",
    "\n",
    "        for i, element in enumerate(cPrec):\n",
    "            stnDict['cPrec'] = cPrec[1]\n",
    "            stnDict['dP'] = cPrec[3]\n",
    "            \n",
    "    '''Step 10: Assigning the elevation, Lat and long to the dictionaries'''\n",
    "    for i in range(len(stnDicts)):\n",
    "        for j in range(1, len(pcpData)):\n",
    "            \n",
    "            #if pcpData[j][1][2:-1] == stnDicts[i]['fileName'][2:]:\n",
    "            if pcpData[j][1][:-1] == stnDicts[i]['fileName'][:]:\n",
    "                stnDicts[i]['lat']= pcpData[j][2]\n",
    "                stnDicts[i]['long']= pcpData[j][3]\n",
    "                stnDicts[i]['elev']= pcpData[j][4]\n",
    "                \n",
    "    return stnDicts, inputFolder, ablationFolder, accumulationFolder, climate_ref_Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Initializiing the main dictionary for a case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseStudyStns = {}\n",
    "inputFolder = ''\n",
    "ablationFolder = ''\n",
    "accumulationFolder = ''\n",
    "climateFolder = ''\n",
    "#root = 'C:/Users/ashrafse/SA_2/snowModelUZH/case2_Atzmaening'\n",
    "#root = 'C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations'\n",
    "root = r'C:\\Users\\ashrafse\\SA_2\\snowModelUZH\\case3_hoch-ybrig'\n",
    "\n",
    "## calling the function with multiple return values\n",
    "caseStudyStns, inputFolder, ablationFolder, accumulationFolder, climateFolder = initialize_input_dict(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fM': '1.012',\n",
       "  'iPot': '1000',\n",
       "  'rSnow': '0.5',\n",
       "  'dSnow': '0.5',\n",
       "  'cPrec': '0',\n",
       "  'dP': '0',\n",
       "  'elev': '1755',\n",
       "  'lat': '47.00',\n",
       "  'long': '8.7708333',\n",
       "  'fileName': '47-0000000_8-7708333'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseStudyStns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Matching the station names values in the dictionary of stations with CSV files in Climate folder of the case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pcpAtzmaening = []\n",
    "#tmpAtzmaening = []\n",
    "pcpCaseStudy = []\n",
    "tmpCaseStudy = []\n",
    "\n",
    "for i in range(len(caseStudyStns)):\n",
    "    pcpCaseStudy.append(os.path.join(climateFolder, caseStudyStns[i]['fileName'] + 'p.csv'))\n",
    "    tmpCaseStudy.append(os.path.join(climateFolder, caseStudyStns[i]['fileName'] + 't.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\ashrafse\\\\SA_2\\\\snowModelUZH\\\\case3_hoch-ybrig\\\\input\\\\Climate_ref\\\\47-0000000_8-7708333t.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpCaseStudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\ashrafse\\\\SA_2\\\\snowModelUZH\\\\case3_hoch-ybrig\\\\input\\\\Climate_ref\\\\47-0000000_8-7708333p.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcpCaseStudy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: building a database for each precipitation and temperature file in Climate folder and saving them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpcp = [None for _ in range(len(pcpCaseStudy))]\n",
    "dftmp = [None for _ in range(len(tmpCaseStudy))]\n",
    "for i in range(len(pcpCaseStudy)):\n",
    "    dfpcp[i] = pd.read_csv(pcpCaseStudy[i])\n",
    "    dftmp[i] = pd.read_csv(tmpCaseStudy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### making a header for output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpcpCol = dfpcp[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftmpCol = dftmp[0].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining the length of simulations and scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenariosLength = len(dfpcpCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulationLength = len(dftmp[0][dftmpCol[0]]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Main Snow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1st column as index: makaing date from 01 01 1981 to 2099 12 31\n",
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date ).days + 1)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def snow_Model (caseStudyStns, dateList, seasonList, thershold = None):\n",
    "#def snow_Model (caseStudyStns, dateList, seasonList, x1 = None):\n",
    "def snow_Model (x1 = None):\n",
    "\n",
    "    \n",
    "    caseStudyStns = {}\n",
    "    inputFolder = ''\n",
    "    ablationFolder = ''\n",
    "    accumulationFolder = ''\n",
    "    climateFolder = ''\n",
    "    #root = 'C:/Users/ashrafse/SA_2/snowModelUZH/case2_Atzmaening'\n",
    "    #root = 'C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations'\n",
    "    root = r'C:\\Users\\ashrafse\\SA_2\\snowModelUZH\\case3_hoch-ybrig'\n",
    "\n",
    "    ## calling the function with multiple return values\n",
    "    caseStudyStns, inputFolder, ablationFolder, accumulationFolder, climateFolder = initialize_input_dict(root)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Reading the beginning and end of the simulation\n",
    "    start_date = date(1981, 1, 1)\n",
    "    end_date = date(2099, 12, 31)\n",
    "    dateList = []\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        dateList.append(single_date.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "    seasonList = []\n",
    "    for n in range (1981, 2100, 1):\n",
    "        seasonList.append(str(n))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### PART 1 : daily outputs\n",
    "    ## Running the model for each climate station:\n",
    "    \n",
    "    for k in range(len(caseStudyStns)):\n",
    "\n",
    "        ## making a header for output files\n",
    "        dfpcpCol = dfpcp[k].columns\n",
    "        dftmpCol = dftmp[k].columns\n",
    "\n",
    "\n",
    "        ## defining the length of simulations and scenarios\n",
    "        scenariosLength = len(dfpcpCol)\n",
    "        simulationLength = len(dftmp[0][dftmpCol[0]]) - 1\n",
    "\n",
    "\n",
    "        ## declaring the initial arrays\n",
    "        accumulation = [0 for _ in range(simulationLength)]\n",
    "        ablation =  [0 for _ in range(simulationLength)]\n",
    "        snowDeposite = [0 for _ in range(simulationLength)]\n",
    "        total = np.zeros([simulationLength, 3*scenariosLength])\n",
    "\n",
    "\n",
    "        ## Running the model for each climate scenario:\n",
    "        for j in range(len(dfpcpCol)):\n",
    "            ## Reading the information and inputs of the first day of simulation\n",
    "            todayPCP = dfpcp[k][dfpcpCol[j]].iloc[1] if (dfpcp[k][dfpcpCol[j]].iloc[1] != -99) else 0\n",
    "            todayTMPMAX = round(dftmp[k][dftmpCol[2*j]].iloc[1],2) if(dftmp[k][dftmpCol[2*j]].iloc[1] != -99) else 0\n",
    "            todayTMPMIN = round(dftmp[k][dftmpCol[2*j+1]].iloc[1],2) if(dftmp[k][dftmpCol[2*j+1]].iloc[1] != -99) else 0\n",
    "            todayTMPAVE = round((todayTMPMAX+todayTMPMIN)/2,2) if((todayTMPMAX+todayTMPMIN)/2 != -99) else 0\n",
    "\n",
    "\n",
    "            ### Accumulation for the first day:\n",
    "            if (todayTMPAVE) <=-1:\n",
    "                accumulation[0] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*(1)\n",
    "\n",
    "            elif -1 < (todayTMPAVE) <= 1:\n",
    "                accumulation[0] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*float((1-todayTMPAVE)/2)\n",
    "\n",
    "            else: accumulation[0] = 0\n",
    "\n",
    "\n",
    "            ### Ablation for the first day:\n",
    "            if todayTMPAVE <= 1:\n",
    "                 ablation[0] = 0\n",
    "            else: \n",
    "                ablation[0] = (float(caseStudyStns[k]['fM']) + float(caseStudyStns[k]['rSnow'])*float(caseStudyStns[k]['iPot'])*0.001)*float(todayTMPAVE)*(1+0)\n",
    "\n",
    "            ### Main mass balance equation for the first day:\n",
    "            snowDeposite[0] = 0 if (0 + accumulation[0] - ablation[0]) < 0 else (0 + accumulation[0] - ablation[0])\n",
    "\n",
    "            ### storing three values in a list for the first day\n",
    "            total[0,3*j+0] = round((accumulation[0] - ablation[0]), 2)\n",
    "            total[0,3*j+1] = round(snowDeposite[0], 2)\n",
    "            total[0,3*j+2] = 1 if (total[0,3*j+1] > x1) else 0\n",
    "\n",
    "\n",
    "            ## For the second day to the end of simulation:\n",
    "            i = 0\n",
    "            for i in range(2, simulationLength + 1, 1):\n",
    "                # precipitation and temperature missing values were handled\n",
    "\n",
    "                ## Reading the information and inputs of the first day of simulation\n",
    "                todayPCP = dfpcp[k][dfpcpCol[j]].iloc[i] if (dfpcp[k][dfpcpCol[j]].iloc[i] != -99) else 0\n",
    "                todayTMPMAX = round(dftmp[k][dftmpCol[2*j]].iloc[i],2) if(dftmp[k][dftmpCol[2*j]].iloc[i] != -99) else 0\n",
    "                todayTMPMIN = round(dftmp[k][dftmpCol[2*j+1]].iloc[i],2) if(dftmp[k][dftmpCol[2*j+1]].iloc[i] != -99) else 0\n",
    "                todayTMPAVE = round((todayTMPMAX+todayTMPMIN)/2,2) if((todayTMPMAX+todayTMPMIN)/2 != -99) else 0\n",
    "\n",
    "                ### Accumulation :\n",
    "                if(todayTMPAVE) <= -1:\n",
    "                    ##\n",
    "                    accumulation[i-1] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*(1)\n",
    "\n",
    "                elif -1 < (todayTMPAVE) <= 1:\n",
    "                    accumulation[i-1] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*float((1-todayTMPAVE)/2)\n",
    "\n",
    "                else: accumulation[i-1] = 0\n",
    "\n",
    "                ### Ablation :\n",
    "                if todayTMPAVE <= 0:\n",
    "                    ablation[i-1] = 0\n",
    "                else: \n",
    "                    ablation[i-1] = (float(caseStudyStns[k]['fM']) + float(caseStudyStns[k]['rSnow'])*float(caseStudyStns[k]['iPot'])*0.001)*float(todayTMPAVE)*(1+0)\n",
    "\n",
    "                ### Main mass balance equation for second day to the end of simulation:\n",
    "                snowDeposite[i-1] = 0 if (snowDeposite[i-2] + accumulation[i-1] - ablation[i-1]) < 0 else (snowDeposite[i-2] + accumulation[i-1] - ablation[i-1])\n",
    "\n",
    "\n",
    "                ### storing three values in a list \n",
    "                total[i-1,3*j+0] = round((accumulation[i-1] - ablation[i-1]) , 2)\n",
    "                total[i-1,3*j+1] = round(snowDeposite[i-1], 2)\n",
    "                total[i-1,3*j+2] = 1 if (total[i-1,3*j+1] > x1) else 0\n",
    "\n",
    "\n",
    "        ### Saving the output of total list in a csv file in a specific path\n",
    "\n",
    "        ## 1st row as the column names:\n",
    "        columnsDF = [] \n",
    "        for col in dfpcpCol:\n",
    "            columnsDF.append('SnowAmount_' + col)\n",
    "            columnsDF.append('TotalSnowAmount_' + col)\n",
    "            columnsDF.append('isOverSnow_' + col)\n",
    "\n",
    "\n",
    "        columnsDF0 = ['DATE']\n",
    "        dfnew0 = pd.DataFrame(dateList, columns = columnsDF0)\n",
    "        dfnew = pd.DataFrame(total, columns = columnsDF)\n",
    "        df1 = pd.concat([dfnew0, dfnew], axis=1, sort=False)\n",
    "\n",
    "        if os.path.isdir(os.path.join(root, 'Outputs_py')):\n",
    "            pass\n",
    "        else: os.mkdir(os.path.join(root, 'Outputs_py'))\n",
    "\n",
    "        outfolder =os.path.join(root, 'Outputs_py') \n",
    "        outfileName = 'Total_daily_' + caseStudyStns[k]['fileName'] + '.csv'\n",
    "        outputFile = os.path.join(outfolder, outfileName )\n",
    "        df1.to_csv(outputFile, index = False)\n",
    "        #return df1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ##### PART 2 seasonal outputs\n",
    "        \n",
    "        total_Daily_Files = list()\n",
    "        for filename in os.walk(outfolder):\n",
    "            total_Daily_Files = filename[2]\n",
    "            \n",
    "        ##get just total nor existing seasonal...\n",
    "        \n",
    "        totalFiles = []\n",
    "        for i in range(len(total_Daily_Files)):\n",
    "            totalFiles.append(os.path.join(outfolder, total_Daily_Files[i]))\n",
    "        \n",
    "        totalFiles = []\n",
    "        for i in range(len(total_Daily_Files)):\n",
    "            if 'season' in total_Daily_Files[i]:\n",
    "                continue\n",
    "            else: totalFiles.append(os.path.join(outfolder, total_Daily_Files[i]))\n",
    "                \n",
    "        \n",
    "        dfSeason = [ None for _ in range(len(totalFiles))]\n",
    "        for i in range(len(totalFiles)):\n",
    "            dfSeason[i] = pd.read_csv(totalFiles[i], low_memory=False)\n",
    "\n",
    "            start_date = date(1981, 1, 2)\n",
    "            end_date = date(2099, 12, 31)\n",
    "            dateList = []\n",
    "            for single_date in daterange(start_date, end_date):\n",
    "                dateList.append(single_date.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "            start_season = []\n",
    "            end_season = []\n",
    "\n",
    "            for pp in range (1981, 2099, 1):\n",
    "                #start_season.append(date(i, 1, 1))\n",
    "                #end_season.append(date(i+1, 12, 31)) if i != 2099 else end_season.append(date(i, 12, 30))\n",
    "                start_season.append(date(pp, 11, 1))\n",
    "                end_season.append(date(pp+1, 4, 30))\n",
    "\n",
    "            df2 = dfSeason[i]\n",
    "            df2.set_index('DATE', inplace = True)\n",
    "            df2Col = df2.columns\n",
    "\n",
    "            df2ColCal = []\n",
    "            for m in range(68):\n",
    "            #for k in range(68):\n",
    "                df2ColCal.append(df2Col[3*m+2])\n",
    "\n",
    "            sumGoodCondition = np.zeros([len(start_season), len(df2ColCal)])\n",
    "\n",
    "\n",
    "            for j in range(len(df2ColCal)):\n",
    "                for k in range(len(start_season)):\n",
    "                #for i in range(3):\n",
    "                    start_date = start_season[k]\n",
    "                    end_date = end_season[k]\n",
    "                        #start_date = date(1981, 1, 2)\n",
    "                        #end_date = date(1981, 1, 10)\n",
    "                    for single_date in daterange(start_date, end_date):\n",
    "                        #sumGood1[i] += df2['TotalSnowAmount_CLMCOM-CCLM4-ECEARTH-EUR11-RCP45-pcp'].loc[single_date.strftime(\"%m/%d/%Y\")]\n",
    "                        sumGoodCondition[k,j] += df2[df2ColCal[j]].loc[single_date.strftime(\"%m/%d/%Y\")]\n",
    "                            #print(single_date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "            df3 = pd.DataFrame(sumGoodCondition, columns = df2ColCal)\n",
    "\n",
    "\n",
    "            firstCol = []\n",
    "            for o in range (len(seasonList)-1):\n",
    "                firstCol.append(seasonList[o] +'-' + seasonList[o+1])\n",
    "\n",
    "            columnsDF1 = ['Season']\n",
    "            dfnew0 = pd.DataFrame(firstCol, columns = columnsDF1)\n",
    "\n",
    "            dfFinalSeason = pd.concat([dfnew0, df3], axis=1, sort=False)\n",
    "\n",
    "            outfileNameSeason = 'season_' + total_Daily_Files[i]\n",
    "            outputFile = os.path.join(outfolder, outfileNameSeason)\n",
    "            dfFinalSeason.to_csv(outputFile, index = False)\n",
    "\n",
    "            \n",
    "        \n",
    "        #return df1, outfolder, dfFinalSeason\n",
    "        return {'y': x1*2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: EMA_Workbench connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 20 dec. 2010\n",
    "\n",
    "This file illustrated the use the EMA classes for a contrived example\n",
    "It's main purpose has been to test the parallel processing functionality\n",
    "\n",
    ".. codeauthor:: jhkwakkel <j.h.kwakkel (at) tudelft (dot) nl>\n",
    "'''\n",
    "from __future__ import (absolute_import, print_function, division,\n",
    "                        unicode_literals)\n",
    "\n",
    "from ema_workbench import (Model, RealParameter, ScalarOutcome, ema_logging,\n",
    "                           perform_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.evaluators/INFO/MainProcess] performing 2 scenarios * 1 policies * 1 model(s) = 2 experiments\n",
      "[EMA.ema_workbench.em_framework.evaluators/INFO/MainProcess] performing experiments sequentially\n",
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 1 cases completed\n",
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 2 cases completed\n",
      "[EMA.ema_workbench.em_framework.evaluators/INFO/MainProcess] experiments finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ema_logging.LOG_FORMAT = '[%(name)s/%(levelname)s/%(processName)s] %(message)s'\n",
    "    ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "    #model = Model('simpleModel', function=some_model)  # instantiate the model\n",
    "    model = Model('simpleModel', function = snow_Model)  # instantiate the model\n",
    "\n",
    "    # specify uncertainties\n",
    "    model.uncertainties = [RealParameter(\"x1\", 100.0, 300.0)]\n",
    "    \n",
    "    #model.uncertainties = [RealParameter(\"x1\", 0.1, 10),\n",
    "     #                      RealParameter(\"x2\", -0.01, 0.01),\n",
    "      #                     RealParameter(\"x3\", -0.01, 0.01)]\n",
    "   \n",
    "\n",
    "    # specify outcomes\n",
    "    #model.outcomes = [ScalarOutcome('y')]\n",
    "\n",
    "    results = perform_experiments(model, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(           x1 scenario policy        model\n",
       " 0  143.215471        2   None  simpleModel\n",
       " 1  246.907592        3   None  simpleModel, {})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
