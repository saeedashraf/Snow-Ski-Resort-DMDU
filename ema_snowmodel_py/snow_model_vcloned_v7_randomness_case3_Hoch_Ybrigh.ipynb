{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reading the libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import random\n",
    "from operator import add\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import shutil\n",
    "import ema_workbench\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Seting up the display extent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',69)\n",
    "pd.set_option('display.max_rows',138)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up Climate Scenarios (CH2018, and Random Scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leap(year):\n",
    "    \"\"\" return true for leap years, False for non leap years \"\"\"\n",
    "    return year % 4 == 0 and ( year % 100 != 0 or year % 400 == 0)\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1.1. *Function that pruduces new climate (precipitation) realization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pcp(dfpcp, FirstYear, LastYear, ScenarioNumbers, RCPNames, Xfactor1):\n",
    "    \n",
    "    #outFileName = OutFileName\n",
    "    dfpcpCol = dfpcp.columns\n",
    "    #sceNum = len(dfCol)\n",
    "\n",
    "    sceNum = ScenarioNumbers\n",
    "    firstYear = FirstYear\n",
    "    lastYear = LastYear\n",
    "    simLen = lastYear - firstYear + 1\n",
    "\n",
    "    from random import shuffle\n",
    "    #a = []\n",
    "    #for x in range(simLen): \n",
    "        #randomInd = [z for z in range(sceNum)]\n",
    "        #shuffle(randomInd)\n",
    "        #a.append(randomInd)\n",
    "        \n",
    "        \n",
    "    a = []\n",
    "    for i in range(simLen): \n",
    "        randomInd = [z for z in range(sceNum)]\n",
    "        #x = [[i] for i in range(10)] \n",
    "        for x in range(int(round(Xfactor1))):\n",
    "            shuffle(randomInd)\n",
    "        a.append(randomInd)\n",
    "    \n",
    "    \n",
    "        \n",
    "    RCP = RCPNames\n",
    "    columnsDfpcp = ['sc_' + RCP + str(k) for k in range(1, sceNum+1,1)] \n",
    "    c = [[19810101 for p in range(sceNum)]]\n",
    "    #df1 = 'df' + str(outDFNumber)\n",
    "    df1pcp = pd.DataFrame(c, columns =columnsDfpcp)\n",
    "    #df1.to_csv('SAeidVaghefimodified1111222.csv', index = False)\n",
    "\n",
    "    c.clear()\n",
    "\n",
    "    i = 0\n",
    "    p = 1\n",
    "    for m in range(firstYear, lastYear + 1, 1):\n",
    "        if is_leap(m):\n",
    "            for j in range(((m - firstYear)*365+p) , ((m - firstYear)*365+367+p-1), 1):\n",
    "                c.append(dfpcp[dfpcpCol[a[i]]].iloc[j].values) \n",
    "            i += 1 # counter i; equal to simulation length (simLen)\n",
    "            p += 1\n",
    "\n",
    "        else:\n",
    "            for j in range(((m - firstYear)*365+p), ((m - firstYear)*365+366+p-1), 1):\n",
    "                c.append(dfpcp[dfpcpCol[a[i]]].iloc[j].values) \n",
    "            i += 1\n",
    "                \n",
    "        #print(m) # this line show the progress of the work by typing the years of simulation\n",
    "\n",
    "        dfnewpcp = 'df' + str(m)\n",
    "        dfnewpcp = pd.DataFrame(c, columns =columnsDfpcp)\n",
    "        c.clear()\n",
    "        df1pcp = df1pcp.append(dfnewpcp, ignore_index=True)\n",
    "        \n",
    "    return df1pcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1.2. *Function that pruduces new climate (temperature) realization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tmp(dftmp, FirstYear, LastYear, ScenarioNumbers, RCPNames, Xfactor1):\n",
    "\n",
    "    #dfCol = df.columns\n",
    "    #sceNum = len(dfCol) // 2\n",
    "    sceNum = ScenarioNumbers\n",
    "    firstYear = FirstYear\n",
    "    lastYear = LastYear\n",
    "    simLen = lastYear - firstYear + 1\n",
    "\n",
    "    dftmpColMax = dftmp.columns[[i for i in range(0, sceNum*2, 2)]]\n",
    "    dftmpColMin = dftmp.columns[[i for i in range(1, sceNum*2, 2)]]\n",
    "\n",
    "    ## yek list be toole 119 ke dakhelesh list haye 68 ta ee darim be soorate random\n",
    "    from random import shuffle\n",
    "    #a = []\n",
    "    #for i in range(simLen): \n",
    "        #randomInd = [j for j in range(sceNum)]\n",
    "        #x = [[i] for i in range(10)] \n",
    "        #shuffle(randomInd)\n",
    "        #a.append(randomInd)\n",
    "        \n",
    "    \n",
    "    a = []\n",
    "    for i in range(simLen): \n",
    "        randomInd = [j for j in range(sceNum)]\n",
    "        #x = [[i] for i in range(10)] \n",
    "        for x in range(int(round(Xfactor1))):\n",
    "            shuffle(randomInd)\n",
    "        a.append(randomInd)\n",
    "    \n",
    "        \n",
    "    #print('end!')\n",
    "\n",
    "    cT = []\n",
    "    RCP = RCPNames\n",
    "    columnsDfOdd = ['sc_' + RCP + str(k)  for k in range(1, sceNum+1,1)] \n",
    "    columnsDfEven = [\"\"] * sceNum\n",
    "\n",
    "    columnsDftmp = []\n",
    "    #colOdd = ['Scr_' + str(i) for i in range(1, sceNum+1, 1)]\n",
    "    #colEven = ['' for i in range(1, sceNum+1,1)]\n",
    "\n",
    "    for i in range (sceNum):\n",
    "        columnsDftmp.append(columnsDfOdd[i])\n",
    "        columnsDftmp.append(columnsDfEven[i])\n",
    "\n",
    "\n",
    "    #### OR:\n",
    "    #columnsDf = [\"Sr\", \"\"] * sceNum\n",
    "    df1tmp = pd.DataFrame(cT, columns =columnsDftmp)\n",
    "    #df1.to_csv(\"rrrrrrrrmodified1111222.csv\", index = False)\n",
    "\n",
    "\n",
    "    cMax = [[19810101 for p in range(sceNum)]]\n",
    "    cMin = [[\"\" for p in range(sceNum)]]\n",
    "    i = 0\n",
    "    p = 1\n",
    "    for m in range(firstYear, lastYear + 1, 1):\n",
    "        if is_leap(m):\n",
    "            for j in range(((m - firstYear)*365+p) , ((m - firstYear)*365+367+p-1), 1):\n",
    "                cMax.append(dftmp[dftmpColMax[a[i]]].iloc[j].values)\n",
    "                cMin.append(dftmp[dftmpColMin[a[i]]].iloc[j].values) \n",
    "    \n",
    "            i += 1\n",
    "\n",
    "        else:\n",
    "            for j in range(((m - firstYear)*365+p), ((m - firstYear)*365+366+p-1), 1):\n",
    "                cMax.append(dftmp[dftmpColMax[a[i]]].iloc[j].values)\n",
    "                cMin.append(dftmp[dftmpColMin[a[i]]].iloc[j].values)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        c = []\n",
    "        for y in range(0, len(cMax), 1): # the length of simulation years\n",
    "            for z in range(sceNum): # range(4)\n",
    "                c.append(cMax[y][z])\n",
    "                c.append(cMin[y][z])\n",
    "\n",
    "        cMax.clear()\n",
    "        cMin.clear()\n",
    "\n",
    "        cMain = []\n",
    "        cMain = list(chunks(c, sceNum * 2))\n",
    "        #print(m) # this line show the progress of the work by typing the years of simulation\n",
    "\n",
    "    ### Should be checked\n",
    "\n",
    "        dfnewtmp = 'dftmp' + str(m)\n",
    "        #columnsDf = [\"Sr\", \"\"]*sceNum\n",
    "        #columnsDf = [['sc_' + str(k), \"\"] for k in range(1, sceNum+1,1)] \n",
    "        dfnewtmp = pd.DataFrame(cMain, columns =columnsDftmp)\n",
    "        c.clear()\n",
    "        df1tmp = df1tmp.append(dfnewtmp, ignore_index=True)\n",
    "        \n",
    "    return df1tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1.3. *Function that calls the random_pcp and random_tmp for all stations of a Ski resort*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomness_pcp_tmp(fnames, Xfactor1):\n",
    "    for f in fnames:\n",
    "        if 'p.csv' in f:\n",
    "            print('Writing pcp files started!')\n",
    "            #df = pd.read_csv('47-0625000_8-6666667p.csv')\n",
    "            dfpcp = pd.read_csv(f)\n",
    "\n",
    "\n",
    "            filt1 = dfpcp.columns.str.contains('RCP26|_26_') #12\n",
    "            filt2 = dfpcp.columns.str.contains('RCP45|_45_') #25\n",
    "            filt3 = dfpcp.columns.str.contains('RCP85|_85_') #31\n",
    "\n",
    "            dfpcpRCP26 = dfpcp.loc[:, filt1]\n",
    "            dfpcpRCP45 = dfpcp.loc[:, filt2]\n",
    "            dfpcpRCP85 = dfpcp.loc[:, filt3]\n",
    "\n",
    "            dfpcpRCP26_n = random_pcp(dfpcpRCP26, 1981, 2099, 12, '26_', Xfactor1)\n",
    "            dfpcpRCP45_n = random_pcp(dfpcpRCP45, 1981, 2099, 25, '45_', Xfactor1)\n",
    "            dfpcpRCP85_n = random_pcp(dfpcpRCP85, 1981, 2099, 31, '85_', Xfactor1)\n",
    "\n",
    "\n",
    "            result = pd.concat([dfpcpRCP26_n, dfpcpRCP45_n, dfpcpRCP85_n], axis=1, sort=False)\n",
    "            #result.to_csv('47-0625000_8-6666667p_n1.csv', index = False)\n",
    "\n",
    "\n",
    "            #newName = 'n_'+ f\n",
    "            newName = f\n",
    "            #filepath = os.path.join(os.getcwd(), newName)\n",
    "            root = os.getcwd()\n",
    "            \n",
    "            '''This part makes a new dir for outouts''' ## should be cooment out later\n",
    "            #if os.path.isdir(os.path.join(root, 'Outputs_randomness')):\n",
    "                #pass\n",
    "            #else: os.mkdir(os.path.join(root, 'Outputs_randomness'))\n",
    "\n",
    "            #outfolder = os.path.join(os.getcwd(), 'Outputs_randomness')\n",
    "            outfolder =os.path.join(os.getcwd()) # we want the results to be over written\n",
    "\n",
    "            filepath = os.path.join(outfolder, newName)\n",
    "\n",
    "            result.to_csv(filepath, index = False)\n",
    "            print('End of writing pcp files!')\n",
    "            #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "        elif 't.csv' in f:\n",
    "            print('Writing tmp files started!')\n",
    "            dftmp = pd.read_csv(f)\n",
    "            dftmpCol = list(dftmp.columns)\n",
    "\n",
    "            filt1_max = [dftmpCol.index(s) for s in dftmpCol if (\"_26_\") in s or (\"RCP26\") in s]\n",
    "            filt2_max = [dftmpCol.index(s) for s in dftmpCol if (\"_45_\") in s or (\"RCP45\") in s]\n",
    "            filt3_max = [dftmpCol.index(s) for s in dftmpCol if (\"_85_\") in s or (\"RCP85\") in s]\n",
    "\n",
    "            aOnefilt1= [1]*len(filt1_max)\n",
    "            aOnefilt2= [1]*len(filt2_max)\n",
    "            aOnefilt3= [1]*len(filt3_max)\n",
    "\n",
    "            filt1_min = list(map(add, filt1_max, aOnefilt1)) # \n",
    "            filt2_min = list(map(add, filt2_max, aOnefilt2))\n",
    "            filt3_min = list(map(add, filt3_max, aOnefilt3))\n",
    "\n",
    "            filt1Tot = []\n",
    "            for i in range(len(filt1_max)):\n",
    "                filt1Tot.append(filt1_max[i])\n",
    "                filt1Tot.append(filt1_min[i])\n",
    "\n",
    "            filt2Tot = []\n",
    "            for j in range(len(filt2_max)):\n",
    "                filt2Tot.append(filt2_max[j])\n",
    "                filt2Tot.append(filt2_min[j])\n",
    "\n",
    "            filt3Tot = []\n",
    "            for k in range(len(filt3_max)):\n",
    "                filt3Tot.append(filt3_max[k])\n",
    "                filt3Tot.append(filt3_min[k])\n",
    "\n",
    "            dftmpRCP26 = dftmp[dftmp.columns[filt1Tot]]\n",
    "            dftmpRCP45 = dftmp[dftmp.columns[filt2Tot]]\n",
    "            dftmpRCP85 = dftmp[dftmp.columns[filt3Tot]]\n",
    "\n",
    "            dftmpRCP26_n = random_tmp (dftmpRCP26, 1981, 2099, 12, '26_', Xfactor1)\n",
    "            dftmpRCP45_n = random_tmp (dftmpRCP45, 1981, 2099, 25, '45_', Xfactor1)\n",
    "            dftmpRCP85_n = random_tmp (dftmpRCP85, 1981, 2099, 31, '85_', Xfactor1)\n",
    "\n",
    "            result = pd.concat([dftmpRCP26_n, dftmpRCP45_n, dftmpRCP85_n], axis=1, sort=False)\n",
    "\n",
    "            #ewName = 'n'+f\n",
    "            #ilepath = os.path.join(os.environ.get('HOME'), newName)\n",
    "            #esult.to_csv(filepath, index = False)\n",
    "\n",
    "            #newName = 'n_'+ f\n",
    "            newName = f\n",
    "            #filepath = os.path.join(os.getcwd(), newName)\n",
    "\n",
    "            #outfolder =os.path.join(os.getcwd(), 'Outputs_randomness')\n",
    "            outfolder =os.path.join(os.getcwd()) # we want the results to be over written\n",
    "            \n",
    "            filepath = os.path.join(outfolder, newName)\n",
    "            result.to_csv(filepath, index = False)\n",
    "\n",
    "            print('End of writing tmp files')\n",
    "\n",
    "        else :\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Function for initiating the main dictionary of climate stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dic(a):\n",
    "    '''Function: creating a dictionary for each climate station'''\n",
    "    \n",
    "    a = {}\n",
    "    keys = ['fM', 'iPot', 'rSnow', 'dSnow', 'cPrec', 'dP', 'elev', 'lat', 'long', 'fileName']\n",
    "    a = {key: None for key in keys}\n",
    "    return a\n",
    "\n",
    "def initialize_input_dict (mainFolderSki):\n",
    "    ''' This function returns a dictionary , and addresses of 4 folders'''\n",
    "    \n",
    "    \n",
    "    '''Step 1''' \n",
    "    rootFolder = mainFolderSki\n",
    "    inputFolder = os.path.join(rootFolder,'input')\n",
    "    ablationFolder = os.path.join(inputFolder, 'Ablation')\n",
    "    accumulationFolder = os.path.join(inputFolder, 'Accumulation')\n",
    "    climate_ref_Folder = os.path.join(inputFolder, 'Climate_ref')\n",
    "    climate_Ref_Folder_org = os.path.join(inputFolder, 'Climate_ref_no_randomness_0')\n",
    "    climate_ref_Folder_rand_1 = os.path.join(inputFolder, 'Climate_ref_randomness_1')\n",
    "    climate_ref_Folder_rand_2 = os.path.join(inputFolder, 'Climate_ref_randomness_2')\n",
    "    \n",
    "    '''Step 2: Reading all files names inside the Ablation, Accumulation, and Climate folders'''  \n",
    "    ablationFiles = []\n",
    "    for filename in os.walk(ablationFolder):\n",
    "        ablationFiles = filename[2]\n",
    "    \n",
    "    accumulationFiles = list()\n",
    "    for filename in os.walk(accumulationFolder):\n",
    "        accumulationFiles = filename[2]\n",
    "\n",
    "    climate_ref_Files = list()\n",
    "    for filename in os.walk(climate_ref_Folder):\n",
    "        climate_ref_Files = filename[2]\n",
    "        \n",
    "        \n",
    "    '''Step 3: Reading files inside ablation folder '''\n",
    "    os.chdir(ablationFolder)\n",
    "    with open(ablationFiles[0], 'r') as file:\n",
    "        FM1 = file.read()\n",
    "    with open(ablationFiles[1], 'r') as file:\n",
    "        Ipot1 = file.read()\n",
    "    with open(ablationFiles[2], 'r') as file:\n",
    "        Rsnow1 = file.read()\n",
    "        \n",
    "        \n",
    "    '''Step 4: Reading the lines of files inside ablation folder'''\n",
    "    FM1 = FM1.replace('\\n', '\\t')\n",
    "    FM1 = FM1.split('\\t')\n",
    "    Ipot1 = Ipot1.replace('\\n', '\\t').split('\\t')\n",
    "    Rsnow1 = Rsnow1.replace('\\n', '\\t').split('\\t')\n",
    "        \n",
    "        \n",
    "    '''Step 5: Reading the lines of files inside accumulation folder''' \n",
    "    os.chdir(accumulationFolder)\n",
    "    \n",
    "    with open(accumulationFiles[0], 'r') as file:\n",
    "        cPrec = file.read()\n",
    "    with open(accumulationFiles[1], 'r') as file:\n",
    "        dSnow1 = file.read()\n",
    "    \n",
    "    cPrec = cPrec.replace('\\n', '\\t')\n",
    "    cPrec = cPrec.split('\\t')\n",
    "    dSnow1 = dSnow1.replace('\\n', '\\t').split('\\t')\n",
    "    \n",
    "    \n",
    "    '''Step 6: Reading the lines of files inside climate folder''' \n",
    "    os.chdir(climate_ref_Folder)\n",
    "    \n",
    "    with open('pcp.txt', 'r') as file:\n",
    "        pcpData = file.read()\n",
    "    with open('tmp.txt', 'r') as file:\n",
    "        tmpData = file.read()\n",
    "        \n",
    "    pcpData = pcpData.split('\\n')\n",
    "    \n",
    "    for i in range(len(pcpData)):\n",
    "        pcpData[i] = pcpData[i].split(',')\n",
    "        \n",
    "        \n",
    "    '''Step 7: Initialazing the input dictionary of climate stations which holds the information of accumulation\n",
    "     and ablation, and etc of the stations''' \n",
    "    nameStn = []\n",
    "    for file in climate_ref_Files:\n",
    "        if 'p.csv' in file:\n",
    "            #nameStn.append('n_' + file[-25: -5])\n",
    "            nameStn.append(file[-25: -5])\n",
    "\n",
    "    stnDicts = []\n",
    "    for i in range(len(nameStn)):\n",
    "        stnDicts.append(create_dic(nameStn[i]))\n",
    "    \n",
    "    \n",
    "    '''Step 8: Assigning the file names to the dictionary'''\n",
    "    for i in range (len(nameStn)):\n",
    "        stnDicts[i]['fileName'] = nameStn[i]\n",
    "\n",
    "    \n",
    "    '''Step 9: Assigning the accumulation and ablation values'''\n",
    "    for stnDict in stnDicts:\n",
    "        for i, element in enumerate(FM1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['fM'] = FM1[i+1]\n",
    "                \n",
    "        for i, element in enumerate(Ipot1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['iPot'] = Ipot1[i+1]\n",
    "\n",
    "        for i, element in enumerate(Rsnow1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:  \n",
    "                stnDict['rSnow'] = Rsnow1[i+1]\n",
    "\n",
    "        for i, element in enumerate(dSnow1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['dSnow'] = dSnow1[i+1]\n",
    "\n",
    "        for i, element in enumerate(cPrec):\n",
    "            stnDict['cPrec'] = cPrec[1]\n",
    "            stnDict['dP'] = cPrec[3]\n",
    "            \n",
    "    '''Step 10: Assigning the elevation, Lat and long to the dictionaries'''\n",
    "    for i in range(len(stnDicts)):\n",
    "        for j in range(1, len(pcpData)):\n",
    "            \n",
    "            #if pcpData[j][1][2:-1] == stnDicts[i]['fileName'][2:]:\n",
    "            if pcpData[j][1][:-1] == stnDicts[i]['fileName'][:]:\n",
    "                stnDicts[i]['lat']= pcpData[j][2]\n",
    "                stnDicts[i]['long']= pcpData[j][3]\n",
    "                stnDicts[i]['elev']= pcpData[j][4]\n",
    "                \n",
    "    return stnDicts, inputFolder, ablationFolder, accumulationFolder, climate_ref_Folder, climate_Ref_Folder_org, \\\n",
    "climate_ref_Folder_rand_1, climate_ref_Folder_rand_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Main Snow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3.1 *Initializiing the main dictionary for a case study*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseStudyStns = {}\n",
    "inputFolder = ''\n",
    "ablationFolder = ''\n",
    "accumulationFolder = ''\n",
    "climateFolder = ''\n",
    "climateFolder_org = ''\n",
    "climateFolder1 = ''\n",
    "climateFolder2 = ''\n",
    "#root = 'C:/Users/ashrafse/SA_2/snowModelUZH/case2_Atzmaening'\n",
    "#root = 'C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations'\n",
    "root = r'C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2'\n",
    "#root = r'C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case6_davos_elevations_b2584'\n",
    "\n",
    "## calling the function with multiple return values\n",
    "caseStudyStns, inputFolder, ablationFolder, accumulationFolder, climateFolder, climateFolder_org, \\\n",
    "climateFolder1, climateFolder2 = initialize_input_dict(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\input\\Climate_ref\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\input\\Climate_ref_no_randomness_0\n",
      "C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\input\\Climate_ref\n"
     ]
    }
   ],
   "source": [
    "os.chdir(climateFolder_org)\n",
    "src = os.getcwd()\n",
    "os.chdir(climateFolder)\n",
    "dst = os.getcwd()\n",
    "#copytree(src, dst)\n",
    "print(src)\n",
    "print(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3.2 *Check if we have initialized correctly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\input\\Climate_ref\n",
      "C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\input\\Climate_ref_randomness_2\n",
      "C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\input\\Climate_ref_no_randomness_0\n",
      "C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\input\\Climate_ref_randomness_1\n"
     ]
    }
   ],
   "source": [
    "print(climateFolder)\n",
    "print(climateFolder2)\n",
    "print(climateFolder_org)\n",
    "print(climateFolder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fM': '1.011',\n",
       "  'iPot': '1000',\n",
       "  'rSnow': '0.5',\n",
       "  'dSnow': '0.5',\n",
       "  'cPrec': '0',\n",
       "  'dP': '0',\n",
       "  'elev': '1755',\n",
       "  'lat': '47.00',\n",
       "  'long': '8.7708333',\n",
       "  'fileName': '47-0000000_8-7708333'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseStudyStns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.011'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseStudyStns[0].get(\"fM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the X variables which control modeling (hyper parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2fM = caseStudyStns[0].get(\"fM\") # change 0 to i for all stations\n",
    "X3iPot = caseStudyStns[0].get(\"iPot\")\n",
    "X4rSnow =  caseStudyStns[0].get(\"rSnow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.011\n",
      "1000\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(X2fM)\n",
    "print(X3iPot)\n",
    "print(X4rSnow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3.3 *Function that runs the main model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copytree(src, dst, symlinks=False, ignore=None):\n",
    "    for item in os.listdir(src):\n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "        else:\n",
    "            shutil.copy2(s, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1st column as index: makaing date from 01 01 1981 to 2099 12 31\n",
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date ).days + 1)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "def policy_release1(x1SnowThershold):\n",
    "    return x1SnowThershold\n",
    "\n",
    "\n",
    "### OR Let's make this function in a more OOP way:\n",
    "class Policy_Ski:\n",
    "    def __init__(self, x1SnowThershold):\n",
    "        self.x1SnowThershold = x1SnowThershold\n",
    "        \n",
    "    def policy_release2(self):\n",
    "        return(self.x1SnowThershold)\n",
    "    \n",
    "    def policy_release3(self):\n",
    "        ''' this function should make a matrix of evaluation fot the condition of 100 day ay minimum condition'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Economic_Model_Ski:\n",
    "    def __init__(self, xCostDay, xRevenueDay):\n",
    "        self.costDayFixed = xCostDay\n",
    "        self.revenueDayFixed = xRevenueDay\n",
    "        \n",
    "    def economic_costDay(self):\n",
    "        return(self.costDayFixed)\n",
    "    \n",
    "    def economic_revenueDay(self):\n",
    "        return(self.revenueDayFixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCP_Model:\n",
    "    def __init__(self, xRCP, xClimateModel):\n",
    "        self.input1 = round(xRCP)\n",
    "        self.input2 = xClimateModel  \n",
    "        \n",
    "    def rcpGenerator(self):\n",
    "        if self.input1 == 1:\n",
    "            RCP = str(2.6)\n",
    "            rcpInt = 1\n",
    "        if self.input1 == 2:\n",
    "            RCP = str(4.5)\n",
    "            rcpInt = 2\n",
    "        if self.input1 == 3:\n",
    "            RCP = str(8.5)\n",
    "            rcpInt = 3\n",
    "        return(RCP, rcpInt)\n",
    "\n",
    "    \n",
    "    def climateModel(self):\n",
    "        a, b = RCP_Model.rcpGenerator(self)\n",
    "        \n",
    "        if b == 1:\n",
    "            climateModel = round(self.input2*11)\n",
    "            \n",
    "        elif b == 2:\n",
    "            climateModel = 11 + max(1,round(self.input2*25))\n",
    "            \n",
    "        else:\n",
    "            climateModel = 36 + max(1, round(self.input2*31))\n",
    "            \n",
    "        return (int(climateModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['47-0000000_8-7708333p.csv', '47-0000000_8-7708333t.csv', 'pcp.txt', 'tmp.txt']\n"
     ]
    }
   ],
   "source": [
    "os.chdir(climateFolder)\n",
    "fnames = os.listdir()\n",
    "print(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RCP and Climate Model Controler'''\n",
    "#rcp_Model = RCP_Model(1.545008,0.5991817)\n",
    "rcp_Model2 = RCP_Model(1.545008,0.00909991817)\n",
    "RCP11, intRCP11 = rcp_Model2.rcpGenerator()\n",
    "climateModel11 = rcp_Model2.climateModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5\n",
      "2\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(RCP11)\n",
    "print(intRCP11)\n",
    "print(climateModel11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n"
     ]
    }
   ],
   "source": [
    "xClimateRandomness = round(5.888)\n",
    "\n",
    "if (xClimateRandomness == 1):\n",
    "    os.chdir(climateFolder_org)\n",
    "    src = os.getcwd()\n",
    "    os.chdir(climateFolder)\n",
    "    dst = os.getcwd()\n",
    "    copytree(src, dst)\n",
    "    print('A')\n",
    "elif (xClimateRandomness == 2) :\n",
    "    os.chdir(climateFolder1)\n",
    "    src = os.getcwd()\n",
    "    os.chdir(climateFolder)\n",
    "    dst = os.getcwd()\n",
    "    copytree(src, dst)\n",
    "    print('B')\n",
    "else:\n",
    "    os.chdir(climateFolder2)\n",
    "    src = os.getcwd()\n",
    "    os.chdir(climateFolder)\n",
    "    dst = os.getcwd()\n",
    "    copytree(src, dst)\n",
    "    print('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomness_pcp_tmp(fnames, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLR Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snow_Model (xRCP=None, xClimateModel=None, Xfactor1 = None,  X2fM = None, X3iPot = None, X4rSnow = None, \n",
    "                X5temp = None, X6tempArt = None, xCostDay = None, xRevenueDay = None, x1SnowThershold = None,\n",
    "                xGoodDays = None):\n",
    "    '''' This function controls the Ski resort model in an XLR framework'''\n",
    "    \n",
    "    \n",
    "    ''' VERY IMPORTANT --- Controling the randomness --- VERY IMPORTANT'''\n",
    "    xClimateRandomness = round(Xfactor1)\n",
    "    \n",
    "    if (xClimateRandomness == 1):\n",
    "        os.chdir(climateFolder_org)\n",
    "        src = os.getcwd()\n",
    "        os.chdir(climateFolder)\n",
    "        dst = os.getcwd()\n",
    "        #copytree(src, dst)\n",
    "        print('Original CH2018 is being used')\n",
    "    elif (xClimateRandomness == 2) :\n",
    "        os.chdir(climateFolder1)\n",
    "        src = os.getcwd()\n",
    "        os.chdir(climateFolder)\n",
    "        dst = os.getcwd()\n",
    "        #copytree(src, dst)\n",
    "        print('Random Climate realization version 1 is being used')\n",
    "    else:\n",
    "        os.chdir(climateFolder2)\n",
    "        src = os.getcwd()\n",
    "        os.chdir(climateFolder)\n",
    "        dst = os.getcwd()\n",
    "        #copytree(src, dst)\n",
    "        print('Random Climate realization version 2 is being used')\n",
    "        \n",
    "    os.chdir(climateFolder)\n",
    "    fnames = os.listdir()\n",
    "    #randomness_pcp_tmp(fnames, Xfactor1)\n",
    "    \n",
    "    print('Snow_Model: Matching the station names values with CSV files!')   \n",
    "    '''Matching the station names values in the dictionary of stations with CSV files in Climate folder of the case Study'''\n",
    "    pcpCaseStudy = []\n",
    "    tmpCaseStudy = []\n",
    "\n",
    "    if (xClimateRandomness == 1):\n",
    "        for i in range(len(caseStudyStns)):\n",
    "            pcpCaseStudy.append(os.path.join(climateFolder, caseStudyStns[i]['fileName'] + 'p.csv'))\n",
    "            tmpCaseStudy.append(os.path.join(climateFolder, caseStudyStns[i]['fileName'] + 't.csv'))\n",
    "    elif (xClimateRandomness == 2) :\n",
    "        for i in range(len(caseStudyStns)):\n",
    "            pcpCaseStudy.append(os.path.join(climateFolder1, caseStudyStns[i]['fileName'] + 'p.csv'))\n",
    "            tmpCaseStudy.append(os.path.join(climateFolder1, caseStudyStns[i]['fileName'] + 't.csv'))\n",
    "    else:\n",
    "        for i in range(len(caseStudyStns)):\n",
    "            pcpCaseStudy.append(os.path.join(climateFolder2, caseStudyStns[i]['fileName'] + 'p.csv'))\n",
    "            tmpCaseStudy.append(os.path.join(climateFolder2, caseStudyStns[i]['fileName'] + 't.csv'))\n",
    "        \n",
    "        \n",
    "    print('Snow_Model: Building a database for each csv file (tmp and pcp)!')\n",
    "    \n",
    "    '''Step 6: building a database for each precipitation and temperature file in Climate folder and saving them in a list'''\n",
    "    '''6.1 reading the csv files as databases'''\n",
    "    dfpcp = [None for _ in range(len(pcpCaseStudy))]\n",
    "    dftmp = [None for _ in range(len(tmpCaseStudy))]\n",
    "    for i in range(len(pcpCaseStudy)):\n",
    "        dfpcp[i] = pd.read_csv(pcpCaseStudy[i])\n",
    "        dftmp[i] = pd.read_csv(tmpCaseStudy[i])\n",
    "        \n",
    "    '''6.2 making a header for output files'''\n",
    "    dfpcpCol = dfpcp[0].columns\n",
    "    dftmpCol = dftmp[0].columns\n",
    "    \n",
    "    '''6.3 defining the length of simulations and scenarios'''\n",
    "    scenariosLength = len(dfpcpCol)\n",
    "    simulationLength = len(dftmp[0][dftmpCol[0]]) - 1\n",
    "        \n",
    "    \n",
    "    '''Reading the beginning and end of the simulation''' \n",
    "    start_date = date(1981, 1, 1)\n",
    "    end_date = date(2099, 12, 31)\n",
    "    dateList = []\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        dateList.append(single_date.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "    seasonList = []\n",
    "    for n in range (1981, 2100, 1):\n",
    "        seasonList.append(str(n))\n",
    "    \n",
    "    \n",
    "    print('Snow_Model: Part 1 Running the model, daily output!')\n",
    "\n",
    "    '''################################ PART1 ################################'''\n",
    "    '''Running the model for each climate station:'''\n",
    "    \n",
    "    for k in range(len(caseStudyStns)):\n",
    "        \n",
    "        '''making a header for output files'''\n",
    "        dfpcpCol = dfpcp[k].columns\n",
    "        dftmpCol = dftmp[k].columns\n",
    "\n",
    "        #X2fM = caseStudyStns[k].get(\"fM\") # change 0 to i for all stations\n",
    "        #X3iPot = caseStudyStns[k].get(\"iPot\")\n",
    "        #X4rSnow =  caseStudyStns[k].get(\"rSnow\")\n",
    "        \n",
    "\n",
    "        '''defining the length of simulations and scenarios'''\n",
    "        #scenariosLength = len(dfpcpCol)\n",
    "        scenariosLength = 1\n",
    "        simulationLength = len(dftmp[0][dftmpCol[0]]) - 1\n",
    "\n",
    "\n",
    "        '''declaring the initial arrays'''\n",
    "        accumulation = [0 for _ in range(simulationLength)]\n",
    "        ablation =  [0 for _ in range(simulationLength)]\n",
    "        snowDeposite = [0 for _ in range(simulationLength)]\n",
    "        total = np.zeros([simulationLength, 3*scenariosLength])\n",
    "        \n",
    "        \n",
    "        '''declaring the new variables for financial analyses and temrature Index for artificial snow making'''\n",
    "        artSnowCheck = [0 for _ in range(simulationLength)]\n",
    "        revenue = [0 for _ in range(simulationLength)]\n",
    "        cost = [0 for _ in range(simulationLength)]\n",
    "        profit = [0 for _ in range(simulationLength)]\n",
    "        totalMoney = np.zeros([simulationLength, 4*scenariosLength])\n",
    "\n",
    "\n",
    "        '''RCP and Climate Model Controler'''\n",
    "        rcp_Model = RCP_Model(xRCP, xClimateModel)\n",
    "        RCP, intRCP = rcp_Model.rcpGenerator()\n",
    "        climateModel = rcp_Model.climateModel()\n",
    "        \n",
    "        \n",
    "        '''Running the model for each climate scenario:'''\n",
    "        for j in range(climateModel, climateModel + 1, 1):\n",
    "            \n",
    "            \n",
    "        #for j in range(len(dfpcpCol)):\n",
    "            ## Reading the information and inputs of the first day of simulation\n",
    "            todayPCP = dfpcp[k][dfpcpCol[j]].iloc[1] if (dfpcp[k][dfpcpCol[j]].iloc[1] != -99) else 0\n",
    "            todayTMPMAX = round(dftmp[k][dftmpCol[2*j]].iloc[1],2) if(dftmp[k][dftmpCol[2*j]].iloc[1] != -99) else 0\n",
    "            todayTMPMIN = round(dftmp[k][dftmpCol[2*j+1]].iloc[1],2) if(dftmp[k][dftmpCol[2*j+1]].iloc[1] != -99) else 0\n",
    "            todayTMPAVE = round((todayTMPMAX+todayTMPMIN)/2,2) if((todayTMPMAX+todayTMPMIN)/2 != -99) else 0\n",
    "\n",
    "            \n",
    "            '''Thershold 300 mm\n",
    "            EMA_workbench_controler for the thershold of good snow condition'''\n",
    "            #A = policy_release1(x1SnowThershold)\n",
    "            \n",
    "            \n",
    "            policySkiResort = Policy_Ski(x1SnowThershold) ## 300 mm\n",
    "            snowThershold = policySkiResort.policy_release2()\n",
    "            \n",
    "            \n",
    "            \n",
    "            '''EMA_workbench_controler for the thershold daily fixed revenue and cost expenses'''\n",
    "            economyDaySki = Economic_Model_Ski(xCostDay, xRevenueDay) \n",
    "            revenueDayFixed = economyDaySki.economic_revenueDay()  # self.revenueDayFixed\n",
    "            costDayFixed = economyDaySki.economic_costDay()  # self.costDayFixed\n",
    "            \n",
    "\n",
    "          \n",
    "            '''Accumulation for the first day:'''\n",
    "            if (todayTMPAVE) <= X5temp:\n",
    "                accumulation[0] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*(1)\n",
    "\n",
    "            elif X5temp -1 < (todayTMPAVE) <= X5temp + 1:\n",
    "                accumulation[0] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*float((X5temp + 1 -todayTMPAVE)/2)\n",
    "\n",
    "            else: accumulation[0] = 0\n",
    "\n",
    "\n",
    "            '''Ablation for the first day:'''\n",
    "            if todayTMPAVE <= X5temp:\n",
    "                 ablation[0] = 0\n",
    "            else: \n",
    "                #ablation[0] = (float(caseStudyStns[k]['fM']) + float(caseStudyStns[k]['rSnow'])*float(caseStudyStns[k]['iPot'])*0.001)*float(todayTMPAVE)*(1+0)\n",
    "                ablation[0] = (float(X2fM) + float(X4rSnow)*float(X3iPot)*0.001)*float(todayTMPAVE)*(1+0)\n",
    "\n",
    "                \n",
    "            '''Main mass balance equation for the first day:'''\n",
    "            snowDeposite[0] = 0 if (0 + accumulation[0] - ablation[0]) < 0 else (0 + accumulation[0] - ablation[0])\n",
    "\n",
    "            \n",
    "            '''storing three values in a list for the first day'''\n",
    "            #total[0,3*j+0] = round((accumulation[0] - ablation[0]), 2)\n",
    "            #total[0,3*j+1] = round(snowDeposite[0], 2)\n",
    "            #total[0,3*j+2] = 1 if (total[0,3*j+1] > snowThershold) else total[0,3*j+1] / snowThershold\n",
    "            \n",
    "            total[0,0] = round((accumulation[0] - ablation[0]), 2)\n",
    "            total[0,1] = round(snowDeposite[0], 2)\n",
    "            total[0,2] = 1 if (total[0,1] > snowThershold) else total[0,1] / snowThershold\n",
    "\n",
    "\n",
    "            \n",
    "            '''Check the posiibility of Snow Making'''\n",
    "            if (todayTMPAVE) <= X6tempArt:\n",
    "                artSnowCheck[0] = 1\n",
    "            \n",
    "            elif X6tempArt < (todayTMPAVE) <= X6tempArt + 2:\n",
    "                artSnowCheck[0] = 1 * float((X6tempArt + 2 -todayTMPAVE)/2)\n",
    "                \n",
    "            else:\n",
    "                artSnowCheck[0] = 0\n",
    "                \n",
    "                \n",
    "            '''Revenue and financial status'''\n",
    "            #revenue[0] = round((total[0,3*j+2] * 10), 2)\n",
    "            revenue[0] = float(round(revenueDayFixed,2)) if (total[0,2] > 0.6 * snowThershold ) else float(round(revenueDayFixed,2))*float((total[0,2] / snowThershold))\n",
    "            \n",
    "            '''Cost'''\n",
    "            #cost[0] = round((revenue[0] * 0.4) , 2)\n",
    "            #cost[0] = float(round(costDayFixed, 2)) if (total[0,3*j+2] > snowThershold ) else float(round(costDayFixed,2))*float((total[0,3*j+2] / snowThershold)) \n",
    "            #cost[0] = float(round(costDayFixed, 2)) if (total[0,3*j+2] > snowThershold else float(round(costDayFixed,2))*float((total[0,3*j+2] / snowThershold))\n",
    "            cost[0] = float(round(costDayFixed, 2)) if (total[0,2] > snowThershold ) else float(round(costDayFixed,2))*float((total[0,2] / snowThershold))\n",
    "                                                        \n",
    "            '''Profit'''\n",
    "            profit[0] = revenue[0] - cost[0]\n",
    "            \n",
    "\n",
    "            '''Storing the artificial snow possibility check and financial situation'''\n",
    "            #totalMoney[0,1*j+0] = round(100.345, 2)\n",
    "            totalMoney[0,0] = round(artSnowCheck[0], 2)\n",
    "            totalMoney[0,1] = round(revenue[0], 2)\n",
    "            totalMoney[0,2] = round(cost[0], 2)\n",
    "            totalMoney[0,3] = round(profit[0], 2)\n",
    "            \n",
    "            \n",
    "            '''For the SECOND DAY to the End of Simulation:'''\n",
    "            i = 0\n",
    "            for i in range(2, simulationLength + 1, 1):\n",
    "                '''# precipitation and temperature missing values were handled'''\n",
    "                todayPCP = dfpcp[k][dfpcpCol[j]].iloc[i] if (dfpcp[k][dfpcpCol[j]].iloc[i] != -99) else 0\n",
    "                todayTMPMAX = round(dftmp[k][dftmpCol[2*j]].iloc[i],2) if(dftmp[k][dftmpCol[2*j]].iloc[i] != -99) else 0\n",
    "                todayTMPMIN = round(dftmp[k][dftmpCol[2*j+1]].iloc[i],2) if(dftmp[k][dftmpCol[2*j+1]].iloc[i] != -99) else 0\n",
    "                todayTMPAVE = round((todayTMPMAX+todayTMPMIN)/2,2) if((todayTMPMAX+todayTMPMIN)/2 != -99) else 0\n",
    "\n",
    "                '''### Accumulation :'''\n",
    "                if(todayTMPAVE) <= X5temp:\n",
    "                    ##\n",
    "                    accumulation[i-1] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*(1)\n",
    "\n",
    "                elif X5temp -1 < (todayTMPAVE) <= X5temp + 1:\n",
    "                    accumulation[i-1] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*float((X5temp + 1 -todayTMPAVE)/2)\n",
    "\n",
    "                else: accumulation[i-1] = 0\n",
    "\n",
    "                '''### Ablation :'''\n",
    "                if todayTMPAVE <= X5temp:\n",
    "                    ablation[i-1] = 0\n",
    "                else: \n",
    "                    #ablation[i-1] = (float(caseStudyStns[k]['fM']) + float(caseStudyStns[k]['rSnow'])*float(caseStudyStns[k]['iPot'])*0.001)*float(todayTMPAVE)*(1+0)\n",
    "                    ablation[i-1] = (float(X2fM) + float(X4rSnow)*float(X3iPot)*0.001)*float(todayTMPAVE)*(1+0)\n",
    "\n",
    "                '''### Main mass balance equation for second day to the end of simulation:'''\n",
    "                snowDeposite[i-1] = 0 if (snowDeposite[i-2] + accumulation[i-1] - ablation[i-1]) < 0 else (snowDeposite[i-2] + accumulation[i-1] - ablation[i-1])\n",
    "\n",
    "\n",
    "                '''### storing three values in a list''' \n",
    "                total[i-1,0] = round((accumulation[i-1] - ablation[i-1]) , 2)\n",
    "                total[i-1,1] = round(snowDeposite[i-1], 2)\n",
    "                #total[i-1,3*j+2] = 1 if (total[i-1,3*j+1] > A) else 0\n",
    "                total[i-1,2] = 1 if (total[i-1,1] > snowThershold) else total[i-1,1] / snowThershold\n",
    "\n",
    "                \n",
    "                ## 2020/06/22\n",
    "                '''Check the posiibility of Snow Making'''\n",
    "                if (todayTMPAVE) <= X6tempArt:\n",
    "                    artSnowCheck[i-1] = 1\n",
    "\n",
    "                elif X6tempArt < (todayTMPAVE) <= X6tempArt + 2:\n",
    "                    artSnowCheck[i-1] = 1 * float((X6tempArt + 2 -todayTMPAVE)/2)\n",
    "\n",
    "                else:\n",
    "                    artSnowCheck[i-1] = 0\n",
    "                \n",
    "                \n",
    "                '''Revenue'''\n",
    "                #revenue[i-1] = round((total[i-1,3*j+2] * 10), 2)\n",
    "                revenue[i-1] = float(round(revenueDayFixed,2)) if (total[i-1,2] > 0.6 * snowThershold ) else float(round(revenueDayFixed,2))*float(total[i-1,2] / (0.6 *snowThershold))\n",
    "                \n",
    "                '''Cost'''                                                                                                                              #cost[i-1] = round((revenue[i-1] * 0.4) , 2)\n",
    "                #cost[i-1] = float(round(costDayFixed, 2)) if (total[i-1,3*j+2] >  snowThershold ) else float(round(costDayFixed,2))*float((total[i-1,3*j+2] /  snowThershold)) \n",
    "                #cost[i-1] = float(round(costDayFixed, 2))\n",
    "                cost[i-1] = float(round(costDayFixed, 2)) if (total[i-1,2] >  snowThershold ) else float(round(costDayFixed,2))*float((total[i-1,2] /  snowThershold)) \n",
    "                \n",
    "                '''Profit'''\n",
    "                profit[i-1] = revenue[i-1] - cost[i-1]\n",
    "\n",
    "                \n",
    "                '''Storing the artificial snow possibility and financial situation'''\n",
    "                totalMoney[i-1,0] = round(artSnowCheck[i-1], 2)\n",
    "                totalMoney[i-1,1] = round(revenue[i-1], 2)\n",
    "                totalMoney[i-1,2] = round(cost[i-1], 2)\n",
    "                totalMoney[i-1,3] = round(profit[i-1], 2)\n",
    "                             \n",
    "\n",
    "        '''Saving the Outputs of total list in a CSV file in a specific path'''\n",
    "\n",
    "        ## 1st row as the column names:\n",
    "        \n",
    "        columnsDF = []\n",
    "        columnsDF_aerSnowCheck = []\n",
    "        \n",
    "        #for col in dfpcpCol[j]:\n",
    "        #    columnsDF.append('SnowAmount_' + col)\n",
    "        #    columnsDF.append('TotalSnowAmount_' + col)\n",
    "        #    columnsDF.append('isOverSnow_' + col)\n",
    "        #    columnsDF_aerSnowCheck.append('ArtSnowPossibility_' + col)\n",
    "        #    columnsDF_aerSnowCheck.append('Revenue_' + col)\n",
    "        #    columnsDF_aerSnowCheck.append('Cost_' + col)\n",
    "        #    columnsDF_aerSnowCheck.append('Money_' + col)\n",
    "        \n",
    "        \n",
    "        #nameHeader = dfpcpCol[int(xClimateModel)]\n",
    "        nameHeader = dfpcpCol[climateModel]\n",
    "\n",
    "        \n",
    "        columnsDF.append('SnowAmount_' + nameHeader)\n",
    "        columnsDF.append('TotalSnowAmount_' + nameHeader)\n",
    "        columnsDF.append('isOverSnow_' + nameHeader)\n",
    "        columnsDF_aerSnowCheck.append('ArtSnowPossibility_')\n",
    "        columnsDF_aerSnowCheck.append('Revenue_' + nameHeader)\n",
    "        columnsDF_aerSnowCheck.append('Cost_' + nameHeader)\n",
    "        columnsDF_aerSnowCheck.append('Money_' + nameHeader)\n",
    "         \n",
    "        \n",
    "        '''Snow daily'''\n",
    "        columnsDF0 = ['DATE']\n",
    "        dfnew0 = pd.DataFrame(dateList, columns = columnsDF0)\n",
    "        dfnew1 = pd.DataFrame(total, columns = columnsDF)\n",
    "        df1 = pd.concat([dfnew0, dfnew1], axis=1, sort=False)\n",
    "        \n",
    "        '''Money and Artifical Snow'''\n",
    "        dfnew2 = pd.DataFrame(totalMoney, columns = columnsDF_aerSnowCheck)\n",
    "        df2 = pd.concat([dfnew0, dfnew2], axis=1, sort=False)\n",
    "        \n",
    "\n",
    "        if os.path.isdir(os.path.join(root, 'Outputs_py')):\n",
    "            pass\n",
    "        else: os.mkdir(os.path.join(root, 'Outputs_py'))\n",
    "\n",
    "            \n",
    "        '''daile Snow Outputs'''\n",
    "        outfolder =os.path.join(root, 'Outputs_py') \n",
    "        outfileName = 'Total_daily_' + caseStudyStns[k]['fileName'] + '.csv'\n",
    "        outputFile = os.path.join(outfolder, outfileName )\n",
    "        df1.to_csv(outputFile, index = False)\n",
    "        \n",
    "        \n",
    "        '''Artificial Snow and Financial Outputs'''\n",
    "        outfileName2 = 'Total_Moneydaily_' + caseStudyStns[k]['fileName'] + '.csv'\n",
    "        outputFile2 = os.path.join(outfolder, outfileName2)\n",
    "        df2.to_csv(outputFile2, index = False)\n",
    "        #return df1, df2\n",
    "        \n",
    "        print('End of Part 1 Calculations!')\n",
    "        \n",
    "        '''################################ PART2 ################################'''\n",
    "        '''##### PART 2 seasonal outputs Tipping points and Liklihood of Survival#####'''\n",
    "        \n",
    "        print('Snow_Model: Starting Part 2, Running the model, seasonal outputs, reading files!')\n",
    "        \n",
    "        #### 2020/06/10 ####\n",
    "        total_Daily_FilesAll = list()\n",
    "        total_Daily_Files = []\n",
    "        \n",
    "        #### 2020/06/22 ####\n",
    "        total_Money_Files = []\n",
    "        \n",
    "        for filename in os.walk(outfolder):\n",
    "            total_Daily_FilesAll = filename[2]\n",
    "\n",
    "        for bIndex in range (len(total_Daily_FilesAll)):        \n",
    "            if 'Moneydaily_' in total_Daily_FilesAll[bIndex]:\n",
    "                total_Money_Files.append(total_Daily_FilesAll[bIndex])\n",
    "            elif 'Total_daily_' in total_Daily_FilesAll[bIndex]:\n",
    "                total_Daily_Files.append(total_Daily_FilesAll[bIndex])\n",
    "            else: continue\n",
    "                \n",
    "                            \n",
    "        \n",
    "        '''##Adding the whole address of directory to the name of total daily snow files'''\n",
    "        totalFiles = []\n",
    "        for i in range(len(total_Daily_Files)):\n",
    "            totalFiles.append(os.path.join(outfolder, total_Daily_Files[i]))\n",
    "        \n",
    "                \n",
    "        '''##Adding the whole address of directory to the name of total daily money files'''\n",
    "        totalMoneyFiles = []\n",
    "        for i in range(len(total_Money_Files)):\n",
    "            totalMoneyFiles.append(os.path.join(outfolder, total_Money_Files[i]))\n",
    "        \n",
    "        \n",
    "        print('Snow Model: Continuing of Part 2, Seasonal Outputs, Performing  Tipping Points Analyses!')\n",
    "        \n",
    "        \n",
    "        ## databases are read here: \n",
    "        dfSeason = [ None for _ in range(len(totalFiles))]\n",
    "        \n",
    "        ##2020/06/22\n",
    "        dfSeasonMoney = [ None for _ in range(len(totalMoneyFiles))]\n",
    "        \n",
    "        \n",
    "        ##Here we calcluate seasonal tipping points here\n",
    "        for i in range(len(totalFiles)):\n",
    "            dfSeason[i] = pd.read_csv(totalFiles[i], low_memory=False)\n",
    "\n",
    "            \n",
    "            start_date = date(1981, 1, 1)\n",
    "            end_date = date(2099, 12, 31)\n",
    "            dateList = []\n",
    "            for single_date in daterange(start_date, end_date):\n",
    "                dateList.append(single_date.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "            start_season = []\n",
    "            end_season = []\n",
    "\n",
    "            for pp in range (1981, 2099, 1):\n",
    "                start_season.append(date(pp, 11, 1))\n",
    "                end_season.append(date(pp+1, 4, 30))\n",
    "\n",
    "            df2 = dfSeason[i]\n",
    "            df2.set_index('DATE', inplace = True)\n",
    "            df2Col = df2.columns\n",
    "\n",
    "            df2ColCal = []\n",
    "            \n",
    "            for m in range(1):\n",
    "            #for m in range(68):\n",
    "                df2ColCal.append(df2Col[3*m+2])\n",
    "\n",
    "            sumGoodCondition = np.zeros([len(start_season), len(df2ColCal)])\n",
    "            #sumRows = np.zeros(len(df2ColCal))  ### Saeed  2020/06/11\n",
    "            sumRows = np.zeros(len(start_season)) ### Saeed  2020/08/17\n",
    " \n",
    "            for j in range(len(df2ColCal)):\n",
    "                for k in range(len(start_season)):\n",
    "                #for i in range(3):\n",
    "                    start_date = start_season[k]\n",
    "                    end_date = end_season[k]\n",
    "                        #start_date = date(1981, 1, 2)\n",
    "                        #end_date = date(1981, 1, 10)\n",
    "                    for single_date in daterange(start_date, end_date):\n",
    "                        sumGoodCondition[k,j] += df2[df2ColCal[j]].loc[single_date.strftime(\"%m/%d/%Y\")]\n",
    "                    #sumRows[j] +=  sumGoodCondition[k,j] ### Saeed  2020/06/11\n",
    "                    sumRows[k] +=  sumGoodCondition[k,j]  ### Saeed  2020/08/17\n",
    "                                                                                                                                                                                                                                                                 \n",
    "            #AveragesumRows = np.average(sumRows/len(df2ColCal))\n",
    "            #AveragesumRows: the averange nmber of days in a season with good snow condition(season 1981-1982 to 2098-2099)                                                                                                                         \n",
    "            #AveragesumRows = np.average(sumRows/118)  ## Saeed 2020/07/31\n",
    "            AveragesumRows = np.average(sumRows) ## Saeed 2020/08/17\n",
    "            df3 = pd.DataFrame(sumGoodCondition, columns = df2ColCal)\n",
    "\n",
    "\n",
    "            firstCol = []\n",
    "            for o in range (len(seasonList)-1):\n",
    "                firstCol.append(seasonList[o] +'-' + seasonList[o+1])\n",
    "\n",
    "            columnsDF1 = ['Season']\n",
    "            dfnew3 = pd.DataFrame(firstCol, columns = columnsDF1)\n",
    "\n",
    "            dfFinalSeason = pd.concat([dfnew3, df3], axis=1, sort=False)          \n",
    "            \n",
    "            if os.path.isdir(os.path.join(root, 'outSeason')):\n",
    "                pass\n",
    "            else: \n",
    "                os.mkdir(os.path.join(root, 'outSeason'))\n",
    "            \n",
    "            outfileNameSeason = 'season_' + total_Daily_Files[i]\n",
    "            outFolderSeason = os.path.join(root, 'outSeason')\n",
    "            outputFileSeason = os.path.join(outFolderSeason, outfileNameSeason)\n",
    "            \n",
    "            outFilesFinal = []\n",
    "            for filename in os.walk(outFolderSeason):\n",
    "                outFilesFinal = filename[2]\n",
    "                iii = len(outFilesFinal)\n",
    "                if os.path.isfile(outputFileSeason):\n",
    "                    newOutFileNameSeason = outputFileSeason[0 : -4] + '_' + str(iii) + '.csv'\n",
    "                    dfFinalSeason.to_csv(newOutFileNameSeason, index = False)\n",
    "                else: \n",
    "                    dfFinalSeason.to_csv(outputFileSeason, index = False)\n",
    "            \n",
    "            \n",
    "            \n",
    "            print('Snow Model: Continuing of Part 2, Seasonal Outputs, Likelihood Analyses!')\n",
    "            \n",
    "            df_sum_ch2018 = dfFinalSeason\n",
    "            ### We transfer the data to a Matrix dfFinalSeason\n",
    "            df_sum_ch2018_M = df_sum_ch2018.iloc[0: , 1:]\n",
    "            df_sum_ch2018_Matrix = df_sum_ch2018_M.to_numpy()\n",
    "            df_sum_ch2018_M_Columns= df_sum_ch2018_M.columns\n",
    "\n",
    "\n",
    "            ## We initialize the Matrix of Survival\n",
    "            #reportMatrix = np.zeros((118, 68))\n",
    "            reportMatrix = np.zeros((118, 1))\n",
    "\n",
    "            ## We Calculate the Chance of Survival\n",
    "            xGoodDays_Condiion  = xGoodDays\n",
    "            for j in range (len(df_sum_ch2018_M_Columns)):\n",
    "                for iii in range(len(df_sum_ch2018_M[df_sum_ch2018_M_Columns[0]])):\n",
    "                    if df_sum_ch2018_M[df_sum_ch2018_M_Columns[j]][iii] < xGoodDays:\n",
    "                        reportMatrix[iii,j] = (df_sum_ch2018_M[df_sum_ch2018_M_Columns[j]][iii] / xGoodDays) * 100\n",
    "                    else:\n",
    "                        reportMatrix[iii,j] = 100\n",
    "\n",
    "                        \n",
    "            AveragereportMatrix = np.average(reportMatrix/118)\n",
    "            #we sevae the results in a database\n",
    "            dfFinalSeasonLikelihood_noFirstCol = pd.DataFrame(reportMatrix, columns = df_sum_ch2018_M_Columns)\n",
    "            dfFinalSeasonLikelihood = pd.concat([dfnew3, dfFinalSeasonLikelihood_noFirstCol], axis=1, sort=False)\n",
    "\n",
    "            #make a directory for outputs of part 4\n",
    "            if os.path.isdir(os.path.join(root, 'outSeason_Likelihood_survival')):\n",
    "                pass\n",
    "            else:\n",
    "                os.mkdir(os.path.join(root, 'outSeason_Likelihood_survival'))\n",
    "\n",
    "            outfileNameSeasonLikelihood = 'season_Likelihood_' +  total_Daily_Files[i]\n",
    "            outFolderSeasonLikelihood = os.path.join(root, 'outSeason_Likelihood_survival')\n",
    "            outputFileSeasonLikelihood = os.path.join(outFolderSeasonLikelihood, outfileNameSeasonLikelihood)\n",
    "\n",
    "            ####\n",
    "            outFilesFinalLikelihood = []\n",
    "            for fname in os.walk(outFolderSeasonLikelihood): \n",
    "                outFilesFinalLikelihood = fname[2]\n",
    "                qq = len(outFilesFinalLikelihood)\n",
    "                if os.path.isfile(outputFileSeasonLikelihood):\n",
    "                    newOutFileNameSeasonLikelihood = outputFileSeasonLikelihood[0 : -4] + '_' + str(qq) + '.csv'\n",
    "                    dfFinalSeasonLikelihood.to_csv(newOutFileNameSeasonLikelihood, index = False)\n",
    "                else:\n",
    "                    dfFinalSeasonLikelihood.to_csv(outputFileSeasonLikelihood, index = False)    \n",
    "\n",
    "        print('End of Part 2 Calculations')\n",
    "        \n",
    "        '''################################ PART3 ################################'''\n",
    "        '''##### PART 3 seasonal outputs for Artificial snow possibility and Economic Model#####'''\n",
    "        print('PART3: Running the Artificial snow possibility and Economic Model, seasonal outputs analyses!')\n",
    "        \n",
    "        dfSeasonMoney = [ None for _ in range(len(totalMoneyFiles))]\n",
    "        \n",
    "        \n",
    "        for i in range(len(totalMoneyFiles)):\n",
    "            dfSeasonMoney[i] = pd.read_csv(totalMoneyFiles[i], low_memory=False)\n",
    "\n",
    "            \n",
    "            start_date = date(1981, 1, 1)\n",
    "            end_date = date(2099, 12, 31)\n",
    "            dateList = []\n",
    "            for single_date in daterange(start_date, end_date):\n",
    "                dateList.append(single_date.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "            start_season = []\n",
    "            end_season = []\n",
    "\n",
    "            for pp in range (1981, 2099, 1):\n",
    "                start_season.append(date(pp, 11, 1))\n",
    "                end_season.append(date(pp+1, 4, 30))\n",
    "\n",
    "            df4 = dfSeasonMoney[i]\n",
    "            df4.set_index('DATE', inplace = True)\n",
    "            df4Col = df4.columns\n",
    "            df4ColCal = []   # list columns \n",
    "            df4ColCalMoney = [] \n",
    "            \n",
    "            #### Here is the syntax that controls the columns that should be taken to account for cal\n",
    "            for m in range(1):\n",
    "                df4ColCal.append(df4Col[4*m+0])\n",
    "                df4ColCalMoney.append(df4Col[4*m+3])\n",
    "\n",
    "            sumGoodArtSnow = np.zeros([len(start_season), len(df4ColCal)])\n",
    "            sumRowsArtSnow = np.zeros(len(df4ColCal))  ### Saeed  2020/06/11\n",
    "            \n",
    "            \n",
    "            sumProfit = np.zeros([len(start_season), len(df4ColCalMoney)])\n",
    "            sumRowsProfit = np.zeros(len(df4ColCalMoney))\n",
    "            \n",
    "            '''Artificial Snow'''\n",
    "            for j in range(len(df4ColCal)):\n",
    "                for k in range(len(start_season)):\n",
    "                \n",
    "                    start_date = start_season[k]\n",
    "                    end_date = end_season[k]\n",
    "                        \n",
    "                        \n",
    "                    for single_date in daterange(start_date, end_date):\n",
    "                        sumGoodArtSnow[k,j] += df4[df4ColCal[j]].loc[single_date.strftime(\"%m/%d/%Y\")]\n",
    "                        \n",
    "                        sumProfit[k,j] += df4[df4ColCalMoney[j]].loc[single_date.strftime(\"%m/%d/%Y\")]\n",
    "                        \n",
    "                        \n",
    "                    sumRowsArtSnow[j] +=  sumGoodArtSnow[k,j] ### Saeed  2020/06/22\n",
    "                    sumRowsProfit[j] += sumProfit[k,j]\n",
    "            \n",
    "            \n",
    "            #AveragesumRowsArtSnow = np.average(sumRowsArtSnow/len(df4ColCal))\n",
    "            AveragesumRowsArtSnow = np.average(sumRowsArtSnow/118)                                                                                                                                       \n",
    "            #AveragesumRowsProfit = np.average(sumRowsProfit/len(df4ColCalMoney))\n",
    "            AveragesumRowsProfit = np.average(sumRowsProfit/118)\n",
    "                                                                                                                                                   \n",
    "            df5 = pd.DataFrame(sumGoodArtSnow, columns = df4ColCal)\n",
    "            df6 = pd.DataFrame(sumProfit, columns = df4ColCalMoney)\n",
    "\n",
    "\n",
    "            firstCol = []\n",
    "            for o in range (len(seasonList)-1):\n",
    "                firstCol.append(seasonList[o] +'-' + seasonList[o+1])\n",
    "\n",
    "            columnsDF2 = ['Season']\n",
    "            dfnew4 = pd.DataFrame(firstCol, columns = columnsDF2)\n",
    "\n",
    "            dfFinalSeasonArtSnow = pd.concat([dfnew4, df5], axis=1, sort=False)\n",
    "            dfFinalSeasonFinancial = pd.concat([dfnew4, df6], axis=1, sort=False)\n",
    "\n",
    "            \n",
    "            if os.path.isdir(os.path.join(root, 'outSeasonArt')):\n",
    "                pass\n",
    "            else: \n",
    "                os.mkdir(os.path.join(root, 'outSeasonArt'))\n",
    "            \n",
    "            if os.path.isdir(os.path.join(root, 'outSeasonFinancial')):\n",
    "                pass\n",
    "            else: \n",
    "                os.mkdir(os.path.join(root, 'outSeasonFinancial'))\n",
    "                \n",
    "                \n",
    "            \n",
    "            outfileNameSeasonArt = 'season_Art_' + total_Money_Files[i]\n",
    "            outFolderSeasonArt = os.path.join(root, 'outSeasonArt')\n",
    "            outputFileSeasonArt = os.path.join(outFolderSeasonArt, outfileNameSeasonArt)\n",
    "            \n",
    "            \n",
    "            outfileNameSeasonMoney = 'season_Financial_' + total_Money_Files[i]\n",
    "            outFolderSeasonMoney = os.path.join(root, 'outSeasonFinancial')\n",
    "            outputFileSeasonMoney = os.path.join(outFolderSeasonMoney, outfileNameSeasonMoney)\n",
    "                \n",
    "            \n",
    "            ##### Moshkel injast\n",
    "            outFilesFinalArt = []\n",
    "            for filename in os.walk(outFolderSeasonArt):\n",
    "                outFilesFinalArt = filename[2]\n",
    "                jjj = len(outFilesFinalArt)\n",
    "                if os.path.isfile(outputFileSeasonArt):\n",
    "                    newOutFileNameSeasonArt = outputFileSeasonArt[0 : -4] + '_' + str(jjj) + '.csv'\n",
    "                    dfFinalSeasonArtSnow.to_csv(newOutFileNameSeasonArt, index = False)\n",
    "                else: \n",
    "                    dfFinalSeasonArtSnow.to_csv(outputFileSeasonArt, index = False)\n",
    "        \n",
    "            ####\n",
    "            outFilesFinalMoney = []\n",
    "            for fname in os.walk(outFolderSeasonMoney): \n",
    "                outFilesFinalMoney = fname[2]\n",
    "                q = len(outFilesFinalMoney)\n",
    "                if os.path.isfile(outputFileSeasonMoney):\n",
    "                    newOutFileNameSeasonMoney = outputFileSeasonMoney[0 : -4] + '_' + str(q) + '.csv'\n",
    "                    dfFinalSeasonFinancial.to_csv(newOutFileNameSeasonMoney, index = False)\n",
    "                else:\n",
    "                    dfFinalSeasonFinancial.to_csv(outputFileSeasonMoney, index = False)\n",
    "            \n",
    "            \n",
    "            print('End of all calculations')\n",
    "        \n",
    "        #return df1, outfolder, dfFinalSeason\n",
    "        #return {'y' : x1 * Xfactor1 * X2}\n",
    "        return {'y' : AveragesumRows, 'y1' : climateModel, 'y2' : dfpcpCol[climateModel], 'y3' : sumRows,\n",
    "                'y4' : AveragesumRowsArtSnow, 'y5' : AveragesumRowsProfit  ,'y6' : AveragereportMatrix }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: EMA_Workbench connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 20 dec. 2010\n",
    "\n",
    "This file illustrated the use the EMA classes for a contrived example\n",
    "It's main purpose has been to test the parallel processing functionality\n",
    "\n",
    ".. codeauthor:: jhkwakkel <j.h.kwakkel (at) tudelft (dot) nl>\n",
    "'''\n",
    "from __future__ import (absolute_import, print_function, division,\n",
    "                        unicode_literals)\n",
    "\n",
    "from ema_workbench import (Model, RealParameter, Constant, ScalarOutcome, ema_logging,\n",
    "                          perform_experiments, TimeSeriesOutcome, ArrayOutcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.evaluators/INFO/MainProcess] performing 3 scenarios * 3 policies * 1 model(s) = 9 experiments\n",
      "[EMA.ema_workbench.em_framework.evaluators/INFO/MainProcess] performing experiments sequentially\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CH2018 is being used\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Part 1 Running the model, daily output!\n",
      "End of Part 1 Calculations!\n",
      "Snow_Model: Starting Part 2, Running the model, seasonal outputs, reading files!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Performing  Tipping Points Analyses!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Likelihood Analyses!\n",
      "End of Part 2 Calculations\n",
      "PART3: Running the Artificial snow possibility and Economic Model, seasonal outputs analyses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 1 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of all calculations\n",
      "Random Climate realization version 1 is being used\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Part 1 Running the model, daily output!\n",
      "End of Part 1 Calculations!\n",
      "Snow_Model: Starting Part 2, Running the model, seasonal outputs, reading files!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Performing  Tipping Points Analyses!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Likelihood Analyses!\n",
      "End of Part 2 Calculations\n",
      "PART3: Running the Artificial snow possibility and Economic Model, seasonal outputs analyses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 2 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of all calculations\n",
      "Random Climate realization version 2 is being used\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Part 1 Running the model, daily output!\n",
      "End of Part 1 Calculations!\n",
      "Snow_Model: Starting Part 2, Running the model, seasonal outputs, reading files!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Performing  Tipping Points Analyses!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Likelihood Analyses!\n",
      "End of Part 2 Calculations\n",
      "PART3: Running the Artificial snow possibility and Economic Model, seasonal outputs analyses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 3 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of all calculations\n",
      "Original CH2018 is being used\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Part 1 Running the model, daily output!\n",
      "End of Part 1 Calculations!\n",
      "Snow_Model: Starting Part 2, Running the model, seasonal outputs, reading files!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Performing  Tipping Points Analyses!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Likelihood Analyses!\n",
      "End of Part 2 Calculations\n",
      "PART3: Running the Artificial snow possibility and Economic Model, seasonal outputs analyses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 4 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of all calculations\n",
      "Random Climate realization version 1 is being used\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Part 1 Running the model, daily output!\n",
      "End of Part 1 Calculations!\n",
      "Snow_Model: Starting Part 2, Running the model, seasonal outputs, reading files!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Performing  Tipping Points Analyses!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Likelihood Analyses!\n",
      "End of Part 2 Calculations\n",
      "PART3: Running the Artificial snow possibility and Economic Model, seasonal outputs analyses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 5 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of all calculations\n",
      "Random Climate realization version 2 is being used\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Part 1 Running the model, daily output!\n",
      "End of Part 1 Calculations!\n",
      "Snow_Model: Starting Part 2, Running the model, seasonal outputs, reading files!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Performing  Tipping Points Analyses!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Likelihood Analyses!\n",
      "End of Part 2 Calculations\n",
      "PART3: Running the Artificial snow possibility and Economic Model, seasonal outputs analyses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 6 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of all calculations\n",
      "Original CH2018 is being used\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Part 1 Running the model, daily output!\n",
      "End of Part 1 Calculations!\n",
      "Snow_Model: Starting Part 2, Running the model, seasonal outputs, reading files!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Performing  Tipping Points Analyses!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Likelihood Analyses!\n",
      "End of Part 2 Calculations\n",
      "PART3: Running the Artificial snow possibility and Economic Model, seasonal outputs analyses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 7 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of all calculations\n",
      "Random Climate realization version 1 is being used\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Part 1 Running the model, daily output!\n",
      "End of Part 1 Calculations!\n",
      "Snow_Model: Starting Part 2, Running the model, seasonal outputs, reading files!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Performing  Tipping Points Analyses!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Likelihood Analyses!\n",
      "End of Part 2 Calculations\n",
      "PART3: Running the Artificial snow possibility and Economic Model, seasonal outputs analyses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 8 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of all calculations\n",
      "Random Climate realization version 2 is being used\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Part 1 Running the model, daily output!\n",
      "End of Part 1 Calculations!\n",
      "Snow_Model: Starting Part 2, Running the model, seasonal outputs, reading files!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Performing  Tipping Points Analyses!\n",
      "Snow Model: Continuing of Part 2, Seasonal Outputs, Likelihood Analyses!\n",
      "End of Part 2 Calculations\n",
      "PART3: Running the Artificial snow possibility and Economic Model, seasonal outputs analyses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 9 cases completed\n",
      "[EMA.ema_workbench.em_framework.evaluators/INFO/MainProcess] experiments finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of all calculations\n",
      "end!\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ema_logging.LOG_FORMAT = '[%(name)s/%(levelname)s/%(processName)s] %(message)s'\n",
    "    ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "    model = Model('UZHModel', function = snow_Model)  # instantiate the model\n",
    "    \n",
    "    \n",
    "    # specify process model parameters  xRCP=None, xClimateModel=None\n",
    "    model.uncertainties = [RealParameter(\"Xfactor1\",  0.51, 3.49),\n",
    "                           RealParameter(\"xRCP\", 0.51, 3.49),\n",
    "                           RealParameter(\"xClimateModel\", 0, 1),\n",
    "                           RealParameter(\"X2fM\", 1.01, 1.61),\n",
    "                           RealParameter(\"X3iPot\", 900, 1100),                        \n",
    "                           RealParameter(\"X5temp\", 0, 6.0),\n",
    "                           RealParameter(\"X6tempArt\", -2.0, -1.0)]\n",
    "    \n",
    "    # specify polices\n",
    "    model.levers = [RealParameter(\"x1SnowThershold\", 200.0, 300.0),\n",
    "                    RealParameter(\"xGoodDays\", 70.0 , 100.0)]\n",
    "   \n",
    "\n",
    "    # specify outcomes\n",
    "    model.outcomes = [ScalarOutcome('y'),\n",
    "                      ScalarOutcome('y1'),\n",
    "                      ArrayOutcome('y3'),\n",
    "                      ScalarOutcome('y4'),\n",
    "                      ScalarOutcome('y5'),\n",
    "                      ScalarOutcome('y6')]\n",
    "    \n",
    "    # override some of the defaults of the model\n",
    "    model.constants = [Constant(\"X4rSnow\", 0.7),\n",
    "                       Constant(\"xCostDay\", 6),\n",
    "                       Constant(\"xRevenueDay\", 10)]\n",
    "    \n",
    "\n",
    "    results = perform_experiments(model, 3, 3)\n",
    "    \n",
    "    #with MultiprocessingEvaluator(model, n_processes=4) as evaluator:\n",
    "     #   results = evaluator.perform_experiments(scenarios=4, policies=5)\n",
    "\n",
    "\n",
    "print('end!')\n",
    "training_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 74.96320223808289 seconds ---\n",
      "training time : 1.0 mins and 15.0 seconds\n",
      "training time : 0.0 hours 1.0 mins and 15.0 seconds \n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (training_time))\n",
    "print('training time : {} mins and {} seconds'.format((training_time // 60) , round((training_time % 60), 1)))\n",
    "print('training time : {} hours {} mins and {} seconds '.format(training_time // 3600 , round((training_time % 3600 // 60), 1), round((training_time % 3600) % 60 ,1)))\n",
    "# Save the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       X2fM       X3iPot    X5temp  X6tempArt  Xfactor1  xClimateModel  \\\n",
       " 0  1.587765  1073.999667  2.725308  -1.276246  0.606692       0.746953   \n",
       " 1  1.402168   904.947212  4.945030  -1.577610  1.538576       0.249330   \n",
       " 2  1.150785  1013.073856  1.370164  -1.694022  2.559994       0.570478   \n",
       " 3  1.587765  1073.999667  2.725308  -1.276246  0.606692       0.746953   \n",
       " 4  1.402168   904.947212  4.945030  -1.577610  1.538576       0.249330   \n",
       " 5  1.150785  1013.073856  1.370164  -1.694022  2.559994       0.570478   \n",
       " 6  1.587765  1073.999667  2.725308  -1.276246  0.606692       0.746953   \n",
       " 7  1.402168   904.947212  4.945030  -1.577610  1.538576       0.249330   \n",
       " 8  1.150785  1013.073856  1.370164  -1.694022  2.559994       0.570478   \n",
       " \n",
       "        xRCP  x1SnowThershold  xGoodDays scenario policy     model  \n",
       " 0  1.754046       211.110355  85.479930        0      0  UZHModel  \n",
       " 1  0.758518       211.110355  85.479930        1      0  UZHModel  \n",
       " 2  3.326348       211.110355  85.479930        2      0  UZHModel  \n",
       " 3  1.754046       283.502589  94.001302        0      1  UZHModel  \n",
       " 4  0.758518       283.502589  94.001302        1      1  UZHModel  \n",
       " 5  3.326348       283.502589  94.001302        2      1  UZHModel  \n",
       " 6  1.754046       251.510916  73.404537        0      2  UZHModel  \n",
       " 7  0.758518       251.510916  73.404537        1      2  UZHModel  \n",
       " 8  3.326348       251.510916  73.404537        2      2  UZHModel  ,\n",
       " {'y': array([44.56153676, 89.06828201, 29.30631611, 35.43553509, 74.8612144 ,\n",
       "         22.50038984, 38.99800473, 80.83460601, 25.14132419]),\n",
       "  'y1': array([30,  3, 54, 30,  3, 54, 30,  3, 54]),\n",
       "  'y3': array([[ 12.96232012, 115.90349229, 143.44807157, ...,   1.38960498,\n",
       "           76.38541528,  29.80379623],\n",
       "         [ 50.6741131 , 115.01254991, 126.44079817, ...,  71.95111117,\n",
       "           82.84673631, 101.82044644],\n",
       "         [ 20.58226846, 137.95588669, 119.56410365, ...,   3.0625215 ,\n",
       "            0.77386067,  10.69596989],\n",
       "         ...,\n",
       "         [ 10.88016393, 100.8573539 , 139.0951152 , ...,   1.16639072,\n",
       "           64.67417096,  25.01636944],\n",
       "         [ 42.53425718, 104.16699979, 106.59592988, ...,  60.76257135,\n",
       "           70.04538911,  94.50778815],\n",
       "         [ 17.27610898, 131.58327362, 100.37934086, ...,   2.57058425,\n",
       "            0.64955431,   8.97786081]]),\n",
       "  'y4': array([67.12949153, 67.75881356, 52.42135593, 67.12949153, 67.75881356,\n",
       "         52.42135593, 67.12949153, 67.75881356, 52.42135593]),\n",
       "  'y5': array([2.20881356, 4.46449153, 1.45127119, 1.29033898, 2.85330508,\n",
       "         0.80135593, 1.59118644, 3.33025424, 1.01872881]),\n",
       "  'y6': array([0.39868895, 0.71951219, 0.26719746, 0.30629394, 0.62545316,\n",
       "         0.19775705, 0.39439333, 0.71866624, 0.26328217])})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.util.utilities/INFO/MainProcess] results saved successfully to C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\CHrandomness_5\\6000_runs.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from ema_workbench import save_results\n",
    "#save_results(results, r'./1000 runs.tar.gz')\n",
    "save_results(results, r'C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\CHrandomness_5\\6000_runs.tar.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 20 sep. 2011\n",
    "\n",
    ".. codeauthor:: jhkwakkel <j.h.kwakkel (at) tudelft (dot) nl>\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ema_workbench import load_results, ema_logging\n",
    "\n",
    "from ema_workbench.analysis.pairs_plotting import (pairs_lines, pairs_scatter,\n",
    "                                                   pairs_density)\n",
    "\n",
    "ema_logging.log_to_stderr(level=ema_logging.DEFAULT_LEVEL)\n",
    "\n",
    "# load the data\n",
    "fh = r'C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\CHrandomness_3\\6000_runs.tar.gz'\n",
    "experiments, outcomes = load_results(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiments.shape)\n",
    "print(list(outcomes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import random\n",
    "from operator import add\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.integrate import simps\n",
    "from numpy import trapz\n",
    "from decimal import Decimal, ROUND_DOWN, ROUND_UP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualization of the EMA_Workbench Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with ema-workbench - part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import pairs_plotting\n",
    "fig, axes = pairs_plotting.pairs_scatter(experiments, outcomes, group_by='policy',\n",
    "                                         legend=True)\n",
    "fig.set_size_inches(15, 15)\n",
    "fig.savefig(r'C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\CHrandomness_3\\SA_All.svg', format='svg', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "rootVisualization = r'C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\CHrandomness_3'\n",
    "with tarfile.open(os.path.join(rootVisualization, '6000_runs.tar.gz'),\"r\") as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(rootVisualization, '6000_runs'))\n",
    "\n",
    "# We have five types of outputs\n",
    "outDaily = os.path.join(rootVisualization, 'Outputs_py')\n",
    "outSeasonTippingPoint = os.path.join(rootVisualization, 'outSeason')\n",
    "outSeason_Likelihood_survival = os.path.join(rootVisualization, 'outSeason_Likelihood_survival')\n",
    "outSeasonArtificialSnowPossibility = os.path.join(rootVisualization, 'outSeasonArt')\n",
    "outSeasonFinancial = os.path.join(rootVisualization, 'outSeasonFinancial')\n",
    "out_ema = os.path.join(rootVisualization, '6000_runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_ema_experiment = pd.read_csv(os.path.join(out_ema, 'experiments.csv'))\n",
    "\n",
    "df4_ema_y = pd.read_csv(os.path.join(out_ema, 'y.csv'), header=None)\n",
    "df4_ema_y.columns = [\"Yout\"]\n",
    "df4_ema_y1 = pd.read_csv(os.path.join(out_ema, 'y1.csv'), header=None)\n",
    "df4_ema_y1.columns = [\"Yout1\"]\n",
    "df4_ema_y3 = pd.read_csv(os.path.join(out_ema, 'y3.csv'), header=None)\n",
    "df4_ema_y3.columns = [\"Yout3\"]\n",
    "df4_ema_y4 = pd.read_csv(os.path.join(out_ema, 'y4.csv'), header=None)\n",
    "df4_ema_y4.columns = [\"Yout4\"]\n",
    "df4_ema_y5 = pd.read_csv(os.path.join(out_ema, 'y5.csv'), header=None)\n",
    "df4_ema_y5.columns = [\"Yout5\"]\n",
    "df4_ema_y6 = pd.read_csv(os.path.join(out_ema, 'y6.csv'), header=None)\n",
    "df4_ema_y6.columns = [\"Yout6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ema = pd.concat((df4_ema_experiment, df4_ema_y, df4_ema_y1, df4_ema_y3, df4_ema_y4, df4_ema_y5, df4_ema_y6 ), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the values\n",
    "x2 = df_final_ema['X2fM'].values\n",
    "x3 = df_final_ema['X3iPot'].values\n",
    "x5 = df_final_ema['X5temp'].values\n",
    "x6 = df_final_ema['X6tempArt'].values\n",
    "xRCP = df_final_ema['xRCP'].values\n",
    "xClimateModel = df_final_ema['xClimateModel'].values\n",
    "x1SnowThershold = df_final_ema['x1SnowThershold'].values\n",
    "xGoodDays = df_final_ema['xGoodDays'].values\n",
    "\n",
    "\n",
    "\n",
    "YY = df_final_ema['Yout'].values\n",
    "y1 = df_final_ema['Yout1'].values\n",
    "y3 = df_final_ema['Yout3'].values\n",
    "y4 = df_final_ema['Yout4'].values\n",
    "y5 = df_final_ema['Yout5'].values\n",
    "y6 = df_final_ema['Yout6'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outFolder = r'C:\\Saeid\\Prj100\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\CHrandomness_3'\n",
    "outFolder = rootVisualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outSeasonFolder = os.path.join(outFolder, 'outSeason')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2020/07/22 ####\n",
    "all_Files = []\n",
    "for filename in os.walk(outSeasonFolder):\n",
    "    all_Files = filename[2]\n",
    "    \n",
    "totalFiles_loc = []\n",
    "for i in range(len(all_Files)):\n",
    "    totalFiles_loc.append(os.path.join(outSeasonFolder, all_Files[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(totalFiles_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyMatrixGoodDays = [np.empty([119, 69]) for _ in range(len(totalFiles_loc))]\n",
    "    \n",
    "for i in range(0, len(totalFiles_loc), 1):\n",
    "    b = []\n",
    "    with open(totalFiles_loc[i], 'r') as file:\n",
    "        outputReaderlines = file.readlines()\n",
    "        for j in range (len(outputReaderlines)):\n",
    "            b.append(outputReaderlines[j].replace('\\n','').split(','))\n",
    "    \n",
    "    emptyMatrixGoodDays[i] = np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#matrix_GoodDays = np.stack((emptyMatrixGoodDays[i] for i in range (0,len(totalFiles_loc),1)), axis=2)\n",
    "matrix_GoodDays = np.stack((emptyMatrixGoodDays[i] for i in range (0,len(totalFiles_loc),1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_GoodDays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyMatrixGoodDays[1][0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_GoodDays26= np.stack((emptyMatrixGoodDays[i] for i in range (0,len(totalFiles_loc),1) \\\n",
    "                             if 'RCP26' in emptyMatrixGoodDays[i][0,1]) , axis=0) \n",
    "\n",
    "matrix_GoodDays45= np.stack((emptyMatrixGoodDays[i] for i in range (0,len(totalFiles_loc),1) \\\n",
    "                             if 'RCP45' in emptyMatrixGoodDays[i][0,1]) , axis=0) \n",
    "\n",
    "matrix_GoodDays85= np.stack((emptyMatrixGoodDays[i] for i in range (0,len(totalFiles_loc),1) \\\n",
    "                             if 'RCP85' in emptyMatrixGoodDays[i][0,1]) , axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_GoodDays26.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_GoodDays45.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_GoodDays85.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(matrix_GoodDays))\n",
    "print(matrix_GoodDays.ndim)\n",
    "print(len(matrix_GoodDays))\n",
    "print(matrix_GoodDays.size)\n",
    "print(matrix_GoodDays.dtype)\n",
    "print(matrix_GoodDays.dtype.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the quantiles for each year of simulation (1981-2100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_GoodDays[2, 1:, 1:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q10_years = np.quantile(matrix_GoodDays[:, 1:, 1:2].astype(float), 0.1, axis=0)\n",
    "q25_years = np.quantile(matrix_GoodDays[:, 1:, 1:2].astype(float), 0.25, axis=0)\n",
    "q50_years = np.quantile(matrix_GoodDays[:, 1:, 1:2].astype(float), 0.5, axis=0)\n",
    "q75_years = np.quantile(matrix_GoodDays[:, 1:, 1:2].astype(float), 0.75, axis=0)\n",
    "q90_years = np.quantile(matrix_GoodDays[:, 1:, 1:2].astype(float), 0.9, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q10_years[0])\n",
    "print(q25_years[0])\n",
    "print(q50_years[0])\n",
    "print(q75_years[0])\n",
    "print(q90_years[0])\n",
    "print(q50_years.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a22 =  matrix_GoodDays[3100, 1:, 1:2].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(15,9))\n",
    "x_axis = np.arange(1981, 2099, step=1)\n",
    "\n",
    "\n",
    "for i in range(0, 6000, 5):\n",
    "    a = matrix_GoodDays[i, 1:, 1:2].astype(float)\n",
    "    plt.plot(x_axis, a[:,0], color='green', linestyle = '--', alpha=0.01)\n",
    "\n",
    "\n",
    "axs.axhline(y=100, color='green', alpha=0.8)\n",
    "axs.axhline(y=80, color='blue', alpha=0.8)\n",
    "\n",
    "axs.axvline(x=2020, color='black', alpha=0.8)\n",
    "axs.axvline(x=2030, color='black', alpha=0.8)\n",
    "axs.axvline(x=2050, color='black', alpha=0.8)\n",
    "axs.axvline(x=2060, color='black', alpha=0.8)\n",
    "axs.axvline(x=2090, color='black', alpha=0.8)\n",
    "\n",
    "plt.plot(x_axis, q10_years, color='red', linewidth = 3, alpha=1)\n",
    "plt.plot(x_axis, q25_years, color='brown',linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q50_years, color='black', linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q75_years, color='blue', linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q90_years, color='green', linewidth = 3, alpha=1)\n",
    "\n",
    "\n",
    "axs.set_ylim(bottom=0, top =181)\n",
    "axs.set_title('Hoch_Ybrigh(m)')\n",
    "axs.set_xlabel('Years')\n",
    "axs.set_ylabel('Days With Good Snow Condition')\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'RCP_All.svg'), format='svg', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, outcomes = load_results(fh)\n",
    "print(experiments.shape)\n",
    "print(list(outcomes.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with ema-workbench - part2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import prim\n",
    "\n",
    "x = experiments\n",
    "y = outcomes['y'] < 100\n",
    "prim_alg = prim.Prim(x, y, threshold=0.8)\n",
    "box1 = prim_alg.find_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.show_tradeoff()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.inspect(4)\n",
    "box1.inspect(4, style='graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axs = plt.subplots()\n",
    "axs = box1.show_pairs_scatter(4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with ema-workbench - part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import feature_scoring\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,9))\n",
    "\n",
    "x = experiments\n",
    "Y = outcomes\n",
    "\n",
    "fs = feature_scoring.get_feature_scores_all(x, Y)\n",
    "ax = sns.heatmap(fs, cmap='viridis', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = experiments\n",
    "Y = outcomes['y'] <30\n",
    "#fs, alg = feature_scoring.get_ex_feature_scores(x, y, mode=feature_scoring.CLASSIFICATION)\n",
    "fs, alg = feature_scoring.get_ex_feature_scores(x, Y)\n",
    "\n",
    "fs.sort_values(ascending=False, by=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import dimensional_stacking\n",
    "\n",
    "x = experiments\n",
    "y = outcomes['y'] < 100\n",
    "dimensional_stacking.create_pivot_plot(x,y, 2, nbins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import regional_sa\n",
    "from numpy.lib import recfunctions as rf\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "# model is the same across experiments\n",
    "x = experiments.copy()\n",
    "x = x.drop('model', axis=1)\n",
    "y = outcomes['y'] < 100\n",
    "fig = regional_sa.plot_cdfs(x,y)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots (Inputs vs Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "#axs[0,0].scatter(xGoodDays, y, s = 200, c='b' , marker='o', alpha=0.7, cmap='viridis', \n",
    " #          edgecolors='none', label='Good Days Condition')\n",
    "\n",
    "#axs[0,1].scatter(x1SnowThershold, y, s = 200, c='orange' , marker='o', alpha=0.7, cmap='viridis', \n",
    " #          edgecolors='none', label='Snow Threshold Condition')\n",
    "\n",
    "axs[0,0].scatter(x5, YY, s = 50, c='r' , marker='^', alpha=0.7, cmap='viridis', \n",
    "           edgecolors='none', label='Temperature melt')\n",
    "\n",
    "axs[0,1].scatter(x6, YY, s = 50, c='g' , marker='^', alpha=0.7, cmap='viridis', \n",
    "           edgecolors='none', label='Temperature artificial snow')\n",
    "\n",
    "axs[1,0].scatter(x2, YY, s = 50, c='c' , marker='^', alpha=0.7, cmap='viridis', \n",
    "           edgecolors='none', label='Temperature melt')\n",
    "\n",
    "axs[1,1].scatter(x3, YY, s = 50, c='m' , marker='^', alpha=0.7, cmap='viridis', \n",
    "           edgecolors='none', label='Temperature artificial snow')\n",
    "\n",
    "\n",
    "# produce a legend with the unique colors from the scatter\n",
    "#axs[0,0].set_xlabel(\"Good Days Condition\")\n",
    "#axs[0,1].set_xlabel(\"Snow Threshold Condition\")\n",
    "axs[0,0].set_xlabel(\"Temperature Threshold re-freezing\")\n",
    "axs[0,1].set_xlabel(\"Temperature Threshold artificial snow\")\n",
    "axs[1,0].set_xlabel(\"snow process parmater FM\")\n",
    "axs[1,1].set_xlabel(\"snow process parameter iPot\")\n",
    "\n",
    "\n",
    "axs[0,0].set_ylabel(\"Annual Average of Good Snow Condition (days)\")\n",
    "#axs[0,1].set_ylabel(\"Frequency of Tipping points\")\n",
    "axs[1,0].set_ylabel(\"Annual Average of Good Snow Condition (days)\")\n",
    "#axs[1,1].set_ylabel(\"Frequency of Tipping points\")\n",
    "#axs[2,0].set_ylabel(\"Annual Average of Good Snow Condition (days)\")\n",
    "#axs[2,1].set_ylabel(\"Frequency of Tipping points\")\n",
    "\n",
    "#axs[0,0].set_title('Sensitivity of TDwGSC to Policy-1 (less that 100 days)')\n",
    "#axs[0,1].set_title('Sensitivity of TDwGSC to Policy-2 (300 mm)')\n",
    "axs[0,0].set_title('Sensitivity of TDwGSC to Threshold of fuzzy melting and re-freezing')\n",
    "axs[0,1].set_title('Sensitivity of TDwGSC to Threshold of Snow Making')\n",
    "axs[1,0].set_title('Sensitivity of TDwGSC to FM paramater in Ablation')\n",
    "axs[1,1].set_title('Sensitivity of TDwGSC to iPot paramater in Ablation')\n",
    "\n",
    "\n",
    "#axs[0,0].text(0, 130, r'A', fontsize=15)\n",
    "#axs[0,1].text(-1.9, 130, r'B', fontsize=15)\n",
    "#axs[1,0].text(1.04, 130, r'C', fontsize=15)\n",
    "#axs[1,1].text(920, 130, r'D', fontsize=15)\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'SA_All.svg'), format='svg', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(xGoodDays, x1SnowThershold, YY, s = 40, c='r', marker='o')\n",
    "\n",
    "ax.set_xlabel('Good Days Condition Policy-1(100-day)')\n",
    "ax.set_ylabel('Natural Snow Cover Condition Policy-2(300-mm)')\n",
    "ax.set_zlabel('Annual Number of days with good snow condition')\n",
    "\n",
    "\n",
    "#ax.scatter(100, 300, 40, color='green')\n",
    "\n",
    "\n",
    "#x = np.linspace(75,100,5)\n",
    "#y = np.linspace(220,300,16)\n",
    "#X,Y = np.meshgrid(x,y)\n",
    "#Z= X + Y -X - Y + 40 \n",
    "#surf = ax.plot_surface(X, Y, Z, alpha = 0.2)\n",
    "ax.view_init(20, -45)\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'SA_Policy_All.svg'), format='svg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Q0.9 and Q0.1 databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q10_years[0])\n",
    "print(q25_years[0])\n",
    "print(q50_years[0])\n",
    "print(q75_years[0])\n",
    "print(q90_years[0])\n",
    "print(q50_years.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_GoodDays[:, :, 1:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_GoodDays[600, 0, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_GoodDays[3333].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for different RCP2.6, RCP4.5, and RCP8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix_GoodDays26.shape)\n",
    "print(matrix_GoodDays45.shape)\n",
    "print(matrix_GoodDays85.shape)\n",
    "print(matrix_GoodDays.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q10_years_26 = np.quantile(matrix_GoodDays26[:, 1:, 1:2].astype(float), 0.1, axis=0)\n",
    "q25_years_26 = np.quantile(matrix_GoodDays26[:, 1:, 1:2].astype(float), 0.25, axis=0)\n",
    "q50_years_26 = np.quantile(matrix_GoodDays26[:, 1:, 1:2].astype(float), 0.5, axis=0)\n",
    "q75_years_26 = np.quantile(matrix_GoodDays26[:, 1:, 1:2].astype(float), 0.75, axis=0)\n",
    "q90_years_26 = np.quantile(matrix_GoodDays26[:, 1:, 1:2].astype(float), 0.9, axis=0)\n",
    "\n",
    "q10_years_45 = np.quantile(matrix_GoodDays45[:, 1:, 1:2].astype(float), 0.1, axis=0)\n",
    "q25_years_45 = np.quantile(matrix_GoodDays45[:, 1:, 1:2].astype(float), 0.25, axis=0)\n",
    "q50_years_45 = np.quantile(matrix_GoodDays45[:, 1:, 1:2].astype(float), 0.5, axis=0)\n",
    "q75_years_45 = np.quantile(matrix_GoodDays45[:, 1:, 1:2].astype(float), 0.75, axis=0)\n",
    "q90_years_45 = np.quantile(matrix_GoodDays45[:, 1:, 1:2].astype(float), 0.9, axis=0)\n",
    "\n",
    "q10_years_85 = np.quantile(matrix_GoodDays85[:, 1:, 1:2].astype(float), 0.1, axis=0)\n",
    "q25_years_85 = np.quantile(matrix_GoodDays85[:, 1:, 1:2].astype(float), 0.25, axis=0)\n",
    "q50_years_85 = np.quantile(matrix_GoodDays85[:, 1:, 1:2].astype(float), 0.5, axis=0)\n",
    "q75_years_85 = np.quantile(matrix_GoodDays85[:, 1:, 1:2].astype(float), 0.75, axis=0)\n",
    "q90_years_85 = np.quantile(matrix_GoodDays85[:, 1:, 1:2].astype(float), 0.9, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfq50_26 = pd.DataFrame(q50_years_26)\n",
    "dfq50_26.columns = ['median26']\n",
    "dfq50_45 = pd.DataFrame(q50_years_45)\n",
    "dfq50_45.columns = ['median45']\n",
    "dfq50_85 = pd.DataFrame(q50_years_85)\n",
    "dfq50_85.columns = ['median85']\n",
    "\n",
    "\n",
    "dfq90_26 = pd.DataFrame(q90_years_26)\n",
    "dfq90_26.columns = ['Q90_26']\n",
    "dfq90_45 = pd.DataFrame(q90_years_45)\n",
    "dfq90_45.columns = ['Q90_45']\n",
    "dfq90_85 = pd.DataFrame(q90_years_85)\n",
    "dfq90_85.columns = ['Q90_85']\n",
    "\n",
    "\n",
    "dfq10_26 = pd.DataFrame(q10_years_26)\n",
    "dfq10_26.columns = ['Q10_26']\n",
    "dfq10_45 = pd.DataFrame(q10_years_45)\n",
    "dfq10_45.columns = ['Q10_45']\n",
    "dfq10_85 = pd.DataFrame(q10_years_85)\n",
    "dfq10_85.columns = ['Q10_85']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfq50_26.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfAllAverages = pd.concat((dfq50_26, dfq50_45, dfq50_85), axis=1)\n",
    "dfAllAverages_Q90 = pd.concat((dfq90_26, dfq90_45, dfq90_85), axis=1)\n",
    "dfAllAverages_Q10 = pd.concat((dfq10_26, dfq10_45, dfq10_85), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllAverages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** All RCPs***')\n",
    "print(q10_years[0])\n",
    "print(q25_years[0])\n",
    "print(q50_years[0])\n",
    "print(q75_years[0])\n",
    "print(q90_years[0])\n",
    "print(q50_years.shape)\n",
    "print('*** RCP2.6***')\n",
    "print(q10_years_26[0])\n",
    "print(q25_years_26[0])\n",
    "print(q50_years_26[0])\n",
    "print(q75_years_26[0])\n",
    "print(q90_years_26[0])\n",
    "print(q50_years_26.shape)\n",
    "print('*** RCP4.5***')\n",
    "print(q10_years_45[0])\n",
    "print(q25_years_45[0])\n",
    "print(q50_years_45[0])\n",
    "print(q75_years_45[0])\n",
    "print(q90_years_45[0])\n",
    "print(q50_years_45.shape)\n",
    "print('*** RCP8.5***')\n",
    "print(q10_years_85[0])\n",
    "print(q25_years_85[0])\n",
    "print(q50_years_85[0])\n",
    "print(q75_years_85[0])\n",
    "print(q90_years_85[0])\n",
    "print(q50_years_85.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(15,9))\n",
    "x_axis = np.arange(1981, 2099, step=1)\n",
    "\n",
    "\n",
    "for i in range(0, 1050, 5):\n",
    "    a = matrix_GoodDays26[i, 1:, 1:2].astype(float)\n",
    "    plt.plot(x_axis, a[:,0], color='green', linestyle = '--', alpha=0.01)\n",
    "\n",
    "\n",
    "axs.axhline(y=100, color='green', alpha=0.8)\n",
    "axs.axhline(y=80, color='blue', alpha=0.8)\n",
    "\n",
    "axs.axvline(x=2020, color='black', alpha=0.8)\n",
    "axs.axvline(x=2030, color='black', alpha=0.8)\n",
    "axs.axvline(x=2050, color='black', alpha=0.8)\n",
    "axs.axvline(x=2060, color='black', alpha=0.8)\n",
    "axs.axvline(x=2090, color='black', alpha=0.8)\n",
    "\n",
    "plt.plot(x_axis, q10_years_26, color='red', linewidth = 3, alpha=1)\n",
    "plt.plot(x_axis, q25_years_26, color='brown',linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q50_years_26, color='black', linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q75_years_26, color='blue', linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q90_years_26, color='green', linewidth = 3, alpha=1)\n",
    "\n",
    "\n",
    "axs.set_ylim(bottom=0, top =200)\n",
    "axs.set_title('Davos_band_2584(m)')\n",
    "axs.set_xlabel('Years')\n",
    "axs.set_ylabel('Days With Good Snow Condition')\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'RCP26.svg'), format='svg', dpi=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(15,9))\n",
    "x_axis = np.arange(1981, 2099, step=1)\n",
    "\n",
    "\n",
    "for i in range(0, 2050, 5):\n",
    "    a = matrix_GoodDays45[i, 1:, 1:2].astype(float)\n",
    "    plt.plot(x_axis, a[:,0], color='green', linestyle = '--', alpha=0.01)\n",
    "\n",
    "\n",
    "axs.axhline(y=100, color='green', alpha=0.8)\n",
    "axs.axhline(y=80, color='blue', alpha=0.8)\n",
    "\n",
    "axs.axvline(x=2020, color='black', alpha=0.8)\n",
    "axs.axvline(x=2030, color='black', alpha=0.8)\n",
    "axs.axvline(x=2050, color='black', alpha=0.8)\n",
    "axs.axvline(x=2060, color='black', alpha=0.8)\n",
    "axs.axvline(x=2090, color='black', alpha=0.8)\n",
    "\n",
    "plt.plot(x_axis, q10_years_45, color='red', linewidth = 3, alpha=1)\n",
    "plt.plot(x_axis, q25_years_45, color='brown',linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q50_years_45, color='black', linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q75_years_45, color='blue', linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q90_years_45, color='green', linewidth = 3, alpha=1)\n",
    "\n",
    "\n",
    "axs.set_ylim(bottom=0, top =200)\n",
    "axs.set_title('Davos_band_2584(m)')\n",
    "axs.set_xlabel('Years')\n",
    "axs.set_ylabel('Days With Good Snow Condition')\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'RCP45.svg'), format='svg', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(15,9))\n",
    "x_axis = np.arange(1981, 2099, step=1)\n",
    "\n",
    "\n",
    "for i in range(0, 2900, 5):\n",
    "    a = matrix_GoodDays85[i, 1:, 1:2].astype(float)\n",
    "    plt.plot(x_axis, a[:,0], color='green', linestyle = '--', alpha=0.01)\n",
    "\n",
    "\n",
    "axs.axhline(y=100, color='green', alpha=0.8)\n",
    "axs.axhline(y=80, color='blue', alpha=0.8)\n",
    "\n",
    "axs.axvline(x=2020, color='black', alpha=0.8)\n",
    "axs.axvline(x=2030, color='black', alpha=0.8)\n",
    "axs.axvline(x=2050, color='black', alpha=0.8)\n",
    "axs.axvline(x=2060, color='black', alpha=0.8)\n",
    "axs.axvline(x=2090, color='black', alpha=0.8)\n",
    "\n",
    "plt.plot(x_axis, q10_years_85, color='red', linewidth = 3, alpha=1)\n",
    "plt.plot(x_axis, q25_years_85, color='brown',linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q50_years_85, color='black', linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q75_years_85, color='blue', linewidth = 2, alpha=1)\n",
    "plt.plot(x_axis, q90_years_85, color='green', linewidth = 3, alpha=1)\n",
    "\n",
    "\n",
    "axs.set_ylim(bottom=0, top =200)\n",
    "axs.set_title('Davos_band_2584(m)')\n",
    "axs.set_xlabel('Years')\n",
    "axs.set_ylabel('Days With Good Snow Condition')\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'RCP45.svg'), format='svg', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,1, figsize=(15,15))\n",
    "x_axis = np.arange(1981, 2099, step=1)\n",
    "\n",
    "\n",
    "for i in range(0, 1050, 5):\n",
    "    a = matrix_GoodDays26[i, 1:, 1:2].astype(float)\n",
    "    axs[0].plot(x_axis, a[:,0], color='green', linestyle = '--', alpha=0.01)\n",
    "\n",
    "\n",
    "for i in range(0, 2050, 5):\n",
    "    a = matrix_GoodDays45[i, 1:, 1:2].astype(float)\n",
    "    axs[1].plot(x_axis, a[:,0], color='green', linestyle = '--', alpha=0.01)\n",
    "\n",
    "\n",
    "for i in range(0, 2900, 5):\n",
    "    a = matrix_GoodDays85[i, 1:, 1:2].astype(float)\n",
    "    axs[2].plot(x_axis, a[:,0], color='green', linestyle = '--', alpha=0.01)    \n",
    "\n",
    "\n",
    "axs[0].axhline(y=100, color='green', alpha=0.8)\n",
    "axs[0].axhline(y=80, color='blue', alpha=0.8)\n",
    "axs[1].axhline(y=100, color='green', alpha=0.8)\n",
    "axs[1].axhline(y=80, color='blue', alpha=0.8)\n",
    "axs[2].axhline(y=100, color='green', alpha=0.8)\n",
    "axs[2].axhline(y=80, color='blue', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "axs[0].axvline(x=2020, color='black', alpha=0.8)\n",
    "axs[0].axvline(x=2030, color='black', alpha=0.8)\n",
    "axs[0].axvline(x=2050, color='black', alpha=0.8)\n",
    "axs[0].axvline(x=2060, color='black', alpha=0.8)\n",
    "axs[0].axvline(x=2090, color='black', alpha=0.8)\n",
    "axs[1].axvline(x=2020, color='black', alpha=0.8)\n",
    "axs[1].axvline(x=2030, color='black', alpha=0.8)\n",
    "axs[1].axvline(x=2050, color='black', alpha=0.8)\n",
    "axs[1].axvline(x=2060, color='black', alpha=0.8)\n",
    "axs[1].axvline(x=2090, color='black', alpha=0.8)\n",
    "axs[2].axvline(x=2020, color='black', alpha=0.8)\n",
    "axs[2].axvline(x=2030, color='black', alpha=0.8)\n",
    "axs[2].axvline(x=2050, color='black', alpha=0.8)\n",
    "axs[2].axvline(x=2060, color='black', alpha=0.8)\n",
    "axs[2].axvline(x=2090, color='black', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "axs[0].plot(x_axis, q10_years_26, color='red', linewidth = 3, alpha=1)\n",
    "axs[0].plot(x_axis, q25_years_26, color='brown',linewidth = 2, alpha=1)\n",
    "axs[0].plot(x_axis, q50_years_26, color='black', linewidth = 2, alpha=1)\n",
    "axs[0].plot(x_axis, q75_years_26, color='blue', linewidth = 2, alpha=1)\n",
    "axs[0].plot(x_axis, q90_years_26, color='green', linewidth = 3, alpha=1)\n",
    "axs[1].plot(x_axis, q10_years_45, color='red', linewidth = 3, alpha=1)\n",
    "axs[1].plot(x_axis, q25_years_45, color='brown',linewidth = 2, alpha=1)\n",
    "axs[1].plot(x_axis, q50_years_45, color='black', linewidth = 2, alpha=1)\n",
    "axs[1].plot(x_axis, q75_years_45, color='blue', linewidth = 2, alpha=1)\n",
    "axs[1].plot(x_axis, q90_years_45, color='green', linewidth = 3, alpha=1)\n",
    "axs[2].plot(x_axis, q10_years_85, color='red', linewidth = 3, alpha=1)\n",
    "axs[2].plot(x_axis, q25_years_85, color='brown',linewidth = 2, alpha=1)\n",
    "axs[2].plot(x_axis, q50_years_85, color='black', linewidth = 2, alpha=1)\n",
    "axs[2].plot(x_axis, q75_years_85, color='blue', linewidth = 2, alpha=1)\n",
    "axs[2].plot(x_axis, q90_years_85, color='green', linewidth = 3, alpha=1)\n",
    "\n",
    "axs[0].set_ylim(bottom=0, top =200)\n",
    "axs[0].set_title('Davos_band_2584(m)')\n",
    "axs[0].set_xlabel('Years')\n",
    "axs[0].set_ylabel('Days With Good Snow Condition')\n",
    "\n",
    "axs[1].set_ylim(bottom=0, top =200)\n",
    "axs[1].set_title('Davos_band_2584(m)')\n",
    "axs[1].set_xlabel('Years')\n",
    "axs[1].set_ylabel('Days With Good Snow Condition')\n",
    "\n",
    "axs[2].set_ylim(bottom=0, top =200)\n",
    "axs[2].set_title('Davos_band_2584(m)')\n",
    "axs[2].set_xlabel('Years')\n",
    "axs[2].set_ylabel('Days With Good Snow Condition')\n",
    "\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'RCP26_45_85.svg'), format='svg', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllAverages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_axis = np.arange(1981, 2100, step= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Case: Quantile 0.10 >> 90% of the scenarios satisfy this condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "fig, (ax3) = plt.subplots(1, 1, figsize=(9.5,7.5))\n",
    "\n",
    "x_axis = np.arange(0.5, 3.5, step=1)\n",
    "y_axis = np.arange(0, 118, step= 10)\n",
    "y_axis_new = np.arange(1981, 2100, step= 10)\n",
    "\n",
    "\n",
    "c3 = ax3.pcolor(dfAllAverages_Q10, cmap='viridis_r', vmin=1, vmax=100)\n",
    "cb3 = fig.colorbar(c3)\n",
    "\n",
    "#xticks(np.arange(0, 4, step=1))\n",
    "ax3.set_title('default: no edges')\n",
    "\n",
    "\n",
    "ax3.set_xlabel(\"Average over Climate scenarios in RCPs\")\n",
    "\n",
    "ax3.set_ylabel(\"Years of Simulation\")\n",
    "plt.xticks(x_axis, ('12-RCP2.6', '25-RCP4.5', '31-RCP8.5'))\n",
    "#plt.ylim(1981, 2100)\n",
    "plt.yticks(y_axis, y_axis_new)\n",
    "plt.title(\"Likelihood of survival of the resort at Q10 level from {} untill the year {}\".format(1981, 2099))\n",
    "\n",
    "ax3.text(0.2, 110, 'A', color = 'white', fontsize=15)\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'Likelihood_All.svg'), format='svg', dpi=300)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Case: Quantile 0.5 >> 50 % of the scenarios satisfy this condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "fig, (ax4) = plt.subplots(1, 1, figsize=(9.5,7.5))\n",
    "\n",
    "x_axis = np.arange(0.5, 3.5, step=1)\n",
    "y_axis = np.arange(0, 118, step= 10)\n",
    "y_axis_new = np.arange(1981, 2100, step= 10)\n",
    "\n",
    "\n",
    "c4 = ax4.pcolor(dfAllAverages, cmap='viridis_r' ,vmin=1, vmax=100)\n",
    "cb4 = fig.colorbar(c4)\n",
    "\n",
    "#xticks(np.arange(0, 4, step=1))\n",
    "ax4.set_title('default: no edges')\n",
    "\n",
    "\n",
    "ax4.set_xlabel(\"Average over Climate scenarios in RCPs\")\n",
    "\n",
    "ax4.set_ylabel(\"Years of Simulation\")\n",
    "plt.xticks(x_axis, ('12-RCP2.6', '25-RCP4.5', '31-RCP8.5'))\n",
    "#plt.ylim(1981, 2100)\n",
    "plt.yticks(y_axis, y_axis_new)\n",
    "plt.title(\"Likelihood of survival of the resort at Q90 level from {} untill the year {}\".format(1981, 2099))\n",
    "\n",
    "ax4.text(0.2, 110, 'B', color = 'white', fontsize=15)\n",
    "\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'Likelihood_All_Q90.svg'), format='svg', dpi=300)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Case: Quantile 0.9 >> Only 10 % of the scenarios satisfy this condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "fig, (ax4) = plt.subplots(1, 1, figsize=(9.5,7.5))\n",
    "\n",
    "x_axis = np.arange(0.5, 3.5, step=1)\n",
    "y_axis = np.arange(0, 118, step= 10)\n",
    "y_axis_new = np.arange(1981, 2100, step= 10)\n",
    "\n",
    "\n",
    "c4 = ax4.pcolor(dfAllAverages_Q90, cmap='viridis_r' ,vmin=1, vmax=100)\n",
    "cb4 = fig.colorbar(c4)\n",
    "\n",
    "#xticks(np.arange(0, 4, step=1))\n",
    "ax4.set_title('default: no edges')\n",
    "\n",
    "\n",
    "ax4.set_xlabel(\"Average over Climate scenarios in RCPs\")\n",
    "\n",
    "ax4.set_ylabel(\"Years of Simulation\")\n",
    "plt.xticks(x_axis, ('12-RCP2.6', '25-RCP4.5', '31-RCP8.5'))\n",
    "#plt.ylim(1981, 2100)\n",
    "plt.yticks(y_axis, y_axis_new)\n",
    "plt.title(\"Likelihood of survival of the resort at Q90 level from {} untill the year {}\".format(1981, 2099))\n",
    "\n",
    "ax4.text(0.2, 110, 'C', color = 'white', fontsize=15)\n",
    "\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'Likelihood_All_Q90.svg'), format='svg', dpi=300)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making dataframes for all RCP2.6 , RCP4.5, and RCP8.5 scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix_GoodDays26.shape)\n",
    "print(matrix_GoodDays45.shape)\n",
    "print(matrix_GoodDays85.shape)\n",
    "print(matrix_GoodDays.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame((matrix_GoodDays26[0][1:,0:1]), columns=matrix_GoodDays26[0,0,0:1]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df26_all = pd.DataFrame((matrix_GoodDays26[0][1:,0:1]), columns=matrix_GoodDays26[0,0,0:1])\n",
    "for i in range(1050):\n",
    "    #b = pd.DataFrame(matrix_GoodDays26[i][1:,1:], columns=matrix_GoodDays26[i,0,1:])\n",
    "    b = pd.DataFrame(matrix_GoodDays26[i][1:,1:], columns=['sc_'+ str(i+1) + '_'+ str(matrix_GoodDays26[i,0,1:])])\n",
    "    df26_all = pd.concat((df26_all, b), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df26_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_GoodDays45[2,0,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df45_all = pd.DataFrame((matrix_GoodDays45[0][1:,0:1]), columns=matrix_GoodDays45[0,0,0:1])\n",
    "for i in range(2050):\n",
    "    #b = pd.DataFrame(matrix_GoodDays45[i][1:,1:], columns=matrix_GoodDays45[i,0,1:])\n",
    "    b = pd.DataFrame(matrix_GoodDays45[i][1:,1:], columns=['sc_'+ str(i+1) + '_'+ str(matrix_GoodDays45[i,0,1:])])\n",
    "\n",
    "    df45_all = pd.concat((df45_all, b), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df45_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df85_all = pd.DataFrame((matrix_GoodDays85[0][1:,0:1]), columns=matrix_GoodDays85[0,0,0:1])\n",
    "for i in range(2900):\n",
    "    #b = pd.DataFrame(matrix_GoodDays85[i][1:,1:], columns=matrix_GoodDays85[i,0,1:])\n",
    "    b = pd.DataFrame(matrix_GoodDays85[i][1:,1:], columns=['sc_'+ str(i+1) + '_'+ str(matrix_GoodDays85[i,0,1:])])\n",
    "\n",
    "    df85_all = pd.concat((df85_all, b), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df85_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df26_all_Matrix = df26_all.to_numpy()\n",
    "df45_all_Matrix = df45_all.to_numpy()\n",
    "df85_all_Matrix = df85_all.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival Map CH2018 RCP2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax3) = plt.subplots(1, 1, figsize=(9.5,7.5))\n",
    "\n",
    "Z = list(df26_all_Matrix[:,1:].astype('float'))\n",
    "#Z = 100*((np.random.rand(900, 118)).T)\n",
    "\n",
    "c3 = ax3.pcolor(Z, cmap='viridis_r')\n",
    "cb3 = fig.colorbar(c3)\n",
    "ax3.set_title('default: no edges')\n",
    "\n",
    "x_axis = np.arange(0, 1050, step=100)\n",
    "y_axis = np.arange(0, 118, step= 10)\n",
    "y_axis_new = np.arange(1981, 2100, step= 10)\n",
    "\n",
    "\n",
    "plt.xticks(x_axis, x_axis)\n",
    "#plt.ylim(1981, 2100)\n",
    "plt.yticks(y_axis, y_axis_new)\n",
    "\n",
    "ax3.set_xlabel(\"68 Climate scenarios of CH2018\")\n",
    "ax3.set_ylabel(\"Years of Simulation\")\n",
    "plt.title(\"Likelihood of survival of the resort from {} untill the year {}\".format(1981, 2099))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival AMap CH2018 RCP4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax3) = plt.subplots(1, 1, figsize=(9.5,7.5))\n",
    "\n",
    "Z = list(df45_all_Matrix[:,1:].astype('float'))\n",
    "#Z = 100*((np.random.rand(900, 118)).T)\n",
    "\n",
    "c3 = ax3.pcolor(Z)\n",
    "cb3 = fig.colorbar(c3)\n",
    "ax3.set_title('default: no edges')\n",
    "\n",
    "x_axis = np.arange(0, 2050, step=100)\n",
    "y_axis = np.arange(0, 118, step= 10)\n",
    "y_axis_new = np.arange(1981, 2100, step= 10)\n",
    "\n",
    "\n",
    "plt.xticks(x_axis, x_axis)\n",
    "#plt.ylim(1981, 2100)\n",
    "plt.yticks(y_axis, y_axis_new)\n",
    "\n",
    "ax3.set_xlabel(\"68 Climate scenarios of CH2018\")\n",
    "ax3.set_ylabel(\"Years of Simulation\")\n",
    "plt.title(\"Likelihood of survival of the resort from {} untill the year {}\".format(1981, 2099))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival Map CH2018 RCP8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax3) = plt.subplots(1, 1, figsize=(12,7.5))\n",
    "\n",
    "Z = list(df85_all_Matrix[:,1:].astype('float'))\n",
    "#Z = 100*((np.random.rand(900, 118)).T)\n",
    "\n",
    "c3 = ax3.pcolor(Z)\n",
    "cb3 = fig.colorbar(c3)\n",
    "ax3.set_title('default: no edges')\n",
    "\n",
    "x_axis = np.arange(0, 2900, step=300)\n",
    "y_axis = np.arange(0, 118, step= 10)\n",
    "y_axis_new = np.arange(1981, 2100, step= 10)\n",
    "\n",
    "\n",
    "plt.xticks(x_axis, x_axis)\n",
    "#plt.ylim(1981, 2100)\n",
    "plt.yticks(y_axis, y_axis_new)\n",
    "\n",
    "ax3.set_xlabel(\" Climate scenarios of CH2018\")\n",
    "ax3.set_ylabel(\"Years of Simulation\")\n",
    "plt.title(\"Likelihood of survival of the resort from {} untill the year {}\".format(1981, 2099))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 years of not having good snow days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tipping_points_freq(df):\n",
    "    \"\"\"\n",
    "    This function, calculates the frequency of tipping points for each individual resort\n",
    "    \"\"\"\n",
    "    dfColumns= df.columns\n",
    "    \n",
    "    scenarios_length= len(dfColumns)\n",
    "    simulations_Length = len(df[dfColumns[1]])\n",
    "    tipping_freq = np.zeros(scenarios_length)\n",
    "    \n",
    "    for i in range (1, scenarios_length, 1):\n",
    "        m = 0\n",
    "        for j in range (1 , simulations_Length, 1):\n",
    "            #df26_all[df26_allColumns[1]].iloc[0]\n",
    "            if float(df[dfColumns[i]].iloc[j]) < 100.0:\n",
    "                m += 1\n",
    "                if m == 3:\n",
    "                    tipping_freq[i] += 1\n",
    "                    m = 0\n",
    "            else:\n",
    "                #if m >= 3:\n",
    "                #if m == 3:\n",
    "                    #tipping_freq[i] += 1\n",
    "                m = 0\n",
    "                continue    \n",
    "                #break\n",
    "    return tipping_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df26_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df26_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df26_allColumns = df26_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df26_all[df26_allColumns[1]].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df26_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1RCP26 = tipping_points_freq(df26_all)\n",
    "T1RCP45 = tipping_points_freq(df45_all)\n",
    "T1RCP85 = tipping_points_freq(df85_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df26_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(T1RCP26[:]))\n",
    "print(len(T1RCP45[:]))\n",
    "print(len(T1RCP85[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(T1RCP85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(1,6004)\n",
    "x26_axis = np.arange(1,901)\n",
    "x45_axis = np.arange(902,2752)\n",
    "x85_axis = np.arange(2753,6003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(15.5,7.5))\n",
    "\n",
    "ax1.scatter(x_axis[1:1052],T1RCP26[:], s = 100, c='b' , marker='o', alpha=0.3, cmap='viridis', \n",
    "           edgecolors='none', label='RCP26')\n",
    "\n",
    "ax1.scatter(x_axis[1052:3103],T1RCP45[:], s = 200, c='orange' , marker='o', alpha=0.3, cmap='viridis', \n",
    "           edgecolors='none', label='RCP45')\n",
    "\n",
    "ax1.scatter(x_axis[3103:6004],T1RCP85[:-1], s = 300, c='r' , marker='o', alpha=0.3, cmap='viridis', \n",
    "           edgecolors='none', label='RCP85')\n",
    "\n",
    "\n",
    "ax1.set_title(\"Frequency of tipping points (failure of the system) in Worst Case\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_xlabel(\"68 Climate Scenarios\")\n",
    "ax1.set_ylim(bottom=0, top =40)\n",
    "ax1.axhline(y=10, color='red')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "fig.savefig(os.path.join(rootVisualization, 'tipping_point_All.svg'), format='svg', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables of the paper: Uncertainty analysis of CH2018 randomness scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import simps\n",
    "from numpy import trapz\n",
    "from decimal import Decimal, ROUND_DOWN, ROUND_UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_max_min_df (df, lengthSim):\n",
    "\n",
    "    max_scenario = np.array(df.max(axis=1))\n",
    "    min_scenario = np.array(df.min(axis=1))\n",
    "    \n",
    "    # Compute the area using the composite trapezoidal rule.\n",
    "    areamax1 = trapz(max_scenario, dx=1)\n",
    "    areamin1 = trapz(min_scenario, dx=1)\n",
    "    Uncertainty1 = ((areamax1 + areamin1)/2 )/ lengthSim\n",
    "    \n",
    "    # Compute the area using the composite Simpson's rule.\n",
    "    areamax2 = simps(max_scenario, dx=1)\n",
    "    areamin2 = simps(min_scenario, dx=1)\n",
    "    Uncertainty2 = ((areamax2 + areamin2)/2 )/ lengthSim\n",
    "    \n",
    "    print(\"Start\" + \"**\" * 30)\n",
    "    print(\"Uncertainty Band1 ={} {}\".format(Decimal(str(Uncertainty1)).quantize(Decimal('.01'), rounding=ROUND_UP), \"days\"))\n",
    "    print(\"Uncertainty Band2 ={} {}\".format(Decimal(str(Uncertainty2)).quantize(Decimal('.01'), rounding=ROUND_UP), \"days\"))\n",
    "    print(\"**\" * 20 +\"End\" + \"\\n\")\n",
    "\n",
    "    return\n",
    "    #return    max_scenario, min_scenario, Uncertainty1, Uncertainty2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scenario = df26_all.max(axis=1)\n",
    "min_scenario = df26_all.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scenario.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scenario.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resort3 Worst Case\")\n",
    "area_max_min_df (df26_all, 118)\n",
    "area_max_min_df (df45_all, 118)\n",
    "area_max_min_df (df85_all, 118)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python3_UZH",
   "language": "python",
   "name": "python3_uzh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
