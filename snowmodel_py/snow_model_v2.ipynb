{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path \n",
    "from datetime import datetime, date, timedelta\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',69)\n",
    "pd.set_option('display.max_rows',138)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: function for initiating the main dictionary of climate stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dic(a):\n",
    "    '''Function: creating a dictionary for each climate station'''\n",
    "    \n",
    "    a = {}\n",
    "    keys = ['fM', 'iPot', 'rSnow', 'dSnow', 'cPrec', 'dP', 'elev', 'lat', 'long', 'fileName']\n",
    "    a = {key: None for key in keys}\n",
    "    return a\n",
    "\n",
    "def initialize_input_dict (mainFolderSki):\n",
    "    ''' This function returns a dictionary , and addresses of 4 folders'''\n",
    "    \n",
    "    \n",
    "    '''Step 1''' \n",
    "    rootFolder = mainFolderSki\n",
    "    inputFolder = os.path.join(rootFolder,'input')\n",
    "    ablationFolder = os.path.join(inputFolder, 'Ablation')\n",
    "    accumulationFolder = os.path.join(inputFolder, 'Accumulation')\n",
    "    climate_ref_Folder = os.path.join(inputFolder, 'Climate_ref')\n",
    "    \n",
    "    \n",
    "    '''Step 2: Reading all files names inside the Ablation, Accumulation, and Climate folders'''  \n",
    "    ablationFiles = []\n",
    "    for filename in os.walk(ablationFolder):\n",
    "        ablationFiles = filename[2]\n",
    "    \n",
    "    accumulationFiles = list()\n",
    "    for filename in os.walk(accumulationFolder):\n",
    "        accumulationFiles = filename[2]\n",
    "\n",
    "    climate_ref_Files = list()\n",
    "    for filename in os.walk(climate_ref_Folder):\n",
    "        climate_ref_Files = filename[2]\n",
    "        \n",
    "        \n",
    "    '''Step 3: Reading files inside ablation folder '''\n",
    "    os.chdir(ablationFolder)\n",
    "    with open(ablationFiles[0], 'r') as file:\n",
    "        FM1 = file.read()\n",
    "    with open(ablationFiles[1], 'r') as file:\n",
    "        Ipot1 = file.read()\n",
    "    with open(ablationFiles[2], 'r') as file:\n",
    "        Rsnow1 = file.read()\n",
    "        \n",
    "        \n",
    "    '''Step 4: Reading the lines of files inside ablation folder'''\n",
    "    FM1 = FM1.replace('\\n', '\\t')\n",
    "    FM1 = FM1.split('\\t')\n",
    "    Ipot1 = Ipot1.replace('\\n', '\\t').split('\\t')\n",
    "    Rsnow1 = Rsnow1.replace('\\n', '\\t').split('\\t')\n",
    "        \n",
    "        \n",
    "    '''Step 5: Reading the lines of files inside accumulation folder''' \n",
    "    os.chdir(accumulationFolder)\n",
    "    \n",
    "    with open(accumulationFiles[0], 'r') as file:\n",
    "        cPrec = file.read()\n",
    "    with open(accumulationFiles[1], 'r') as file:\n",
    "        dSnow1 = file.read()\n",
    "    \n",
    "    cPrec = cPrec.replace('\\n', '\\t')\n",
    "    cPrec = cPrec.split('\\t')\n",
    "    dSnow1 = dSnow1.replace('\\n', '\\t').split('\\t')\n",
    "    \n",
    "    \n",
    "    '''Step 6: Reading the lines of files inside climate folder''' \n",
    "    os.chdir(climate_ref_Folder)\n",
    "    \n",
    "    with open('pcp.txt', 'r') as file:\n",
    "        pcpData = file.read()\n",
    "    with open('tmp.txt', 'r') as file:\n",
    "        tmpData = file.read()\n",
    "        \n",
    "    pcpData = pcpData.split('\\n')\n",
    "    \n",
    "    for i in range(len(pcpData)):\n",
    "        pcpData[i] = pcpData[i].split(',')\n",
    "        \n",
    "        \n",
    "    '''Step 7: Initialazing the input dictionary of climate stations which holds the information of accumulation\n",
    "     and ablation, and etc of the stations''' \n",
    "    nameStn = []\n",
    "    for file in climate_ref_Files:\n",
    "        if 'p.csv' in file:\n",
    "            #nameStn.append('n_' + file[-25: -5])\n",
    "            nameStn.append(file[-25: -5])\n",
    "\n",
    "    stnDicts = []\n",
    "    for i in range(len(nameStn)):\n",
    "        stnDicts.append(create_dic(nameStn[i]))\n",
    "    \n",
    "    \n",
    "    '''Step 8: Assigning the file names to the dictionary'''\n",
    "    for i in range (len(nameStn)):\n",
    "        stnDicts[i]['fileName'] = nameStn[i]\n",
    "\n",
    "    \n",
    "    '''Step 9: Assigning the accumulation and ablation values'''\n",
    "    for stnDict in stnDicts:\n",
    "        for i, element in enumerate(FM1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['fM'] = FM1[i+1]\n",
    "                \n",
    "        for i, element in enumerate(Ipot1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['iPot'] = Ipot1[i+1]\n",
    "\n",
    "        for i, element in enumerate(Rsnow1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:  \n",
    "                stnDict['rSnow'] = Rsnow1[i+1]\n",
    "\n",
    "        for i, element in enumerate(dSnow1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['dSnow'] = dSnow1[i+1]\n",
    "\n",
    "        for i, element in enumerate(cPrec):\n",
    "            stnDict['cPrec'] = cPrec[1]\n",
    "            stnDict['dP'] = cPrec[3]\n",
    "            \n",
    "    '''Step 10: Assigning the elevation, Lat and long to the dictionaries'''\n",
    "    for i in range(len(stnDicts)):\n",
    "        for j in range(1, len(pcpData)):\n",
    "            \n",
    "            #if pcpData[j][1][2:-1] == stnDicts[i]['fileName'][2:]:\n",
    "            if pcpData[j][1][:-1] == stnDicts[i]['fileName'][:]:\n",
    "                stnDicts[i]['lat']= pcpData[j][2]\n",
    "                stnDicts[i]['long']= pcpData[j][3]\n",
    "                stnDicts[i]['elev']= pcpData[j][4]\n",
    "                \n",
    "    return stnDicts, inputFolder, ablationFolder, accumulationFolder, climate_ref_Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Initializiing the main dictionary for a case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseStudyStns = {}\n",
    "inputFolder = ''\n",
    "ablationFolder = ''\n",
    "accumulationFolder = ''\n",
    "climateFolder = ''\n",
    "#root = 'C:/Users/ashrafse/SA_2/snowModelUZH/case2_Atzmaening'\n",
    "root = 'C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations'\n",
    "\n",
    "## calling the function with multiple return values\n",
    "caseStudyStns, inputFolder, ablationFolder, accumulationFolder, climateFolder = initialize_input_dict(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fM': '1.04',\n",
       "  'iPot': '1100',\n",
       "  'rSnow': '0.5',\n",
       "  'dSnow': '1',\n",
       "  'cPrec': '0',\n",
       "  'dP': '0',\n",
       "  'elev': '2141',\n",
       "  'lat': '46.7500000',\n",
       "  'long': '9.8125000',\n",
       "  'fileName': '46-7500000_9-8125000'},\n",
       " {'fM': '1.09',\n",
       "  'iPot': '1003',\n",
       "  'rSnow': '0.5',\n",
       "  'dSnow': '1',\n",
       "  'cPrec': '0',\n",
       "  'dP': '0',\n",
       "  'elev': '1564',\n",
       "  'lat': '46.7708333',\n",
       "  'long': '9.7916667',\n",
       "  'fileName': '46-7708333_9-7916667'},\n",
       " {'fM': '1.01',\n",
       "  'iPot': '1040',\n",
       "  'rSnow': '0.5',\n",
       "  'dSnow': '1',\n",
       "  'cPrec': '0',\n",
       "  'dP': '0',\n",
       "  'elev': '2584',\n",
       "  'lat': '46.7708333',\n",
       "  'long': '9.8541667',\n",
       "  'fileName': '46-7708333_9-8541667'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseStudyStns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Matching the station names values in the dictionary of stations with CSV files in Climate folder of the case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pcpAtzmaening = []\n",
    "#tmpAtzmaening = []\n",
    "pcpCaseStudy = []\n",
    "tmpCaseStudy = []\n",
    "\n",
    "for i in range(len(caseStudyStns)):\n",
    "    pcpCaseStudy.append(os.path.join(climateFolder, caseStudyStns[i]['fileName'] + 'p.csv'))\n",
    "    tmpCaseStudy.append(os.path.join(climateFolder, caseStudyStns[i]['fileName'] + 't.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations\\\\input\\\\Climate_ref\\\\46-7500000_9-8125000t.csv',\n",
       " 'C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations\\\\input\\\\Climate_ref\\\\46-7708333_9-7916667t.csv',\n",
       " 'C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations\\\\input\\\\Climate_ref\\\\46-7708333_9-8541667t.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpCaseStudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations\\\\input\\\\Climate_ref\\\\46-7500000_9-8125000p.csv',\n",
       " 'C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations\\\\input\\\\Climate_ref\\\\46-7708333_9-7916667p.csv',\n",
       " 'C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations\\\\input\\\\Climate_ref\\\\46-7708333_9-8541667p.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcpCaseStudy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: building a database for each precipitation and temperature file in Climate folder and saving them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpcp = [None for _ in range(len(pcpCaseStudy))]\n",
    "dftmp = [None for _ in range(len(tmpCaseStudy))]\n",
    "for i in range(len(pcpCaseStudy)):\n",
    "    dfpcp[i] = pd.read_csv(pcpCaseStudy[i])\n",
    "    dftmp[i] = pd.read_csv(tmpCaseStudy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### making a header for output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpcpCol = dfpcp[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftmpCol = dftmp[0].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining the length of simulations and scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenariosLength = len(dfpcpCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulationLength = len(dftmp[0][dftmpCol[0]]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Main Snow Model\n",
    "\n",
    "### Part one daily outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Model\n",
    "\n",
    "## 1st column as index: makaing date from 01 01 1981 to 2099 12 31\n",
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date ).days + 1)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "## Reading the beginning and end of the simulation\n",
    "start_date = date(1981, 1, 1)\n",
    "end_date = date(2099, 12, 31)\n",
    "dateList = []\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    dateList.append(single_date.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "\n",
    "## Running the model for each climate station:\n",
    "for k in range(len(caseStudyStns)):\n",
    "    \n",
    "    ## making a header for output files\n",
    "    dfpcpCol = dfpcp[k].columns\n",
    "    dftmpCol = dftmp[k].columns\n",
    "    \n",
    "    \n",
    "    ## defining the length of simulations and scenarios\n",
    "    scenariosLength = len(dfpcpCol)\n",
    "    simulationLength = len(dftmp[0][dftmpCol[0]]) - 1\n",
    "    \n",
    "    \n",
    "    ## declaring the initial arrays\n",
    "    accumulation = [0 for _ in range(simulationLength)]\n",
    "    ablation =  [0 for _ in range(simulationLength)]\n",
    "    snowDeposite = [0 for _ in range(simulationLength)]\n",
    "    total = np.zeros([simulationLength, 3*scenariosLength])\n",
    "    \n",
    "    \n",
    "    ## Running the model for each climate scenario:\n",
    "    for j in range(len(dfpcpCol)):\n",
    "        ## Reading the information and inputs of the first day of simulation\n",
    "        todayPCP = dfpcp[k][dfpcpCol[j]].iloc[1] if (dfpcp[k][dfpcpCol[j]].iloc[1] != -99) else 0\n",
    "        todayTMPMAX = round(dftmp[k][dftmpCol[2*j]].iloc[1],2) if(dftmp[k][dftmpCol[2*j]].iloc[1] != -99) else 0\n",
    "        todayTMPMIN = round(dftmp[k][dftmpCol[2*j+1]].iloc[1],2) if(dftmp[k][dftmpCol[2*j+1]].iloc[1] != -99) else 0\n",
    "        todayTMPAVE = round((todayTMPMAX+todayTMPMIN)/2,2) if((todayTMPMAX+todayTMPMIN)/2 != -99) else 0\n",
    "\n",
    "\n",
    "        ### Accumulation for the first day:\n",
    "        if (todayTMPAVE) <=-1:\n",
    "            accumulation[0] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*(1)\n",
    "\n",
    "        elif -1 < (todayTMPAVE) <= 1:\n",
    "            accumulation[0] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*float((1-todayTMPAVE)/2)\n",
    "\n",
    "        else: accumulation[0] = 0\n",
    "\n",
    "\n",
    "        ### Ablation for the first day:\n",
    "        if todayTMPAVE <= 1:\n",
    "             ablation[0] = 0\n",
    "        else: \n",
    "            ablation[0] = (float(caseStudyStns[k]['fM']) + float(caseStudyStns[k]['rSnow'])*float(caseStudyStns[k]['iPot'])*0.001)*float(todayTMPAVE)*(1+0)\n",
    "\n",
    "        ### Main mass balance equation for the first day:\n",
    "        snowDeposite[0] = 0 if (0 + accumulation[0] - ablation[0]) < 0 else (0 + accumulation[0] - ablation[0])\n",
    "\n",
    "        ### storing three values in a list for the first day\n",
    "        total[0,3*j+0] = round((accumulation[0] - ablation[0]), 2)\n",
    "        total[0,3*j+1] = round(snowDeposite[0], 2)\n",
    "        total[0,3*j+2] = 1 if (total[0,3*j+1] > 300) else 0\n",
    "\n",
    "        \n",
    "        ## For the second day to the end of simulation:\n",
    "        i = 0\n",
    "        for i in range(2, simulationLength + 1, 1):\n",
    "            # precipitation and temperature missing values were handled\n",
    "\n",
    "            ## Reading the information and inputs of the first day of simulation\n",
    "            todayPCP = dfpcp[k][dfpcpCol[j]].iloc[i] if (dfpcp[k][dfpcpCol[j]].iloc[i] != -99) else 0\n",
    "            todayTMPMAX = round(dftmp[k][dftmpCol[2*j]].iloc[i],2) if(dftmp[k][dftmpCol[2*j]].iloc[i] != -99) else 0\n",
    "            todayTMPMIN = round(dftmp[k][dftmpCol[2*j+1]].iloc[i],2) if(dftmp[k][dftmpCol[2*j+1]].iloc[i] != -99) else 0\n",
    "            todayTMPAVE = round((todayTMPMAX+todayTMPMIN)/2,2) if((todayTMPMAX+todayTMPMIN)/2 != -99) else 0\n",
    "\n",
    "            ### Accumulation :\n",
    "            if(todayTMPAVE) <= -1:\n",
    "                ##\n",
    "                accumulation[i-1] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*(1)\n",
    "\n",
    "            elif -1 < (todayTMPAVE) <= 1:\n",
    "                accumulation[i-1] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*float((1-todayTMPAVE)/2)\n",
    "\n",
    "            else: accumulation[i-1] = 0\n",
    "\n",
    "            ### Ablation :\n",
    "            if todayTMPAVE <= 0:\n",
    "                ablation[i-1] = 0\n",
    "            else: \n",
    "                ablation[i-1] = (float(caseStudyStns[k]['fM']) + float(caseStudyStns[k]['rSnow'])*float(caseStudyStns[k]['iPot'])*0.001)*float(todayTMPAVE)*(1+0)\n",
    "\n",
    "            ### Main mass balance equation for second day to the end of simulation:\n",
    "            snowDeposite[i-1] = 0 if (snowDeposite[i-2] + accumulation[i-1] - ablation[i-1]) < 0 else (snowDeposite[i-2] + accumulation[i-1] - ablation[i-1])\n",
    "\n",
    "\n",
    "            ### storing three values in a list \n",
    "            total[i-1,3*j+0] = round((accumulation[i-1] - ablation[i-1]) , 2)\n",
    "            total[i-1,3*j+1] = round(snowDeposite[i-1], 2)\n",
    "            total[i-1,3*j+2] = 1 if (total[i-1,3*j+1] > 300) else 0\n",
    "\n",
    "            \n",
    "    ### Saving the output of total list in a csv file in a specific path\n",
    "    \n",
    "    ## 1st row as the column names:\n",
    "    columnsDF = [] \n",
    "    for col in dfpcpCol:\n",
    "        columnsDF.append('SnowAmount_' + col)\n",
    "        columnsDF.append('TotalSnowAmount_' + col)\n",
    "        columnsDF.append('isOverSnow_' + col)\n",
    "    \n",
    "    \n",
    "    columnsDF0 = ['DATE']\n",
    "    dfnew0 = pd.DataFrame(dateList, columns = columnsDF0)\n",
    "    dfnew = pd.DataFrame(total, columns = columnsDF)\n",
    "    df1 = pd.concat([dfnew0, dfnew], axis=1, sort=False)\n",
    "\n",
    "    if os.path.isdir(os.path.join(root, 'Outputs_py')):\n",
    "        pass\n",
    "    else: os.mkdir(os.path.join(root, 'Outputs_py'))\n",
    "\n",
    "    outfolder =os.path.join(root, 'Outputs_py') \n",
    "    outfileName = 'Total_daily_' + caseStudyStns[k]['fileName'] + '.csv'\n",
    "    outputFile = os.path.join(outfolder, outfileName )\n",
    "    df1.to_csv(outputFile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part two: seasonal outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_Daily_Files = list()\n",
    "for filename in os.walk(outfolder):\n",
    "    total_Daily_Files = filename[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalFiles = []\n",
    "for i in range(len(total_Daily_Files)):\n",
    "    totalFiles.append(os.path.join(outfolder, total_Daily_Files[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSeason = [ None for _ in range(len(totalFiles))]\n",
    "for i in range(len(totalFiles)):\n",
    "    dfSeason[i] = pd.read_csv(totalFiles[i], low_memory=False)\n",
    "               \n",
    "    start_date = date(1981, 1, 2)\n",
    "    end_date = date(2099, 12, 31)\n",
    "    dateList = []\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        dateList.append(single_date.strftime(\"%m/%d/%Y\"))\n",
    "        \n",
    "    start_season = []\n",
    "    end_season = []\n",
    "\n",
    "    for pp in range (1981, 2099, 1):\n",
    "        #start_season.append(date(i, 1, 1))\n",
    "        #end_season.append(date(i+1, 12, 31)) if i != 2099 else end_season.append(date(i, 12, 30))\n",
    "        start_season.append(date(pp, 11, 1))\n",
    "        end_season.append(date(pp+1, 4, 30))\n",
    "        \n",
    "    df2 = dfSeason[i]\n",
    "    df2.set_index('DATE', inplace = True)\n",
    "    df2Col = df2.columns\n",
    "    \n",
    "    df2ColCal = []\n",
    "    for m in range(68):\n",
    "    #for k in range(68):\n",
    "        df2ColCal.append(df2Col[3*m+2])\n",
    "        \n",
    "    sumGoodCondition = np.zeros([len(start_season), len(df2ColCal)])\n",
    "    \n",
    "    \n",
    "    for j in range(len(df2ColCal)):\n",
    "        for k in range(len(start_season)):\n",
    "        #for i in range(3):\n",
    "            start_date = start_season[k]\n",
    "            end_date = end_season[k]\n",
    "                #start_date = date(1981, 1, 2)\n",
    "                #end_date = date(1981, 1, 10)\n",
    "            for single_date in daterange(start_date, end_date):\n",
    "                #sumGood1[i] += df2['TotalSnowAmount_CLMCOM-CCLM4-ECEARTH-EUR11-RCP45-pcp'].loc[single_date.strftime(\"%m/%d/%Y\")]\n",
    "                sumGoodCondition[k,j] += df2[df2ColCal[j]].loc[single_date.strftime(\"%m/%d/%Y\")]\n",
    "                    #print(single_date.strftime(\"%Y-%m-%d\"))\n",
    "                    \n",
    "    df3 = pd.DataFrame(sumGoodCondition, columns = df2ColCal)\n",
    "    \n",
    "    seasonList = []\n",
    "\n",
    "    for n in range (1981, 2100, 1):\n",
    "        seasonList.append(str(n))\n",
    "        \n",
    "    firstCol = []\n",
    "    for o in range (len(seasonList)-1):\n",
    "        firstCol.append(seasonList[o] +'-' + seasonList[o+1])\n",
    "    \n",
    "    columnsDF1 = ['Season']\n",
    "    dfnew0 = pd.DataFrame(firstCol, columns = columnsDF1)\n",
    "    \n",
    "    dfFinalSeason = pd.concat([dfnew0, df3], axis=1, sort=False)\n",
    "        \n",
    "    outfileNameSeason = 'season_' + total_Daily_Files[i]\n",
    "    outputFile = os.path.join(outfolder, outfileNameSeason)\n",
    "    dfFinalSeason.to_csv(outputFile, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
