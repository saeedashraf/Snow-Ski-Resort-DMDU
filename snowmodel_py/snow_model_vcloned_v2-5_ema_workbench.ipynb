{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reading the libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "from operator import add\n",
    "from datetime import datetime, date, timedelta\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Seting up the display extent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',69)\n",
    "pd.set_option('display.max_rows',138)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up Climate Scenarios (CH2018, and Random Scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leap(year):\n",
    "    \"\"\" return true for leap years, False for non leap years \"\"\"\n",
    "    return year % 4 == 0 and ( year % 100 != 0 or year % 400 == 0)\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1.1. *Function that pruduces new climate (precipitation) realization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pcp(dfpcp, FirstYear, LastYear, ScenarioNumbers, RCPNames, Xfactor1):\n",
    "    \n",
    "    #outFileName = OutFileName\n",
    "    dfpcpCol = dfpcp.columns\n",
    "    #sceNum = len(dfCol)\n",
    "\n",
    "    sceNum = ScenarioNumbers\n",
    "    firstYear = FirstYear\n",
    "    lastYear = LastYear\n",
    "    simLen = lastYear - firstYear + 1\n",
    "\n",
    "    from random import shuffle\n",
    "    #a = []\n",
    "    #for x in range(simLen): \n",
    "        #randomInd = [z for z in range(sceNum)]\n",
    "        #shuffle(randomInd)\n",
    "        #a.append(randomInd)\n",
    "        \n",
    "        \n",
    "    a = []\n",
    "    for i in range(simLen): \n",
    "        randomInd = [z for z in range(sceNum)]\n",
    "        #x = [[i] for i in range(10)] \n",
    "        for x in range(int(round(Xfactor1))):\n",
    "            shuffle(randomInd)\n",
    "        a.append(randomInd)\n",
    "    \n",
    "    \n",
    "        \n",
    "    RCP = RCPNames\n",
    "    columnsDfpcp = ['sc_' + RCP + str(k) for k in range(1, sceNum+1,1)] \n",
    "    c = [[19810101 for p in range(sceNum)]]\n",
    "    #df1 = 'df' + str(outDFNumber)\n",
    "    df1pcp = pd.DataFrame(c, columns =columnsDfpcp)\n",
    "    #df1.to_csv('SAeidVaghefimodified1111222.csv', index = False)\n",
    "\n",
    "    c.clear()\n",
    "\n",
    "    i = 0\n",
    "    p = 1\n",
    "    for m in range(firstYear, lastYear + 1, 1):\n",
    "        if is_leap(m):\n",
    "            for j in range(((m - firstYear)*365+p) , ((m - firstYear)*365+367+p-1), 1):\n",
    "                c.append(dfpcp[dfpcpCol[a[i]]].iloc[j].values) \n",
    "            i += 1 # counter i; equal to simulation length (simLen)\n",
    "            p += 1\n",
    "\n",
    "        else:\n",
    "            for j in range(((m - firstYear)*365+p), ((m - firstYear)*365+366+p-1), 1):\n",
    "                c.append(dfpcp[dfpcpCol[a[i]]].iloc[j].values) \n",
    "            i += 1\n",
    "                \n",
    "        #print(m) # this line show the progress of the work by typing the years of simulation\n",
    "\n",
    "        dfnewpcp = 'df' + str(m)\n",
    "        dfnewpcp = pd.DataFrame(c, columns =columnsDfpcp)\n",
    "        c.clear()\n",
    "        df1pcp = df1pcp.append(dfnewpcp, ignore_index=True)\n",
    "        \n",
    "    return df1pcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1.2. *Function that pruduces new climate (temperature) realization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tmp(dftmp, FirstYear, LastYear, ScenarioNumbers, RCPNames, Xfactor1):\n",
    "\n",
    "    #dfCol = df.columns\n",
    "    #sceNum = len(dfCol) // 2\n",
    "    sceNum = ScenarioNumbers\n",
    "    firstYear = FirstYear\n",
    "    lastYear = LastYear\n",
    "    simLen = lastYear - firstYear + 1\n",
    "\n",
    "    dftmpColMax = dftmp.columns[[i for i in range(0, sceNum*2, 2)]]\n",
    "    dftmpColMin = dftmp.columns[[i for i in range(1, sceNum*2, 2)]]\n",
    "\n",
    "    ## yek list be toole 119 ke dakhelesh list haye 68 ta ee darim be soorate random\n",
    "    from random import shuffle\n",
    "    #a = []\n",
    "    #for i in range(simLen): \n",
    "        #randomInd = [j for j in range(sceNum)]\n",
    "        #x = [[i] for i in range(10)] \n",
    "        #shuffle(randomInd)\n",
    "        #a.append(randomInd)\n",
    "        \n",
    "    \n",
    "    a = []\n",
    "    for i in range(simLen): \n",
    "        randomInd = [j for j in range(sceNum)]\n",
    "        #x = [[i] for i in range(10)] \n",
    "        for x in range(int(round(Xfactor1))):\n",
    "            shuffle(randomInd)\n",
    "        a.append(randomInd)\n",
    "    \n",
    "        \n",
    "    #print('end!')\n",
    "\n",
    "    cT = []\n",
    "    RCP = RCPNames\n",
    "    columnsDfOdd = ['sc_' + RCP + str(k)  for k in range(1, sceNum+1,1)] \n",
    "    columnsDfEven = [\"\"] * sceNum\n",
    "\n",
    "    columnsDftmp = []\n",
    "    #colOdd = ['Scr_' + str(i) for i in range(1, sceNum+1, 1)]\n",
    "    #colEven = ['' for i in range(1, sceNum+1,1)]\n",
    "\n",
    "    for i in range (sceNum):\n",
    "        columnsDftmp.append(columnsDfOdd[i])\n",
    "        columnsDftmp.append(columnsDfEven[i])\n",
    "\n",
    "\n",
    "    #### OR:\n",
    "    #columnsDf = [\"Sr\", \"\"] * sceNum\n",
    "    df1tmp = pd.DataFrame(cT, columns =columnsDftmp)\n",
    "    #df1.to_csv(\"rrrrrrrrmodified1111222.csv\", index = False)\n",
    "\n",
    "\n",
    "    cMax = [[19810101 for p in range(sceNum)]]\n",
    "    cMin = [[\"\" for p in range(sceNum)]]\n",
    "    i = 0\n",
    "    p = 1\n",
    "    for m in range(firstYear, lastYear + 1, 1):\n",
    "        if is_leap(m):\n",
    "            for j in range(((m - firstYear)*365+p) , ((m - firstYear)*365+367+p-1), 1):\n",
    "                cMax.append(dftmp[dftmpColMax[a[i]]].iloc[j].values)\n",
    "                cMin.append(dftmp[dftmpColMin[a[i]]].iloc[j].values) \n",
    "    \n",
    "            i += 1\n",
    "\n",
    "        else:\n",
    "            for j in range(((m - firstYear)*365+p), ((m - firstYear)*365+366+p-1), 1):\n",
    "                cMax.append(dftmp[dftmpColMax[a[i]]].iloc[j].values)\n",
    "                cMin.append(dftmp[dftmpColMin[a[i]]].iloc[j].values)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        c = []\n",
    "        for y in range(0, len(cMax), 1): # the length of simulation years\n",
    "            for z in range(sceNum): # range(4)\n",
    "                c.append(cMax[y][z])\n",
    "                c.append(cMin[y][z])\n",
    "\n",
    "        cMax.clear()\n",
    "        cMin.clear()\n",
    "\n",
    "        cMain = []\n",
    "        cMain = list(chunks(c, sceNum * 2))\n",
    "        #print(m) # this line show the progress of the work by typing the years of simulation\n",
    "\n",
    "    ### Should be checked\n",
    "\n",
    "        dfnewtmp = 'dftmp' + str(m)\n",
    "        #columnsDf = [\"Sr\", \"\"]*sceNum\n",
    "        #columnsDf = [['sc_' + str(k), \"\"] for k in range(1, sceNum+1,1)] \n",
    "        dfnewtmp = pd.DataFrame(cMain, columns =columnsDftmp)\n",
    "        c.clear()\n",
    "        df1tmp = df1tmp.append(dfnewtmp, ignore_index=True)\n",
    "        \n",
    "    return df1tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1.3. *Function that calls the random_pcp and random_tmp for all stations of a Ski resort*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomness_pcp_tmp(fnames, Xfactor1):\n",
    "    for f in fnames:\n",
    "        if 'p.csv' in f:\n",
    "            print('Writing pcp files started!')\n",
    "            #df = pd.read_csv('47-0625000_8-6666667p.csv')\n",
    "            dfpcp = pd.read_csv(f)\n",
    "\n",
    "\n",
    "            filt1 = dfpcp.columns.str.contains('RCP26|_26_') #12\n",
    "            filt2 = dfpcp.columns.str.contains('RCP45|_45_') #25\n",
    "            filt3 = dfpcp.columns.str.contains('RCP85|_85_') #31\n",
    "\n",
    "            dfpcpRCP26 = dfpcp.loc[:, filt1]\n",
    "            dfpcpRCP45 = dfpcp.loc[:, filt2]\n",
    "            dfpcpRCP85 = dfpcp.loc[:, filt3]\n",
    "\n",
    "            dfpcpRCP26_n = random_pcp(dfpcpRCP26, 1981, 2099, 12, '26_', Xfactor1)\n",
    "            dfpcpRCP45_n = random_pcp(dfpcpRCP45, 1981, 2099, 25, '45_', Xfactor1)\n",
    "            dfpcpRCP85_n = random_pcp(dfpcpRCP85, 1981, 2099, 31, '85_', Xfactor1)\n",
    "\n",
    "\n",
    "            result = pd.concat([dfpcpRCP26_n, dfpcpRCP45_n, dfpcpRCP85_n], axis=1, sort=False)\n",
    "            #result.to_csv('47-0625000_8-6666667p_n1.csv', index = False)\n",
    "\n",
    "\n",
    "            #newName = 'n_'+ f\n",
    "            newName = f\n",
    "            #filepath = os.path.join(os.getcwd(), newName)\n",
    "            root = os.getcwd()\n",
    "            \n",
    "            '''This part makes a new dir for outouts''' ## should be cooment out later\n",
    "            #if os.path.isdir(os.path.join(root, 'Outputs_randomness')):\n",
    "                #pass\n",
    "            #else: os.mkdir(os.path.join(root, 'Outputs_randomness'))\n",
    "\n",
    "            #outfolder = os.path.join(os.getcwd(), 'Outputs_randomness')\n",
    "            outfolder =os.path.join(os.getcwd()) # we want the results to be over written\n",
    "\n",
    "            filepath = os.path.join(outfolder, newName)\n",
    "\n",
    "            result.to_csv(filepath, index = False)\n",
    "            print('End of writing pcp files!')\n",
    "            #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "        elif 't.csv' in f:\n",
    "            print('Writing tmp files started!')\n",
    "            dftmp = pd.read_csv(f)\n",
    "            dftmpCol = list(dftmp.columns)\n",
    "\n",
    "            filt1_max = [dftmpCol.index(s) for s in dftmpCol if (\"_26_\") in s or (\"RCP26\") in s]\n",
    "            filt2_max = [dftmpCol.index(s) for s in dftmpCol if (\"_45_\") in s or (\"RCP45\") in s]\n",
    "            filt3_max = [dftmpCol.index(s) for s in dftmpCol if (\"_85_\") in s or (\"RCP85\") in s]\n",
    "\n",
    "            aOnefilt1= [1]*len(filt1_max)\n",
    "            aOnefilt2= [1]*len(filt2_max)\n",
    "            aOnefilt3= [1]*len(filt3_max)\n",
    "\n",
    "            filt1_min = list(map(add, filt1_max, aOnefilt1)) # \n",
    "            filt2_min = list(map(add, filt2_max, aOnefilt2))\n",
    "            filt3_min = list(map(add, filt3_max, aOnefilt3))\n",
    "\n",
    "            filt1Tot = []\n",
    "            for i in range(len(filt1_max)):\n",
    "                filt1Tot.append(filt1_max[i])\n",
    "                filt1Tot.append(filt1_min[i])\n",
    "\n",
    "            filt2Tot = []\n",
    "            for j in range(len(filt2_max)):\n",
    "                filt2Tot.append(filt2_max[j])\n",
    "                filt2Tot.append(filt2_min[j])\n",
    "\n",
    "            filt3Tot = []\n",
    "            for k in range(len(filt3_max)):\n",
    "                filt3Tot.append(filt3_max[k])\n",
    "                filt3Tot.append(filt3_min[k])\n",
    "\n",
    "            dftmpRCP26 = dftmp[dftmp.columns[filt1Tot]]\n",
    "            dftmpRCP45 = dftmp[dftmp.columns[filt2Tot]]\n",
    "            dftmpRCP85 = dftmp[dftmp.columns[filt3Tot]]\n",
    "\n",
    "            dftmpRCP26_n = random_tmp (dftmpRCP26, 1981, 2099, 12, '26_', Xfactor1)\n",
    "            dftmpRCP45_n = random_tmp (dftmpRCP45, 1981, 2099, 25, '45_', Xfactor1)\n",
    "            dftmpRCP85_n = random_tmp (dftmpRCP85, 1981, 2099, 31, '85_', Xfactor1)\n",
    "\n",
    "            result = pd.concat([dftmpRCP26_n, dftmpRCP45_n, dftmpRCP85_n], axis=1, sort=False)\n",
    "\n",
    "            #ewName = 'n'+f\n",
    "            #ilepath = os.path.join(os.environ.get('HOME'), newName)\n",
    "            #esult.to_csv(filepath, index = False)\n",
    "\n",
    "            #newName = 'n_'+ f\n",
    "            newName = f\n",
    "            #filepath = os.path.join(os.getcwd(), newName)\n",
    "\n",
    "            #outfolder =os.path.join(os.getcwd(), 'Outputs_randomness')\n",
    "            outfolder =os.path.join(os.getcwd()) # we want the results to be over written\n",
    "            \n",
    "            filepath = os.path.join(outfolder, newName)\n",
    "            result.to_csv(filepath, index = False)\n",
    "\n",
    "            print('End of writing tmp files')\n",
    "\n",
    "        else :\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Function for initiating the main dictionary of climate stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dic(a):\n",
    "    '''Function: creating a dictionary for each climate station'''\n",
    "    \n",
    "    a = {}\n",
    "    keys = ['fM', 'iPot', 'rSnow', 'dSnow', 'cPrec', 'dP', 'elev', 'lat', 'long', 'fileName']\n",
    "    a = {key: None for key in keys}\n",
    "    return a\n",
    "\n",
    "def initialize_input_dict (mainFolderSki):\n",
    "    ''' This function returns a dictionary , and addresses of 4 folders'''\n",
    "    \n",
    "    \n",
    "    '''Step 1''' \n",
    "    rootFolder = mainFolderSki\n",
    "    inputFolder = os.path.join(rootFolder,'input')\n",
    "    ablationFolder = os.path.join(inputFolder, 'Ablation')\n",
    "    accumulationFolder = os.path.join(inputFolder, 'Accumulation')\n",
    "    climate_ref_Folder = os.path.join(inputFolder, 'Climate_ref')\n",
    "    \n",
    "    \n",
    "    '''Step 2: Reading all files names inside the Ablation, Accumulation, and Climate folders'''  \n",
    "    ablationFiles = []\n",
    "    for filename in os.walk(ablationFolder):\n",
    "        ablationFiles = filename[2]\n",
    "    \n",
    "    accumulationFiles = list()\n",
    "    for filename in os.walk(accumulationFolder):\n",
    "        accumulationFiles = filename[2]\n",
    "\n",
    "    climate_ref_Files = list()\n",
    "    for filename in os.walk(climate_ref_Folder):\n",
    "        climate_ref_Files = filename[2]\n",
    "        \n",
    "        \n",
    "    '''Step 3: Reading files inside ablation folder '''\n",
    "    os.chdir(ablationFolder)\n",
    "    with open(ablationFiles[0], 'r') as file:\n",
    "        FM1 = file.read()\n",
    "    with open(ablationFiles[1], 'r') as file:\n",
    "        Ipot1 = file.read()\n",
    "    with open(ablationFiles[2], 'r') as file:\n",
    "        Rsnow1 = file.read()\n",
    "        \n",
    "        \n",
    "    '''Step 4: Reading the lines of files inside ablation folder'''\n",
    "    FM1 = FM1.replace('\\n', '\\t')\n",
    "    FM1 = FM1.split('\\t')\n",
    "    Ipot1 = Ipot1.replace('\\n', '\\t').split('\\t')\n",
    "    Rsnow1 = Rsnow1.replace('\\n', '\\t').split('\\t')\n",
    "        \n",
    "        \n",
    "    '''Step 5: Reading the lines of files inside accumulation folder''' \n",
    "    os.chdir(accumulationFolder)\n",
    "    \n",
    "    with open(accumulationFiles[0], 'r') as file:\n",
    "        cPrec = file.read()\n",
    "    with open(accumulationFiles[1], 'r') as file:\n",
    "        dSnow1 = file.read()\n",
    "    \n",
    "    cPrec = cPrec.replace('\\n', '\\t')\n",
    "    cPrec = cPrec.split('\\t')\n",
    "    dSnow1 = dSnow1.replace('\\n', '\\t').split('\\t')\n",
    "    \n",
    "    \n",
    "    '''Step 6: Reading the lines of files inside climate folder''' \n",
    "    os.chdir(climate_ref_Folder)\n",
    "    \n",
    "    with open('pcp.txt', 'r') as file:\n",
    "        pcpData = file.read()\n",
    "    with open('tmp.txt', 'r') as file:\n",
    "        tmpData = file.read()\n",
    "        \n",
    "    pcpData = pcpData.split('\\n')\n",
    "    \n",
    "    for i in range(len(pcpData)):\n",
    "        pcpData[i] = pcpData[i].split(',')\n",
    "        \n",
    "        \n",
    "    '''Step 7: Initialazing the input dictionary of climate stations which holds the information of accumulation\n",
    "     and ablation, and etc of the stations''' \n",
    "    nameStn = []\n",
    "    for file in climate_ref_Files:\n",
    "        if 'p.csv' in file:\n",
    "            #nameStn.append('n_' + file[-25: -5])\n",
    "            nameStn.append(file[-25: -5])\n",
    "\n",
    "    stnDicts = []\n",
    "    for i in range(len(nameStn)):\n",
    "        stnDicts.append(create_dic(nameStn[i]))\n",
    "    \n",
    "    \n",
    "    '''Step 8: Assigning the file names to the dictionary'''\n",
    "    for i in range (len(nameStn)):\n",
    "        stnDicts[i]['fileName'] = nameStn[i]\n",
    "\n",
    "    \n",
    "    '''Step 9: Assigning the accumulation and ablation values'''\n",
    "    for stnDict in stnDicts:\n",
    "        for i, element in enumerate(FM1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['fM'] = FM1[i+1]\n",
    "                \n",
    "        for i, element in enumerate(Ipot1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['iPot'] = Ipot1[i+1]\n",
    "\n",
    "        for i, element in enumerate(Rsnow1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:  \n",
    "                stnDict['rSnow'] = Rsnow1[i+1]\n",
    "\n",
    "        for i, element in enumerate(dSnow1):\n",
    "            if element == stnDict['fileName'][:]:\n",
    "            #if element == stnDict['fileName'][2:]:\n",
    "                stnDict['dSnow'] = dSnow1[i+1]\n",
    "\n",
    "        for i, element in enumerate(cPrec):\n",
    "            stnDict['cPrec'] = cPrec[1]\n",
    "            stnDict['dP'] = cPrec[3]\n",
    "            \n",
    "    '''Step 10: Assigning the elevation, Lat and long to the dictionaries'''\n",
    "    for i in range(len(stnDicts)):\n",
    "        for j in range(1, len(pcpData)):\n",
    "            \n",
    "            #if pcpData[j][1][2:-1] == stnDicts[i]['fileName'][2:]:\n",
    "            if pcpData[j][1][:-1] == stnDicts[i]['fileName'][:]:\n",
    "                stnDicts[i]['lat']= pcpData[j][2]\n",
    "                stnDicts[i]['long']= pcpData[j][3]\n",
    "                stnDicts[i]['elev']= pcpData[j][4]\n",
    "                \n",
    "    return stnDicts, inputFolder, ablationFolder, accumulationFolder, climate_ref_Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Main Snow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3.1 *Initializiing the main dictionary for a case study*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseStudyStns = {}\n",
    "inputFolder = ''\n",
    "ablationFolder = ''\n",
    "accumulationFolder = ''\n",
    "climateFolder = ''\n",
    "#root = 'C:/Users/ashrafse/SA_2/snowModelUZH/case2_Atzmaening'\n",
    "#root = 'C:/Users/ashrafse/SA_2/snowModelUZH/case6_davos_elevations'\n",
    "root = r'C:\\Users\\ashrafse\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2'\n",
    "\n",
    "## calling the function with multiple return values\n",
    "caseStudyStns, inputFolder, ablationFolder, accumulationFolder, climateFolder = initialize_input_dict(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3.2 *Check if we have initialized correctly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ashrafse\\\\SA_2\\\\snowModelUZH\\\\case3_hoch-ybrig_v3_2\\\\input\\\\Climate_ref'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climateFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fM': '1.012',\n",
       "  'iPot': '1000',\n",
       "  'rSnow': '0.5',\n",
       "  'dSnow': '0.5',\n",
       "  'cPrec': '0',\n",
       "  'dP': '0',\n",
       "  'elev': '1755',\n",
       "  'lat': '47.00',\n",
       "  'long': '8.7708333',\n",
       "  'fileName': '47-0000000_8-7708333'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseStudyStns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.012'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseStudyStns[0].get(\"fM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defing the X variables which control modeling (hyper parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = caseStudyStns[0].get(\"fM\") # change 0 to i for all stations\n",
    "X3 = caseStudyStns[0].get(\"iPot\")\n",
    "X4 =  caseStudyStns[0].get(\"rSnow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.012\n",
      "1000\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(X2)\n",
    "print(X3)\n",
    "print(X4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3.3 *Function that runs the main model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1st column as index: makaing date from 01 01 1981 to 2099 12 31\n",
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date ).days + 1)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_release(x1):\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snow_Model (x1 = 250, Xfactor1 = None,  X2 = None, X3 = None, X4 = None, X5= 8):\n",
    "\n",
    "    \n",
    "    ''' VERY IMPORTANT --- Conroling the randomness --- VERY IMPORTANT'''\n",
    "    os.chdir(climateFolder)\n",
    "    fnames = os.listdir()\n",
    "    randomness_pcp_tmp(fnames, Xfactor1)\n",
    "    \n",
    "    print('Snow_Model: Matching the station names values with CSV files!')   \n",
    "    '''Matching the station names values in the dictionary of stations with CSV files in Climate folder of the case Study'''\n",
    "    pcpCaseStudy = []\n",
    "    tmpCaseStudy = []\n",
    "\n",
    "    for i in range(len(caseStudyStns)):\n",
    "        pcpCaseStudy.append(os.path.join(climateFolder, caseStudyStns[i]['fileName'] + 'p.csv'))\n",
    "        tmpCaseStudy.append(os.path.join(climateFolder, caseStudyStns[i]['fileName'] + 't.csv'))\n",
    "    \n",
    "\n",
    "    print('Snow_Model: Building a database for each csv file (tmp and pcp)!')\n",
    "    \n",
    "    '''Step 6: building a database for each precipitation and temperature file in Climate folder and saving them in a list'''\n",
    "    '''6.1 reading the csv files as databases'''\n",
    "    dfpcp = [None for _ in range(len(pcpCaseStudy))]\n",
    "    dftmp = [None for _ in range(len(tmpCaseStudy))]\n",
    "    for i in range(len(pcpCaseStudy)):\n",
    "        dfpcp[i] = pd.read_csv(pcpCaseStudy[i])\n",
    "        dftmp[i] = pd.read_csv(tmpCaseStudy[i])\n",
    "        \n",
    "    '''6.2 making a header for output files'''\n",
    "    dfpcpCol = dfpcp[0].columns\n",
    "    dftmpCol = dftmp[0].columns\n",
    "    \n",
    "    '''6.3 defining the length of simulations and scenarios'''\n",
    "    scenariosLength = len(dfpcpCol)\n",
    "    simulationLength = len(dftmp[0][dftmpCol[0]]) - 1\n",
    "        \n",
    "    \n",
    "    '''Reading the beginning and end of the simulation''' \n",
    "    start_date = date(1981, 1, 1)\n",
    "    end_date = date(2099, 12, 31)\n",
    "    dateList = []\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        dateList.append(single_date.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "    seasonList = []\n",
    "    for n in range (1981, 2100, 1):\n",
    "        seasonList.append(str(n))\n",
    "    \n",
    "    \n",
    "    print('Snow_Model: Running the model, daily output!')\n",
    "\n",
    "    '''PART 1 : daily outputs'''\n",
    "    '''Running the model for each climate station:'''\n",
    "    for k in range(len(caseStudyStns)):\n",
    "        \n",
    "        '''making a header for output files'''\n",
    "        dfpcpCol = dfpcp[k].columns\n",
    "        dftmpCol = dftmp[k].columns\n",
    "\n",
    "\n",
    "        '''defining the length of simulations and scenarios'''\n",
    "        scenariosLength = len(dfpcpCol)\n",
    "        simulationLength = len(dftmp[0][dftmpCol[0]]) - 1\n",
    "\n",
    "\n",
    "        '''declaring the initial arrays'''\n",
    "        accumulation = [0 for _ in range(simulationLength)]\n",
    "        ablation =  [0 for _ in range(simulationLength)]\n",
    "        snowDeposite = [0 for _ in range(simulationLength)]\n",
    "        total = np.zeros([simulationLength, 3*scenariosLength])\n",
    "\n",
    "\n",
    "        '''Running the model for each climate scenario:'''\n",
    "        for j in range(len(dfpcpCol)):\n",
    "            ## Reading the information and inputs of the first day of simulation\n",
    "            todayPCP = dfpcp[k][dfpcpCol[j]].iloc[1] if (dfpcp[k][dfpcpCol[j]].iloc[1] != -99) else 0\n",
    "            todayTMPMAX = round(dftmp[k][dftmpCol[2*j]].iloc[1],2) if(dftmp[k][dftmpCol[2*j]].iloc[1] != -99) else 0\n",
    "            todayTMPMIN = round(dftmp[k][dftmpCol[2*j+1]].iloc[1],2) if(dftmp[k][dftmpCol[2*j+1]].iloc[1] != -99) else 0\n",
    "            todayTMPAVE = round((todayTMPMAX+todayTMPMIN)/2,2) if((todayTMPMAX+todayTMPMIN)/2 != -99) else 0\n",
    "\n",
    "            A = policy_release(x1)\n",
    "            '''Accumulation for the first day:'''\n",
    "            if (todayTMPAVE) <= X5:\n",
    "                accumulation[0] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*(1)\n",
    "\n",
    "            elif X5 -1 < (todayTMPAVE) <= X5 + 1:\n",
    "                accumulation[0] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*float((X5 + 1 -todayTMPAVE)/2)\n",
    "\n",
    "            else: accumulation[0] = 0\n",
    "\n",
    "\n",
    "            '''Ablation for the first day:'''\n",
    "            if todayTMPAVE <= X5:\n",
    "                 ablation[0] = 0\n",
    "            else: \n",
    "                ablation[0] = (float(caseStudyStns[k]['fM']) + float(caseStudyStns[k]['rSnow'])*float(caseStudyStns[k]['iPot'])*0.001)*float(todayTMPAVE)*(1+0)\n",
    "\n",
    "            '''Main mass balance equation for the first day:'''\n",
    "            snowDeposite[0] = 0 if (0 + accumulation[0] - ablation[0]) < 0 else (0 + accumulation[0] - ablation[0])\n",
    "\n",
    "            \n",
    "            '''storing three values in a list for the first day'''\n",
    "            total[0,3*j+0] = round((accumulation[0] - ablation[0]), 2)\n",
    "            total[0,3*j+1] = round(snowDeposite[0], 2)\n",
    "            total[0,3*j+2] = 1 if (total[0,3*j+1] > A) else 0\n",
    "\n",
    "\n",
    "            '''For the second day to the end of simulation:'''\n",
    "            i = 0\n",
    "            for i in range(2, simulationLength + 1, 1):\n",
    "                '''# precipitation and temperature missing values were handled'''\n",
    "                todayPCP = dfpcp[k][dfpcpCol[j]].iloc[i] if (dfpcp[k][dfpcpCol[j]].iloc[i] != -99) else 0\n",
    "                todayTMPMAX = round(dftmp[k][dftmpCol[2*j]].iloc[i],2) if(dftmp[k][dftmpCol[2*j]].iloc[i] != -99) else 0\n",
    "                todayTMPMIN = round(dftmp[k][dftmpCol[2*j+1]].iloc[i],2) if(dftmp[k][dftmpCol[2*j+1]].iloc[i] != -99) else 0\n",
    "                todayTMPAVE = round((todayTMPMAX+todayTMPMIN)/2,2) if((todayTMPMAX+todayTMPMIN)/2 != -99) else 0\n",
    "\n",
    "                '''### Accumulation :'''\n",
    "                if(todayTMPAVE) <= X5:\n",
    "                    ##\n",
    "                    accumulation[i-1] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*(1)\n",
    "\n",
    "                elif X5 -1 < (todayTMPAVE) <= X5 + 1:\n",
    "                    accumulation[i-1] = todayPCP *(1 + float(caseStudyStns[k]['cPrec']))*float(caseStudyStns[k]['dSnow'])*float((X5 + 1 -todayTMPAVE)/2)\n",
    "\n",
    "                else: accumulation[i-1] = 0\n",
    "\n",
    "                '''### Ablation :'''\n",
    "                if todayTMPAVE <= X5:\n",
    "                    ablation[i-1] = 0\n",
    "                else: \n",
    "                    ablation[i-1] = (float(caseStudyStns[k]['fM']) + float(caseStudyStns[k]['rSnow'])*float(caseStudyStns[k]['iPot'])*0.001)*float(todayTMPAVE)*(1+0)\n",
    "\n",
    "                '''### Main mass balance equation for second day to the end of simulation:'''\n",
    "                snowDeposite[i-1] = 0 if (snowDeposite[i-2] + accumulation[i-1] - ablation[i-1]) < 0 else (snowDeposite[i-2] + accumulation[i-1] - ablation[i-1])\n",
    "\n",
    "\n",
    "                '''### storing three values in a list''' \n",
    "                total[i-1,3*j+0] = round((accumulation[i-1] - ablation[i-1]) , 2)\n",
    "                total[i-1,3*j+1] = round(snowDeposite[i-1], 2)\n",
    "                total[i-1,3*j+2] = 1 if (total[i-1,3*j+1] > A) else 0\n",
    "\n",
    "\n",
    "        '''### Saving the output of total list in a csv file in a specific path'''\n",
    "\n",
    "        ## 1st row as the column names:\n",
    "        columnsDF = [] \n",
    "        for col in dfpcpCol:\n",
    "            columnsDF.append('SnowAmount_' + col)\n",
    "            columnsDF.append('TotalSnowAmount_' + col)\n",
    "            columnsDF.append('isOverSnow_' + col)\n",
    "\n",
    "        columnsDF0 = ['DATE']\n",
    "        dfnew0 = pd.DataFrame(dateList, columns = columnsDF0)\n",
    "        dfnew = pd.DataFrame(total, columns = columnsDF)\n",
    "        df1 = pd.concat([dfnew0, dfnew], axis=1, sort=False)\n",
    "\n",
    "        if os.path.isdir(os.path.join(root, 'Outputs_py')):\n",
    "            pass\n",
    "        else: os.mkdir(os.path.join(root, 'Outputs_py'))\n",
    "\n",
    "        outfolder =os.path.join(root, 'Outputs_py') \n",
    "        outfileName = 'Total_daily_' + caseStudyStns[k]['fileName'] + '.csv'\n",
    "        outputFile = os.path.join(outfolder, outfileName )\n",
    "        df1.to_csv(outputFile, index = False)\n",
    "        #return df1\n",
    "           \n",
    "        \n",
    "        '''##### PART 2 seasonal outputs#####'''\n",
    "        print('Snow_Model: Running the model, seasonal outputs, reading files!')\n",
    "        \n",
    "        ####total_Daily_Files = list()\n",
    "        ####for filename in os.walk(outfolder):\n",
    "          ####  total_Daily_Files = filename[2] ######## I should avoid having totoal\n",
    "            \n",
    "            \n",
    "        #### 2020/06/10\n",
    "        total_Daily_FilesAll = list()\n",
    "        total_Daily_Files = []\n",
    "        for filename in os.walk(outfolder):\n",
    "            total_Daily_FilesAll = filename[2]\n",
    "\n",
    "        for bIndex in range (len(total_Daily_FilesAll)):\n",
    "            if 'season' in total_Daily_FilesAll[bIndex]:\n",
    "                print(total_Daily_FilesAll[bIndex])\n",
    "            else:\n",
    "                total_Daily_Files.append(total_Daily_FilesAll[bIndex])\n",
    "                            \n",
    "        \n",
    "        '''##get just total nor existing seasonal...'''\n",
    "        totalFiles = []\n",
    "        for i in range(len(total_Daily_Files)):\n",
    "            totalFiles.append(os.path.join(outfolder, total_Daily_Files[i]))\n",
    "        \n",
    "        totalFiles = []\n",
    "        for i in range(len(total_Daily_Files)):\n",
    "            if 'season' in total_Daily_Files[i]:\n",
    "                continue\n",
    "            else: totalFiles.append(os.path.join(outfolder, total_Daily_Files[i]))\n",
    "                \n",
    "        print('Snow_Model: Running the model, seasonal outputs analysis!')\n",
    "        dfSeason = [ None for _ in range(len(totalFiles))]\n",
    "        for i in range(len(totalFiles)):\n",
    "            dfSeason[i] = pd.read_csv(totalFiles[i], low_memory=False)\n",
    "\n",
    "            start_date = date(1981, 1, 2)\n",
    "            end_date = date(2099, 12, 31)\n",
    "            dateList = []\n",
    "            for single_date in daterange(start_date, end_date):\n",
    "                dateList.append(single_date.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "            start_season = []\n",
    "            end_season = []\n",
    "\n",
    "            for pp in range (1981, 2099, 1):\n",
    "                start_season.append(date(pp, 11, 1))\n",
    "                end_season.append(date(pp+1, 4, 30))\n",
    "\n",
    "            df2 = dfSeason[i]\n",
    "            df2.set_index('DATE', inplace = True)\n",
    "            df2Col = df2.columns\n",
    "\n",
    "            df2ColCal = []\n",
    "            for m in range(68):\n",
    "                df2ColCal.append(df2Col[3*m+2])\n",
    "\n",
    "            sumGoodCondition = np.zeros([len(start_season), len(df2ColCal)])\n",
    "            sumRows = np.zeros(len(df2ColCal))  ### SAEEEDDD  2020/06/11\n",
    "\n",
    "\n",
    "            for j in range(len(df2ColCal)):\n",
    "                for k in range(len(start_season)):\n",
    "                #for i in range(3):\n",
    "                    start_date = start_season[k]\n",
    "                    end_date = end_season[k]\n",
    "                        #start_date = date(1981, 1, 2)\n",
    "                        #end_date = date(1981, 1, 10)\n",
    "                    for single_date in daterange(start_date, end_date):\n",
    "                        sumGoodCondition[k,j] += df2[df2ColCal[j]].loc[single_date.strftime(\"%m/%d/%Y\")]\n",
    "                    sumRows[j] +=  sumGoodCondition[k,j] ### SAEEEDDD  2020/06/11\n",
    "            \n",
    "            AveragesumRows = np.average(sumRows/len(df2ColCal))\n",
    "            df3 = pd.DataFrame(sumGoodCondition, columns = df2ColCal)\n",
    "\n",
    "\n",
    "            firstCol = []\n",
    "            for o in range (len(seasonList)-1):\n",
    "                firstCol.append(seasonList[o] +'-' + seasonList[o+1])\n",
    "\n",
    "            columnsDF1 = ['Season']\n",
    "            dfnew0 = pd.DataFrame(firstCol, columns = columnsDF1)\n",
    "\n",
    "            dfFinalSeason = pd.concat([dfnew0, df3], axis=1, sort=False)\n",
    "\n",
    "            \n",
    "            #outfileNameSeason = 'season_' + total_Daily_Files[i]\n",
    "            #outputFile = os.path.join(outfolder, outfileNameSeason)\n",
    "            #dfFinalSeason.to_csv(outputFile, index = False)\n",
    "            \n",
    "            \n",
    "            if os.path.isdir(os.path.join(root, 'outSeason')):\n",
    "                pass\n",
    "            else: \n",
    "                os.mkdir(os.path.join(root, 'outSeason'))\n",
    "            \n",
    "            outfileNameSeason = 'season_' + total_Daily_Files[i]\n",
    "            outFolderSeason = os.path.join(root, 'outSeason')\n",
    "            outputFileSeason = os.path.join(outFolderSeason, outfileNameSeason)\n",
    "            \n",
    "            outFilesFinal = []\n",
    "            for filename in os.walk(outFolderSeason):\n",
    "                outFilesFinal = filename[2]\n",
    "                iii = len(outFilesFinal)\n",
    "                if os.path.isfile(outputFileSeason):\n",
    "                    newOutFileNameSeason = outputFileSeason[0 : -4] + '_' + str(iii) + '.csv'\n",
    "                    dfFinalSeason.to_csv(newOutFileNameSeason, index = False)\n",
    "                else: \n",
    "                    dfFinalSeason.to_csv(outputFileSeason, index = False)\n",
    "            \n",
    "            \n",
    "        #return df1, outfolder, dfFinalSeason\n",
    "        #return {'y' : x1 * Xfactor1 * X2}\n",
    "        return {'y' : AveragesumRows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_release(x1):\n",
    "    at = x1\n",
    "    return at\n",
    "\n",
    "def some_model(x1=1, x2=None, x3=None):\n",
    "    A = policy_release(x1)\n",
    "    return {'y':x2*x3+ A }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: EMA_Workbench connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 20 dec. 2010\n",
    "\n",
    "This file illustrated the use the EMA classes for a contrived example\n",
    "It's main purpose has been to test the parallel processing functionality\n",
    "\n",
    ".. codeauthor:: jhkwakkel <j.h.kwakkel (at) tudelft (dot) nl>\n",
    "'''\n",
    "from __future__ import (absolute_import, print_function, division,\n",
    "                        unicode_literals)\n",
    "\n",
    "from ema_workbench import (Model, RealParameter, ScalarOutcome, ema_logging,\n",
    "                           perform_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.evaluators/INFO/MainProcess] performing 8 scenarios * 4 policies * 1 model(s) = 32 experiments\n",
      "[EMA.ema_workbench.em_framework.evaluators/INFO/MainProcess] performing experiments sequentially\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 3 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 6 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 9 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 12 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 15 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 18 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 21 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 24 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 27 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.callbacks/INFO/MainProcess] 30 cases completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n",
      "Writing pcp files started!\n",
      "End of writing pcp files!\n",
      "Writing tmp files started!\n",
      "End of writing tmp files\n",
      "Snow_Model: Matching the station names values with CSV files!\n",
      "Snow_Model: Building a database for each csv file (tmp and pcp)!\n",
      "Snow_Model: Running the model, daily output!\n",
      "Snow_Model: Running the model, seasonal outputs, reading files!\n",
      "Snow_Model: Running the model, seasonal outputs analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EMA.ema_workbench.em_framework.evaluators/INFO/MainProcess] experiments finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ema_logging.LOG_FORMAT = '[%(name)s/%(levelname)s/%(processName)s] %(message)s'\n",
    "    ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "    #model = Model('simpleModel', function=some_model)  # instantiate the model\n",
    "    model = Model('UZHModel', function = snow_Model)  # instantiate the model\n",
    "    #model1 = Model('simpleModel', function = my_model11)\n",
    "    \n",
    "    \n",
    "    # specify uncertainties\n",
    "    #model.uncertainties = [RealParameter(\"x1\", 100.0, 300.0)]\n",
    "    \n",
    "    #model1.uncertainties = [RealParameter(\"X1\", -0.01, 0.01),\n",
    "     #                       RealParameter(\"X1factor\", 100.0, 300.0),\n",
    "      #                      RealParameter(\"X2\", -0.01, 0.01),\n",
    "       #                    ]\n",
    "    \n",
    "    model.uncertainties = [RealParameter(\"Xfactor1\", 4, 10),\n",
    "                           RealParameter(\"X2\", 1.01, 2.01),\n",
    "                           RealParameter(\"X3\", 900, 1100),\n",
    "                           RealParameter(\"X4\", 0.4, 0.6),\n",
    "                           RealParameter(\"X5\", 5.5, 10)]\n",
    "    \n",
    "    model.levers = [RealParameter(\"x1\", 250.0, 300.0)]\n",
    "   \n",
    "\n",
    "    # specify outcomes\n",
    "    model.outcomes = [ScalarOutcome('y')]\n",
    "\n",
    "    results = perform_experiments(model, 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(          X2           X3        X4        X5  Xfactor1          x1 scenario  \\\n",
       " 0   1.855898   981.628808  0.454622  6.405709  4.076921  254.359109        2   \n",
       " 1   1.600516  1024.598483  0.548800  8.467090  5.407686  254.359109        3   \n",
       " 2   1.339363   973.039910  0.401467  7.301361  7.514033  254.359109        4   \n",
       " 3   1.442585   937.302521  0.476645  6.626032  8.613781  254.359109        5   \n",
       " 4   1.138371  1084.672419  0.550099  5.598566  6.951834  254.359109        6   \n",
       " 5   1.916579  1036.886692  0.505892  9.994882  5.837132  254.359109        7   \n",
       " 6   1.637237   919.644603  0.437287  8.259041  9.273103  254.359109        8   \n",
       " 7   1.031868  1068.972094  0.586581  9.130975  8.278667  254.359109        9   \n",
       " 8   1.855898   981.628808  0.454622  6.405709  4.076921  293.412417        2   \n",
       " 9   1.600516  1024.598483  0.548800  8.467090  5.407686  293.412417        3   \n",
       " 10  1.339363   973.039910  0.401467  7.301361  7.514033  293.412417        4   \n",
       " 11  1.442585   937.302521  0.476645  6.626032  8.613781  293.412417        5   \n",
       " 12  1.138371  1084.672419  0.550099  5.598566  6.951834  293.412417        6   \n",
       " 13  1.916579  1036.886692  0.505892  9.994882  5.837132  293.412417        7   \n",
       " 14  1.637237   919.644603  0.437287  8.259041  9.273103  293.412417        8   \n",
       " 15  1.031868  1068.972094  0.586581  9.130975  8.278667  293.412417        9   \n",
       " 16  1.855898   981.628808  0.454622  6.405709  4.076921  276.679819        2   \n",
       " 17  1.600516  1024.598483  0.548800  8.467090  5.407686  276.679819        3   \n",
       " 18  1.339363   973.039910  0.401467  7.301361  7.514033  276.679819        4   \n",
       " 19  1.442585   937.302521  0.476645  6.626032  8.613781  276.679819        5   \n",
       " 20  1.138371  1084.672419  0.550099  5.598566  6.951834  276.679819        6   \n",
       " 21  1.916579  1036.886692  0.505892  9.994882  5.837132  276.679819        7   \n",
       " 22  1.637237   919.644603  0.437287  8.259041  9.273103  276.679819        8   \n",
       " 23  1.031868  1068.972094  0.586581  9.130975  8.278667  276.679819        9   \n",
       " 24  1.855898   981.628808  0.454622  6.405709  4.076921  267.151176        2   \n",
       " 25  1.600516  1024.598483  0.548800  8.467090  5.407686  267.151176        3   \n",
       " 26  1.339363   973.039910  0.401467  7.301361  7.514033  267.151176        4   \n",
       " 27  1.442585   937.302521  0.476645  6.626032  8.613781  267.151176        5   \n",
       " 28  1.138371  1084.672419  0.550099  5.598566  6.951834  267.151176        6   \n",
       " 29  1.916579  1036.886692  0.505892  9.994882  5.837132  267.151176        7   \n",
       " 30  1.637237   919.644603  0.437287  8.259041  9.273103  267.151176        8   \n",
       " 31  1.031868  1068.972094  0.586581  9.130975  8.278667  267.151176        9   \n",
       " \n",
       "    policy     model  \n",
       " 0       4  UZHModel  \n",
       " 1       4  UZHModel  \n",
       " 2       4  UZHModel  \n",
       " 3       4  UZHModel  \n",
       " 4       4  UZHModel  \n",
       " 5       4  UZHModel  \n",
       " 6       4  UZHModel  \n",
       " 7       4  UZHModel  \n",
       " 8       5  UZHModel  \n",
       " 9       5  UZHModel  \n",
       " 10      5  UZHModel  \n",
       " 11      5  UZHModel  \n",
       " 12      5  UZHModel  \n",
       " 13      5  UZHModel  \n",
       " 14      5  UZHModel  \n",
       " 15      5  UZHModel  \n",
       " 16      6  UZHModel  \n",
       " 17      6  UZHModel  \n",
       " 18      6  UZHModel  \n",
       " 19      6  UZHModel  \n",
       " 20      6  UZHModel  \n",
       " 21      6  UZHModel  \n",
       " 22      6  UZHModel  \n",
       " 23      6  UZHModel  \n",
       " 24      7  UZHModel  \n",
       " 25      7  UZHModel  \n",
       " 26      7  UZHModel  \n",
       " 27      7  UZHModel  \n",
       " 28      7  UZHModel  \n",
       " 29      7  UZHModel  \n",
       " 30      7  UZHModel  \n",
       " 31      7  UZHModel  ,\n",
       " {'y': array([ 42.691609  ,  73.74156574,  56.84083045,  47.40808824,\n",
       "          33.2774654 , 103.30925606,  74.70999135,  89.56487889,\n",
       "          34.14684256,  64.45653114,  46.57071799,  36.30882353,\n",
       "          23.92906574,  87.85683391,  60.89035467,  75.23529412,\n",
       "          37.9824827 ,  71.08866782,  52.14705882,  41.82158304,\n",
       "          26.97923875,  96.26838235,  66.68879758,  80.70588235,\n",
       "          40.07028547,  72.44593426,  53.63321799,  43.58845156,\n",
       "          29.11267301, 100.31596021,  71.05038927,  86.91500865])})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualization of the EMA_Workbench Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = r'C:\\Users\\ashrafse\\SA_2\\snowModelUZH\\case3_hoch-ybrig_v3_2\\Outputs_py\\season_Total_daily_47-0000000_8-7708333.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.iloc[0: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMatrix = df2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0) = plt.subplots(1, 1, figsize=(9.5,7.5))\n",
    "\n",
    "c = ax0.pcolor(dfMatrix)\n",
    "cb = fig.colorbar(c)\n",
    "ax0.set_title('default: no edges')\n",
    "\n",
    "\n",
    "ax0.set_xlabel(\"Climate scenarios\")\n",
    "ax0.set_ylabel(\"Years of Simulation\")\n",
    "plt.title(\"Likelihood of survival (no tipping point occured) of a resort from {} untill the year {}\".format(1981, 2099))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
